---
title: "Data Wrangling and Cleaning: Steps 1-11"
author: "David Mullaly"
date: "2026-02-16"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

## Introduction

This document covers the data wrangling process and all eleven data cleaning steps for the problem. We load multiple relational tables, merge them into a single flat file, and perform comprehensive data cleaning to prepare for missing data analysis.

### Data Cleaning Steps Covered:
**Steps 1-5 (Initial Cleaning)**

1. Open data in your software of choice
2. Review variables for common sense based on SME knowledge
3. Review how the software coded the variables (nominal, continuous)
4. Perform data integrity/validation checks
5. Handle dates

**Steps 6-7 (Categorical & Zero-Variance)**

6. Handle categorical variables - keep as is, combine rare levels, combine similar levels
7. Handle zero-variance predictors

**Steps 8-11 (Advanced Cleaning)**

8. Handle near zero-variance predictors
9. Eliminate redundant columns and linear combination columns
10. Search for outliers and initial search for missing values
11. Sanity check using Decision Tree (1 to 2 splits)



## Data Loading and Wrangling

Load four relational tables and merge into a single flat file (FFdf) using left joins on Cust_ID.


```
## MainDF: 9447 48 | StoreDF: 9272 3 | ConcessDF: 9272 3 | CustomerDF: 14272 7
```


```
## Duplicates in MainDF: 175
```

```
## Duplicates in StoreDF: 0
```

```
## Duplicates in ConcessDF: 0
```

```
## Duplicates in CustomerDF: 0
```


```
## Final flat file dimensions - Rows: 9447 Columns: 58
```

```
## Unique Cust_ID: 9272 | Duplicate rows: 175
```

## Step 1: Open Data in Your Software of Choice

Create identifier variable, arrange columns, and explore distributions visually.

![plot of chunk step1_open_data](figure/step1_open_data-1.png)

```
## Response Variable (Y01): 2196 7251
```

![plot of chunk step1_open_data](figure/step1_open_data-2.png)

**Observations:** Age is roughly normal (30-60), Tenure is right-skewed, NumSeats clusters around 2-4, Total Spent is heavily right-skewed. Response variable Y01 is reasonably balanced.

## Step 2: Review Variables for Common Sense (SME Knowledge)

Standardize variable names and check for unique identifiers.


```
## Dataset: 9447 rows, 59 columns
```

```
## Unique Cust_ID: 9272 out of 9447 rows
```

**Result:** Each row does not necessarily represent a unique customer (175 duplicate Cust_IDs found in MainDF source data).

## Step 3: Review How Software Coded Variables

Convert character variables to factors for proper categorical analysis.


```
## Character variables to convert: 23
```

```
## 
## Sex levels: F M
```

```
## Marital levels: D M S U
```

```
## Account_Type levels: Business Personal Shared
```

**Note:** Marital has levels D (Divorced), M (Married), S (Single), U (Unknown). The "U" for Unknown may need special handling.

## Step 4: Data Integrity/Validation Checks

Check for anomalies, bogus values, and data quality issues.


```
## Age range: 18 99
```

```
## Tenure range: 1 400
```

```
## NumSeats range: 1 10
```

```
## 
## DistA values = 999: 1506
```

```
## DistA NA count after conversion: 2895
```

```
## 
## Survey_Comp range: 0 7.4
```

```
## Survey_Comp values > 1: 110
```

```
## 
## State_Name unique: 50 | State_Loc unique: 50
```

**Issues Found:**

- **DistA = 999:** Placeholder values converted to NA
- **Survey_Comp > 1:** Outlier found when expected range is 0-1
- **Age max = 99:** May be placeholder or extreme value - verify with SME
- **Tenure max = 400:** Suspicious value if measured in years - verify units with SME
- **State_Name/State_Loc:** Redundant columns (same info, different format)
- **Marital = "U":** Unknown status - consider treating as NA
- **Cust_ID duplicates:** 175 duplicate IDs found in MainDF source data
- **Address, Name, PhoneNum:** 100% missing - likely removed from CustomerDF for privacy

## Step 5: Handle Dates

Convert Last_Contact datetime and extract useful components.


```
## Date components extracted: Contact_Year, Contact_Month, Contact_Day, Contact_Weekday, Contact_Hour
```

```
## Contact Year range: 2018 2025
```

```
## Contact Hour range: 0 23
```

## Summary


```
## Final Dataset: 9447 rows x 64 columns
```

```
## Total Missing Values: 94591
```

```
## 
## Columns with missing values:
```

```
##               Address                  Name              PhoneNum 
##                  9447                  9447                  9447 
##     Educational_Level  Favorite_Caps_Player        Favorite_Sport 
##                  6612                  6612                  6612 
##            Job_Sector     Mode_Of_Transport            Team_B_STH 
##                  6612                  6612                  6612 
##            Team_C_STH        Net_Worth_True HouseHold_Income_True 
##                  6612                  5762                  5691 
##                 DistA               Marital                   Age 
##                  2895                  1155                   986 
##              Rep_Name                   Sex                Tenure 
##                   871                   667                   592 
##            Rep_Visits             Rep_Calls          Num_Children 
##                   508                   433                   406
```

### Data Quality Issues for Future Steps:

| Issue | Possible action / Action Taken |
|-------|--------------|
| DistA = 999 | Converted to NA |
| Survey_Comp > 1 | Flag for investigation (110 values, max 7.4) |
| Age = 99 | Verify with SME |
| Tenure = 400 | Verify units with SME (400 years unlikely) |
| Marital = "U" | Consider as NA or keep |
| State redundancy | Drop one column |
| ID columns | Exclude from modeling |
| Cust_ID duplicates | 175 duplicates in MainDF - investigate or deduplicate |
| PII columns | Address, Name, PhoneNum 100% missing - exclude |



**Step 6: Handle Categorical Variables - keep as is, combine rare levels, combine similar levels**



```
## Factor variables: 23
```

```
## 
## Variables with rare levels (< 5%):
```

```
##  [1] "Educational_Level"     "Favorite_Caps_Player"  "Favorite_Sport"       
##  [4] "Favorite_Team"         "Job_Sector"            "Marital"              
##  [7] "Mode_Of_Transport"     "Most_Purch_Concession" "Mult_Loc"             
## [10] "Rep_Name"              "Seating_Location"      "State_Loc"            
## [13] "State_Name"
```

```
## 
## --- Sex Distribution ---
```

```
## 
##    F    M <NA> 
## 1078 7702  667
```

```
## 
## --- Marital Distribution ---
```

```
## 
##    D    M    S    U <NA> 
##  908 6227  909  248 1155
```

```
## 
## --- Account_Type Distribution ---
```

```
## 
## Business Personal   Shared 
##     1057     7462      928
```

```
## 
## --- Educational_Level Distribution ---
```

```
## 
##   AD   BD   HS   MD  PHD   SC <NA> 
##  450  827  547  313  237  461 6612
```


```
## Marital 'U' (Unknown) count: 248
```

```
## Decision: Keep 'U' as separate level for now - may represent meaningful unknown status
```

```
## 
## --- State_Name levels ---
```

```
## [1] 50
```

```
## Number of unique states: 50
```


```
## === LUMPING RARE LEVELS (< 5%) INTO 'Other' ===
```

```
## Favorite_Sport: 8 -> 2 levels
## 
##   NHL Other  <NA> 
##  1996   839  6612 
## 
## Favorite_Team: 25 -> 4 levels
## 
##   New Jersey Devils Philadelphia Flyers Washington Capitals               Other 
##                 788                 856                6632                1171 
## 
## Mode_Of_Transport: 5 -> 4 levels
## 
##       Car    Public Uber/Taxi     Other      <NA> 
##      1104       905       603       223      6612 
## 
## Most_Purch_Concession: 9 -> 8 levels
## 
##      Beer    Burger   Hot Dog   Peanuts   Popcorn      Soda Specialty     Other 
##      2294       955      1422       474      1387      1425       977       513 
## 
## Mult_Loc: 3 -> 2 levels
## 
##    No Other 
##  9163   284 
## 
## Rep_Name: 9 -> 7 levels
## 
## Alice David  Emma Frank Grace   Ivy Other  <NA> 
##   823  1798  1538  1942   952   959   564   871
```

```
## Lumping complete.
```


```
## --- Region Distribution (grouped from State_Name) ---
```

```
## 
##    DCArea   Midwest Northeast     South      West 
##      4842       551      1703      1856       495
```

```
## 
## Region percentages:
```

```
## 
##    DCArea   Midwest Northeast     South      West 
##     51.25      5.83     18.03     19.65      5.24
```

```
## 
## All states successfully mapped to regions.
```

**Step 6 Observations:**

- Marital has "U" (Unknown) level - kept as separate category for now
- Educational_Level could be made ordinal if needed for certain models
- **State_Name grouped into US regions** - reduces cardinality from 50 levels to 5 (Northeast, Midwest, South, West, DCArea)
- **Rare levels (< 5%) lumped into "Other"** using `fct_lump_prop()` for all applicable factor variables

---

**Step 7: Remove Zero-Variance Predictors**




```
## Zero-variance columns found: 8
```

```
## Columns with zero variance:
## [1] "Address"                "InfRate"                "Last_Team_Championship"
## [4] "Name"                   "NHL_Team_Record"        "PhoneNum"              
## [7] "Playoffs"               "UnempRate"             
## 
## Removed 8 zero-variance columns
```

```
## Remaining columns: 58
```

**Step 7 Results:**

The following 8 zero-variance columns were identified and removed:

- **Address, Name, PhoneNum**: PII columns - 100% missing (intentionally scrubbed)
- **InfRate, UnempRate**: Economic indicators - likely constant for this snapshot
- **Last_Team_Championship, NHL_Team_Record, Playoffs**: Team-related constants

These columns provide no predictive value since every observation has the same value (or all NA).

---

## Class 4: Data Cleaning Process: Steps 8-11

**Step 8: Handle Near Zero-Variance Predictors**



```
## Near-zero variance columns (>95% one value):
```

```
##           Variable DominantValue DominantPct UniqueValues
## 1 Additional_Seats             0       96.99           12
## 2         Mult_Loc            No       96.99            2
```


```
## Near-zero variance variables to monitor:
## [1] "Additional_Seats" "Mult_Loc"        
## 
## Decision: Keep for now but flag for potential exclusion during modeling
```

**Step 8 Results:**

Near-zero variance columns identified (>95% one value):

| Variable | Dominant Value | Dominant % |
|----------|----------------|------------|
| Additional_Seats | 0 | 96.99% |
| Mult_Loc | No | 96.99% |

**Observations:**

- **Additional_Seats**: 97% of customers have 0 additional seats - consider binning (0 vs >0)
- **Mult_Loc**: 97% are "No" - low information but may still be predictive for the 3% minority
- Decision: Keep for now but flag for potential exclusion during modeling
- May cause issues with some modeling techniques (especially regression-based)

---

**Step 9: Remove Redundant Columns and Linear Combination Columns**



```
## --- Checking State_Name vs State_Loc redundancy ---
```

```
## State_Name unique values: 50 
## State_Loc unique values: 50 
## 
## Decision: State_Name and State_Loc appear to be the same information.
## Removing State_Loc (keeping State_Name)
```

```
## 
## --- Highly correlated variable pairs (|r| > 0.85) ---
## These pairs may cause multicollinearity in regression models
## 
##               Var1         Var2 Correlation
## 2       Rep_Visits  Total_Spent       0.947
## 4 Weekday_Attended Weekday_Sold      -0.946
## 1 Concession_Total     NumSeats       0.915
## 3 Team_Store_Total  Total_Spent       0.853
```

![plot of chunk step9_correlation_plot](figure/step9_correlation_plot-1.png)


```
## === MULTICOLLINEARITY ANALYSIS ===
```

```
## Cluster 1: Spending & Visit variables
```

```
##   - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)
```

```
##   - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)
```

```
##   Recommendation: Consider removing Rep_Visits or Total_Spent
```

```
## Cluster 2: Concession & Seating
```

```
##   - Concession_Total <-> NumSeats: r = 0.915 (strong positive)
```

```
##   Recommendation: Makes business sense - more seats = more concessions
```

```
## Cluster 3: Attendance pairs
```

```
##   - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)
```

```
##   Note: Negative correlation suggests inverse relationship
```

```
##   Recommendation: Keep both - they capture different behaviors
```

```
## Variables flagged for potential removal due to multicollinearity:
```

```
## [1] "Rep_Visits"       "Team_Store_Total"
```

```
## 
## Decision: Flag but keep for now; remove during modeling if VIF > 10
```

**Step 9 Observations:**

Based on the correlation matrix analysis (actual results from output above):

| Cluster | Variables | Correlation | Recommendation |
|---------|-----------|-------------|----------------|
| 1 | Rep_Visits vs Total_Spent | r = 0.947 | Remove Rep_Visits |
| 1 | Team_Store_Total vs Total_Spent | r = 0.853 | Monitor for VIF |
| 2 | Concession_Total vs NumSeats | r = 0.915 | Keep - business logic |
| 3 | Weekday_Attended vs Weekday_Sold | r = -0.946 | Keep both - inverse relationship |
| - | State_Loc vs State_Name | Redundant | **REMOVED** |

**Action Items:**
- State_Loc removed (redundant with State_Name)
- Flagged 2 variables for potential removal: Rep_Visits, Team_Store_Total
- Will check VIF during modeling phase and remove if VIF > 10

---

**Step 10: Search for Outliers and Initial Search for Missing Values**


```
## Age outliers (IQR method): 0 values
```

```
## Tenure outliers: 8 values | Max: 400
```

```
## Survey_Comp values > 1: 110
```

![plot of chunk step10_outliers](figure/step10_outliers-1.png)


```
## === MISSING DATA ASSESSMENT ===
```

```
## Total missing values: 67405 out of 538479 ( 12.52 %)
```

```
## Columns with missing values:
```

```
##                 Variable Missing_Count Missing_Pct
## 1      Educational_Level          6612       69.99
## 2   Favorite_Caps_Player          6612       69.99
## 3         Favorite_Sport          6612       69.99
## 4             Job_Sector          6612       69.99
## 5      Mode_Of_Transport          6612       69.99
## 6             Team_B_STH          6612       69.99
## 7             Team_C_STH          6612       69.99
## 8         Net_Worth_True          5762       60.99
## 9  HouseHold_Income_True          5691       60.24
## 10                 DistA          2895       30.64
## 11               Marital          1155       12.23
## 12      Marital_Original          1155       12.23
## 13                   Age           986       10.44
## 14              Rep_Name           871        9.22
## 15                   Sex           667        7.06
## 16                Tenure           592        6.27
## 17            Rep_Visits           508        5.38
## 18             Rep_Calls           433        4.58
## 19          Num_Children           406        4.30
```


```
## 
## === OUTLIER DECISIONS ===
```

```
## 1. Tenure max = 400:
```

```
##    - If measured in years, this is impossible
```

```
##    - May be measured in months (400 months = 33 years - plausible)
```

```
##    - ACTION: Verify units with SME; flag for review
```

```
## 2. Age = 99:
```

```
##    - Could be real (elderly customer) or placeholder
```

```
##    - ACTION: Verify with SME; consider if 99 is data entry default
```

```
## 3. Survey_Comp values > 1 (expected 0-1 range):
```

```
##    - Count: 110
```

```
##    - Max value: 7.4
```

```
##    - ACTION: Possible scale issue; cap at 1 or investigate data source
```

```
## Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid
```

---

**Step 11: Sanity Check Using Decision Tree (1 to 2 splits)**


![plot of chunk step11_decision_tree](figure/step11_decision_tree-1.png)

```
## 
## === VARIABLE IMPORTANCE ===
```

```
##           Region    Favorite_Team          PerUsed            DistA 
##       1505.21434        253.50828        245.18919        196.77674 
##           Tenure   First_Year_STH         Rep_Name     Rejoined_STH 
##        187.73632        125.59980         80.93227         50.19569 
## Team_Store_Total        PerTransf 
##         26.31404         21.05856
```


```
## 
## === SANITY CHECK ANALYSIS ===
```

```
## Tree accuracy: 88.11 %
```

```
## Accuracy is reasonable - no obvious data leakage detected
```

**Step 11 Results:**

The decision tree uses Region (from Step 6) instead of raw State_Name/Zip_Codes to avoid overfitting from high-cardinality variables.

Top Variable Importance:

| Variable | Importance |
|----------|------------|
| Region | 1505.2 |
| Favorite_Team | 253.5 |
| PerUsed | 245.2 |
| DistA | 196.8 |
| Tenure | 187.7 |

**Analysis:**

- **Accuracy ~88%** - reasonable, no obvious data leakage
- **Region is the dominant predictor** - geographic location strongly predicts Y01
- **No high-cardinality variables** causing artificial inflation of accuracy

---

## Summary of Steps 6-11 (Data Cleaning Complete)


```
## === DATA CLEANING SUMMARY (Steps 6-11) ===
```

```
## Step 6 - Handle Categorical Variables:
```

```
##   - Rare factor levels (< 5%) lumped into 'Other' via fct_lump_prop()
```

```
##   - Marital 'U' kept as separate category
```

```
##   - State_Name grouped into US Census regions
```

```
## Step 7 - Zero-Variance Predictors:
```

```
##   - Columns removed: 8
```

```
## Step 8 - Near Zero-Variance Predictors:
```

```
##   - Variables flagged: 2
```

```
##   - Decision: Keep for now but monitor during modeling
```

```
## Step 9 - Redundant Columns:
```

```
##   - State_Loc removed (redundant with State_Name)
```

```
##   - Correlation matrix reviewed for multicollinearity
```

```
## Step 10 - Outliers & Missing Data:
```

```
##   - Total missing values: 67997
```

```
##   - Outlier flags created for Tenure and Survey_Comp
```

```
##   - Missing data summary table generated
```

```
## Step 11 - Decision Tree Sanity Check:
```

```
##   - Tree accuracy: 88.11 %
```

```
##   - Review variable importance for potential data leakage
```

```
## Final dataset dimensions: 9447 rows x 59 columns
```

```
## 
## Cleaned dataset saved to: FFdf_cleaned.csv
```

### Issues Identified for Further Action:

| Step | Issue | Recommendation |
|------|-------|----------------|
| 6 | Marital "U" unknown | Keep as category or convert to NA during imputation |
| 6 | Rare factor levels (< 5%) | **RESOLVED:** Lumped into "Other" via fct_lump_prop() |
| 6 | State_Name high cardinality | **RESOLVED:** Grouped into US Census regions |
| 7 | Zero-variance columns | Removed from dataset |
| 8 | Near-zero variance | Monitor during modeling; consider binning |
| 9 | State_Loc redundant | Removed |
| 10 | Tenure = 400 | Verify units with SME (years vs months?) |
| 10 | Survey_Comp > 1 | Investigate scale/cap values at 1 |
| 10 | Missing data patterns | Address in Missing Data phase (Class 5+) |
| 11 | Tree predictors | **RESOLVED:** Using Region variable instead of State_Name |

---

## Class 7: Handling Missing Data - Steps 1-4

**Step 1: Identify Missing Data**


``` r
cat("=== STEP 1: IDENTIFY MISSING DATA ===\n\n")
```

```
## === STEP 1: IDENTIFY MISSING DATA ===
```

``` r
# Start Missing Data workflow from cleaned dataset
MDdf <- FFdf

missing_counts_step1 <- colSums(is.na(MDdf))
missing_counts_step1 <- sort(missing_counts_step1[missing_counts_step1 > 0], decreasing = TRUE)
missing_table_step1 <- data.frame(
  Variable = names(missing_counts_step1),
  Missing_Count = as.numeric(missing_counts_step1),
  Missing_Pct = round(as.numeric(missing_counts_step1) / nrow(MDdf) * 100, 2)
)

cat("Variables with missing values:", length(missing_counts_step1), "\n")
```

```
## Variables with missing values: 20
```

``` r
cat("Total missing cells:", sum(is.na(MDdf)), "\n\n")
```

```
## Total missing cells: 67997
```

``` r
print(missing_table_step1)
```

```
##                 Variable Missing_Count Missing_Pct
## 1      Educational_Level          6612       69.99
## 2   Favorite_Caps_Player          6612       69.99
## 3         Favorite_Sport          6612       69.99
## 4             Job_Sector          6612       69.99
## 5      Mode_Of_Transport          6612       69.99
## 6             Team_B_STH          6612       69.99
## 7             Team_C_STH          6612       69.99
## 8         Net_Worth_True          5762       60.99
## 9  HouseHold_Income_True          5691       60.24
## 10                 DistA          2895       30.64
## 11               Marital          1155       12.23
## 12      Marital_Original          1155       12.23
## 13                   Age           986       10.44
## 14              Rep_Name           871        9.22
## 15                   Sex           667        7.06
## 16                Tenure           592        6.27
## 17      Flag_Tenure_High           592        6.27
## 18            Rep_Visits           508        5.38
## 19             Rep_Calls           433        4.58
## 20          Num_Children           406        4.30
```

``` r
# Optional pattern table from mice
md_pattern_step1 <- tryCatch(mice::md.pattern(MDdf, plot = FALSE), error = function(e) NULL)
if(!is.null(md_pattern_step1)) {
  cat("\nTop missingness patterns (mice::md.pattern):\n")
  print(head(md_pattern_step1, 8))
}
```

```
## 
## Top missingness patterns (mice::md.pattern):
##     Y01 Account_Type Additional_Seats Arrival_Time Auto_Renew_STH
## 168   1            1                1            1              1
## 372   1            1                1            1              1
## 249   1            1                1            1              1
## 598   1            1                1            1              1
## 238   1            1                1            1              1
## 558   1            1                1            1              1
## 400   1            1                1            1              1
## 936   1            1                1            1              1
##     Concession_Total Cust_ID Favorite_Team First_Year_STH ID Last_Contact
## 168                1       1             1              1  1            1
## 372                1       1             1              1  1            1
## 249                1       1             1              1  1            1
## 598                1       1             1              1  1            1
## 238                1       1             1              1  1            1
## 558                1       1             1              1  1            1
## 400                1       1             1              1  1            1
## 936                1       1             1              1  1            1
##     Most_Purch_Concession Mult_Loc NumSeats PerSold PerTransf PerUsed
## 168                     1        1        1       1         1       1
## 372                     1        1        1       1         1       1
## 249                     1        1        1       1         1       1
## 598                     1        1        1       1         1       1
## 238                     1        1        1       1         1       1
## 558                     1        1        1       1         1       1
## 400                     1        1        1       1         1       1
## 936                     1        1        1       1         1       1
##     Rejoined_STH Seating_Location Spent_Other_Teams State_Name STH_Attended
## 168            1                1                 1          1            1
## 372            1                1                 1          1            1
## 249            1                1                 1          1            1
## 598            1                1                 1          1            1
## 238            1                1                 1          1            1
## 558            1                1                 1          1            1
## 400            1                1                 1          1            1
## 936            1                1                 1          1            1
##     Survey_Comp Team_Network_Sub Team_Store_Total Ticket_Form Total_Spent
## 168           1                1                1           1           1
## 372           1                1                1           1           1
## 249           1                1                1           1           1
## 598           1                1                1           1           1
## 238           1                1                1           1           1
## 558           1                1                1           1           1
## 400           1                1                1           1           1
## 936           1                1                1           1           1
##     Weekday_Attended Weekday_Sold Weekend_Attended Weekend_Sold Zip_Codes
## 168                1            1                1            1         1
## 372                1            1                1            1         1
## 249                1            1                1            1         1
## 598                1            1                1            1         1
## 238                1            1                1            1         1
## 558                1            1                1            1         1
## 400                1            1                1            1         1
## 936                1            1                1            1         1
##     Contact_Year Contact_Month Contact_Day Contact_Weekday Contact_Hour Region
## 168            1             1           1               1            1      1
## 372            1             1           1               1            1      1
## 249            1             1           1               1            1      1
## 598            1             1           1               1            1      1
## 238            1             1           1               1            1      1
## 558            1             1           1               1            1      1
## 400            1             1           1               1            1      1
## 936            1             1           1               1            1      1
##     Flag_Survey_Invalid Num_Children Rep_Calls Rep_Visits Tenure
## 168                   1            1         1          1      1
## 372                   1            1         1          1      1
## 249                   1            1         1          1      1
## 598                   1            1         1          1      1
## 238                   1            1         1          1      1
## 558                   1            1         1          1      1
## 400                   1            1         1          1      1
## 936                   1            1         1          1      1
##     Flag_Tenure_High Sex Rep_Name Age Marital Marital_Original DistA
## 168                1   1        1   1       1                1     1
## 372                1   1        1   1       1                1     1
## 249                1   1        1   1       1                1     1
## 598                1   1        1   1       1                1     1
## 238                1   1        1   1       1                1     1
## 558                1   1        1   1       1                1     1
## 400                1   1        1   1       1                1     1
## 936                1   1        1   1       1                1     1
##     HouseHold_Income_True Net_Worth_True Educational_Level Favorite_Caps_Player
## 168                     1              1                 1                    1
## 372                     1              1                 0                    0
## 249                     1              0                 1                    1
## 598                     1              0                 0                    0
## 238                     0              1                 1                    1
## 558                     0              1                 0                    0
## 400                     0              0                 1                    1
## 936                     0              0                 0                    0
##     Favorite_Sport Job_Sector Mode_Of_Transport Team_B_STH Team_C_STH  
## 168              1          1                 1          1          1 0
## 372              0          0                 0          0          0 7
## 249              1          1                 1          1          1 1
## 598              0          0                 0          0          0 8
## 238              1          1                 1          1          1 1
## 558              0          0                 0          0          0 8
## 400              1          1                 1          1          1 2
## 936              0          0                 0          0          0 9
```

**Step 1 Results and Interpretation:**

- Variables with missing values: **20**
- Total missing cells: **67,997**
- Highest-missing variables are concentrated in demographic/profile fields (for example: Educational_Level, Favorite_Caps_Player, Favorite_Sport, Job_Sector, Mode_Of_Transport)
- Interpretation: missingness is substantial and not isolated to a single field, so a structured missing-data workflow is required.

**Step 2: Mark Missing Data**


``` r
cat("=== STEP 2: MARK MISSING DATA ===\n\n")
```

```
## === STEP 2: MARK MISSING DATA ===
```

``` r
# Create indicator columns for variables with missingness
vars_with_na_step2 <- names(MDdf)[colSums(is.na(MDdf)) > 0]
indicator_names_step2 <- paste0(vars_with_na_step2, "_Missing")

for(v in vars_with_na_step2) {
  MDdf[[paste0(v, "_Missing")]] <- ifelse(is.na(MDdf[[v]]), 1, 0)
}

cat("Missingness indicators created:", length(vars_with_na_step2), "\n")
```

```
## Missingness indicators created: 20
```

``` r
cat("Example indicators:\n")
```

```
## Example indicators:
```

``` r
print(head(indicator_names_step2, 10))
```

```
##  [1] "Age_Missing"                   "DistA_Missing"                
##  [3] "Educational_Level_Missing"     "Favorite_Caps_Player_Missing" 
##  [5] "Favorite_Sport_Missing"        "HouseHold_Income_True_Missing"
##  [7] "Job_Sector_Missing"            "Marital_Missing"              
##  [9] "Mode_Of_Transport_Missing"     "Net_Worth_True_Missing"
```

``` r
cat("\nIdentifier columns retained:",
    all(c("ID", "Cust_ID") %in% names(MDdf)), "\n")
```

```
## 
## Identifier columns retained: TRUE
```

**Step 2 Results and Interpretation:**

- Missingness indicators created: **20**
- Indicator columns preserve traceability between original NA values and imputed values.
- Interpretation: this supports mechanism testing (MCAR/MAR checks) and sensitivity analysis later.

**Step 3: Clean Up Obvious Mistakes**


``` r
cat("=== STEP 3: CLEAN UP OBVIOUS MISTAKES ===\n\n")
```

```
## === STEP 3: CLEAN UP OBVIOUS MISTAKES ===
```

``` r
# Standardize text fields and convert blank strings to NA
before_missing_step3 <- sum(is.na(MDdf))
added_na_from_blanks_step3 <- 0

text_cols <- names(MDdf)[sapply(MDdf, function(x) is.factor(x) || is.character(x))]
factor_cols <- names(MDdf)[sapply(MDdf, is.factor)]

for(col in text_cols) {
  x <- trimws(as.character(MDdf[[col]]))
  blank_idx <- !is.na(x) & x == ""
  added_na_from_blanks_step3 <- added_na_from_blanks_step3 + sum(blank_idx)
  x[blank_idx] <- NA
  if(is.factor(MDdf[[col]])) {
    MDdf[[col]] <- as.factor(x)
  } else {
    MDdf[[col]] <- x
  }
}

# "U" in Marital treated as unknown -> missing for missing-data workflow
marital_u_to_na_step3 <- 0
if("Marital" %in% names(MDdf)) {
  if(!("Marital_Original_MD" %in% names(MDdf))) {
    MDdf$Marital_Original_MD <- MDdf$Marital
  }
  mar <- as.character(MDdf$Marital)
  marital_u_to_na_step3 <- sum(mar == "U", na.rm = TRUE)
  mar[mar == "U"] <- NA
  MDdf$Marital <- as.factor(mar)
}

after_missing_step3 <- sum(is.na(MDdf))
net_new_missing_step3 <- after_missing_step3 - before_missing_step3

cat("Blank strings converted to NA:", added_na_from_blanks_step3, "\n")
```

```
## Blank strings converted to NA: 0
```

``` r
cat("Marital 'U' converted to NA:", marital_u_to_na_step3, "\n")
```

```
## Marital 'U' converted to NA: 248
```

``` r
cat("Net change in missing cells:", net_new_missing_step3, "\n")
```

```
## Net change in missing cells: 1403
```

``` r
cat("Total missing cells after cleanup:", after_missing_step3, "\n")
```

```
## Total missing cells after cleanup: 69400
```

``` r
# Keep factors compact after cleanup
for(col in factor_cols) {
  MDdf[[col]] <- droplevels(MDdf[[col]])
}

cat("Whitespace cleanup applied to factor columns:", length(factor_cols), "\n")
```

```
## Whitespace cleanup applied to factor columns: 22
```

``` r
if("Marital" %in% names(MDdf)) {
  cat("Marital levels after cleanup:\n")
  print(levels(MDdf$Marital))
}
```

```
## Marital levels after cleanup:
## [1] "D" "M" "S"
```

**Step 3 Results and Interpretation:**

- Factor columns standardized: **22**
- Blank strings converted to NA: **0**
- Marital "U" converted to NA: **248**
- Marital levels after cleanup: **D, M, S**
- Interpretation: obvious coding issues are cleaned and unknown categories are consistently represented as missing for downstream imputation decisions.

**Step 4: Make Decisions on Rows/Columns**


``` r
cat("=== STEP 4: ROW/COLUMN DECISIONS ===\n\n")
```

```
## === STEP 4: ROW/COLUMN DECISIONS ===
```

``` r
# Missingness by column
na_pct_step4 <- colSums(is.na(MDdf)) / nrow(MDdf)
high_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.40])
extreme_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.65])

cat("Columns > 40% missing:", length(high_missing_cols_step4), "\n")
```

```
## Columns > 40% missing: 9
```

``` r
if(length(high_missing_cols_step4) > 0) {
  print(high_missing_cols_step4)
} else {
  cat("No columns exceed 40% missingness threshold.\n")
}
```

```
## [1] "Educational_Level"     "Favorite_Caps_Player"  "Favorite_Sport"       
## [4] "HouseHold_Income_True" "Job_Sector"            "Mode_Of_Transport"    
## [7] "Net_Worth_True"        "Team_B_STH"            "Team_C_STH"
```

``` r
# Missingness by row
row_missing_pct_step4 <- rowSums(is.na(MDdf)) / ncol(MDdf)
rows_high_missing_step4 <- which(row_missing_pct_step4 > 0.50)

cat("\nRows > 50% missing:", length(rows_high_missing_step4), "\n")
```

```
## 
## Rows > 50% missing: 0
```

``` r
# Mark (do not delete) for modeling decisions
excluded_cols_md_step4 <- unique(c("ID", "Cust_ID", extreme_missing_cols_step4))
cat("Columns marked for potential exclusion from modeling (not deleted):\n")
```

```
## Columns marked for potential exclusion from modeling (not deleted):
```

``` r
print(excluded_cols_md_step4)
```

```
## [1] "ID"                   "Cust_ID"              "Educational_Level"   
## [4] "Favorite_Caps_Player" "Favorite_Sport"       "Job_Sector"          
## [7] "Mode_Of_Transport"    "Team_B_STH"           "Team_C_STH"
```

``` r
cat("Decision: mark rows/columns for modeling strategy; avoid hard deletes at this stage.\n")
```

```
## Decision: mark rows/columns for modeling strategy; avoid hard deletes at this stage.
```

**Step 4 Results and Interpretation:**

- Columns above 40% missingness: **9**
- Rows above 50% missingness: **0**
- Columns marked (not deleted): **9**
- Interpretation: column-level missingness is the dominant issue; variables are flagged for strategy decisions rather than removed outright.

---

## Class 8: Handling Missing Data - Step 5

**Step 5: Assess Missingness Patterns**


``` r
cat("=== STEP 5: ASSESS MISSINGNESS PATTERNS ===\n\n")
```

```
## === STEP 5: ASSESS MISSINGNESS PATTERNS ===
```

``` r
# Pairwise missingness summary
missing_matrix_step5 <- is.na(MDdf)
cat("Any complete rows:", any(rowSums(missing_matrix_step5) == 0), "\n")
```

```
## Any complete rows: TRUE
```

``` r
cat("Rows with at least one missing value:", sum(rowSums(missing_matrix_step5) > 0), "\n")
```

```
## Rows with at least one missing value: 9286
```

``` r
# Bar chart of top missing variables
na_count_step5 <- sort(colSums(missing_matrix_step5), decreasing = TRUE)
na_count_step5 <- na_count_step5[na_count_step5 > 0]

if(length(na_count_step5) > 0) {
  top_na_step5 <- head(na_count_step5, 15)
  barplot(top_na_step5,
          las = 2,
          col = "steelblue",
          main = "Top Variables by Missing Count",
          ylab = "Missing Count")
} else {
  plot.new()
  text(0.5, 0.5, "No missing values detected")
}
```

![plot of chunk md_step5_missing_patterns](figure/md_step5_missing_patterns-1.png)

``` r
cat("\n=== MCAR / MAR / MNAR ASSESSMENT (PRACTICAL) ===\n\n")
```

```
## 
## === MCAR / MAR / MNAR ASSESSMENT (PRACTICAL) ===
```

``` r
# Practical mechanism check:
# - Manual association tests + simple decision tree used for judgment (per course guidance)
# - If missingness relates to observed data -> likely MAR
# - If no relationships detected -> could be MCAR
# - MNAR is flagged as possible for sensitive/high-missing variables, then SME validated

vars_with_na_step5 <- names(MDdf)[colSums(is.na(MDdf)) > 0]
assessment_step5 <- data.frame(
  Variable = character(),
  Missing_Count = integer(),
  Missing_Pct = numeric(),
  Manual_Tests_Run = integer(),
  Significant_Assoc = integer(),
  Tree_Has_Splits = character(),
  Likely_Mechanism = character(),
  MNAR_Risk = character(),
  Judgement_Basis = character(),
  stringsAsFactors = FALSE
)

# Variables where nonresponse may depend on latent/unobserved value (domain risk)
mnar_sensitive_vars <- c(
  "Educational_Level", "Favorite_Caps_Player", "Favorite_Sport",
  "HouseHold_Income_True", "Job_Sector", "Mode_Of_Transport",
  "Net_Worth_True", "Team_B_STH", "Team_C_STH"
)

tree_predictors_step5 <- intersect(c(
  "Age", "Tenure", "Total_Spent", "NumSeats", "Survey_Comp", "DistA",
  "Sex", "Marital", "Account_Type", "Region", "Rep_Visits", "Rep_Calls"
), names(MDdf))

for(v in vars_with_na_step5) {
  miss_ind <- is.na(MDdf[[v]])
  if(sum(miss_ind) == 0 || sum(!miss_ind) == 0) next

  tests_run <- 0
  sig_assoc <- 0

  # Manual tests against a stable predictor set
  candidate_preds <- setdiff(tree_predictors_step5, v)

  for(p in candidate_preds) {
    pred <- MDdf[[p]]
    if(all(is.na(pred))) next
    ok <- !is.na(pred)
    if(sum(ok) < 30) next

    pval <- NA
    if(is.numeric(pred)) {
      # Numeric predictor vs missingness group
      if(length(unique(miss_ind[ok])) == 2) {
        pval <- tryCatch(wilcox.test(pred[ok] ~ miss_ind[ok])$p.value, error = function(e) NA)
      }
    } else if(is.factor(pred) || is.character(pred) || is.logical(pred)) {
      # Categorical predictor vs missingness group
      tab <- table(miss_ind[ok], pred[ok])
      if(all(dim(tab) >= c(2, 2))) {
        pval <- tryCatch(suppressWarnings(chisq.test(tab)$p.value), error = function(e) NA)
      }
    }

    if(!is.na(pval)) {
      tests_run <- tests_run + 1
      if(pval < 0.05) sig_assoc <- sig_assoc + 1
    }
  }

  # Decision tree check for non-random missingness structure
  tree_has_splits <- FALSE
  tree_df <- MDdf[, c(setdiff(tree_predictors_step5, v)), drop = FALSE]
  if(ncol(tree_df) > 0) {
    tree_df$miss_flag <- as.factor(ifelse(is.na(MDdf[[v]]), "Missing", "Observed"))
    tree_df <- tree_df[complete.cases(tree_df), , drop = FALSE]
    if(nrow(tree_df) >= 200 && length(unique(tree_df$miss_flag)) == 2) {
      tree_fit <- tryCatch(
        rpart(miss_flag ~ ., data = tree_df, method = "class",
              control = rpart.control(maxdepth = 2, minsplit = 100, cp = 0.01)),
        error = function(e) NULL
      )
      if(!is.null(tree_fit) && nrow(tree_fit$frame) > 1) {
        tree_has_splits <- TRUE
      }
    }
  }

  missing_pct <- round(mean(miss_ind) * 100, 2)

  mnar_risk <- FALSE
  judgement_basis <- "Observed-data checks only"
  if(v %in% mnar_sensitive_vars && missing_pct >= 40) {
    mnar_risk <- TRUE
    judgement_basis <- "Sensitive/self-report field with high missingness"
  }

  mech <- if(mnar_risk) {
    "Possible MNAR (needs SME validation)"
  } else if(sig_assoc > 0 || tree_has_splits) {
    "Likely MAR (observed-data relationship)"
  } else if(tests_run > 0) {
    "Could be MCAR"
  } else {
    "Insufficient evidence"
  }

  assessment_step5 <- rbind(assessment_step5, data.frame(
    Variable = v,
    Missing_Count = sum(miss_ind),
    Missing_Pct = missing_pct,
    Manual_Tests_Run = tests_run,
    Significant_Assoc = sig_assoc,
    Tree_Has_Splits = ifelse(tree_has_splits, "Yes", "No"),
    Likely_Mechanism = mech,
    MNAR_Risk = ifelse(mnar_risk, "Yes", "No"),
    Judgement_Basis = judgement_basis,
    stringsAsFactors = FALSE
  ))
}

if(nrow(assessment_step5) > 0) {
  assessment_step5 <- assessment_step5[order(assessment_step5$Missing_Pct, decreasing = TRUE), ]
  print(assessment_step5)
  cat("\nMechanism counts:\n")
  mechanism_counts_step5 <- table(assessment_step5$Likely_Mechanism)
  print(mechanism_counts_step5)

  cat("\nInterpretation guide:\n")
  cat("- MCAR: no detected relationship between missingness and observed variables.\n")
  cat("- MAR: missingness related to observed data (manual tests and/or tree split patterns).\n")
  cat("- MNAR: flagged as POSSIBLE for sensitive/high-missing fields; confirm with SME and sensitivity analysis.\n")
  cat("- Course note: judgment is prioritized over strict significance in this stage.\n")
} else {
  cat("No variables with missing values to assess.\n")
}
```

```
##                 Variable Missing_Count Missing_Pct Manual_Tests_Run
## 3      Educational_Level          6612       69.99               12
## 4   Favorite_Caps_Player          6612       69.99               12
## 5         Favorite_Sport          6612       69.99               12
## 7             Job_Sector          6612       69.99               12
## 9      Mode_Of_Transport          6612       69.99               12
## 16            Team_B_STH          6612       69.99               12
## 17            Team_C_STH          6612       69.99               12
## 10        Net_Worth_True          5762       60.99               12
## 6  HouseHold_Income_True          5691       60.24               12
## 2                  DistA          2895       30.64               11
## 8                Marital          1403       14.85               11
## 19      Marital_Original          1155       12.23               11
## 21   Marital_Original_MD          1155       12.23               11
## 1                    Age           986       10.44               11
## 13              Rep_Name           871        9.22               12
## 15                   Sex           667        7.06               11
## 18                Tenure           592        6.27               11
## 20      Flag_Tenure_High           592        6.27               11
## 14            Rep_Visits           508        5.38               11
## 12             Rep_Calls           433        4.58               11
## 11          Num_Children           406        4.30               12
##    Significant_Assoc Tree_Has_Splits                        Likely_Mechanism
## 3                  0              No    Possible MNAR (needs SME validation)
## 4                  0              No    Possible MNAR (needs SME validation)
## 5                  0              No    Possible MNAR (needs SME validation)
## 7                  0              No    Possible MNAR (needs SME validation)
## 9                  0              No    Possible MNAR (needs SME validation)
## 16                 0              No    Possible MNAR (needs SME validation)
## 17                 0              No    Possible MNAR (needs SME validation)
## 10                 0              No    Possible MNAR (needs SME validation)
## 6                  1              No    Possible MNAR (needs SME validation)
## 2                  5             Yes Likely MAR (observed-data relationship)
## 8                  1              No Likely MAR (observed-data relationship)
## 19                 0              No                           Could be MCAR
## 21                 0              No                           Could be MCAR
## 1                  3              No Likely MAR (observed-data relationship)
## 13                 5              No Likely MAR (observed-data relationship)
## 15                 3              No Likely MAR (observed-data relationship)
## 18                 0              No                           Could be MCAR
## 20                 0              No                           Could be MCAR
## 14                 1              No Likely MAR (observed-data relationship)
## 12                 3              No Likely MAR (observed-data relationship)
## 11                 5              No Likely MAR (observed-data relationship)
##    MNAR_Risk                                   Judgement_Basis
## 3        Yes Sensitive/self-report field with high missingness
## 4        Yes Sensitive/self-report field with high missingness
## 5        Yes Sensitive/self-report field with high missingness
## 7        Yes Sensitive/self-report field with high missingness
## 9        Yes Sensitive/self-report field with high missingness
## 16       Yes Sensitive/self-report field with high missingness
## 17       Yes Sensitive/self-report field with high missingness
## 10       Yes Sensitive/self-report field with high missingness
## 6        Yes Sensitive/self-report field with high missingness
## 2         No                         Observed-data checks only
## 8         No                         Observed-data checks only
## 19        No                         Observed-data checks only
## 21        No                         Observed-data checks only
## 1         No                         Observed-data checks only
## 13        No                         Observed-data checks only
## 15        No                         Observed-data checks only
## 18        No                         Observed-data checks only
## 20        No                         Observed-data checks only
## 14        No                         Observed-data checks only
## 12        No                         Observed-data checks only
## 11        No                         Observed-data checks only
## 
## Mechanism counts:
## 
##                           Could be MCAR Likely MAR (observed-data relationship) 
##                                       4                                       8 
##    Possible MNAR (needs SME validation) 
##                                       9 
## 
## Interpretation guide:
## - MCAR: no detected relationship between missingness and observed variables.
## - MAR: missingness related to observed data (manual tests and/or tree split patterns).
## - MNAR: flagged as POSSIBLE for sensitive/high-missing fields; confirm with SME and sensitivity analysis.
## - Course note: judgment is prioritized over strict significance in this stage.
```

**Step 5 Results and Interpretation:**

- Rows with at least one missing value: **9286** of **9447**
- Mechanism calls from assessment table:
  - Possible MNAR (needs SME validation): **9**
  - Likely MAR (observed-data relationship): **8**
  - Could be MCAR: **4**
  - Insufficient evidence: **0**
- Interpretation: this step explicitly accounts for **MCAR/MAR/MNAR** using manual checks, a small decision-tree screen, and domain judgment.

---

## Class 9: Implement Basic Imputation Methods - Step 6

**Step 6: Apply Simple Imputation Techniques**


``` r
cat("=== STEP 6: APPLY SIMPLE TECHNIQUES ===\n\n")
```

```
## === STEP 6: APPLY SIMPLE TECHNIQUES ===
```

``` r
# Create simple-imputation working copy (do not overwrite MDdf)
MDdf_simple <- MDdf

# Missingness tiers for simple-technique decisions
missing_pct_step6 <- colSums(is.na(MDdf_simple)) / nrow(MDdf_simple)
small_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0 & missing_pct_step6 <= 0.10])
moderate_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0.10 & missing_pct_step6 <= 0.40])
high_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0.40])

cat("Small-missing vars (<=10%):", length(small_missing_vars_step6), "\n")
```

```
## Small-missing vars (<=10%): 7
```

``` r
cat("Moderate-missing vars (10-40%):", length(moderate_missing_vars_step6), "\n")
```

```
## Moderate-missing vars (10-40%): 5
```

``` r
cat("High-missing vars (>40%):", length(high_missing_vars_step6), "\n\n")
```

```
## High-missing vars (>40%): 9
```

``` r
# Apply simple methods first to low-missing variables
mode_value_step6 <- function(x) {
  ux <- unique(x[!is.na(x)])
  ux[which.max(tabulate(match(x, ux)))]
}

imputation_log_step6 <- data.frame(
  Variable = character(),
  Missing_Before = integer(),
  Method = character(),
  Missing_After = integer(),
  stringsAsFactors = FALSE
)

vars_for_simple_step6 <- unique(c(
  small_missing_vars_step6,
  intersect(moderate_missing_vars_step6,
            c("DistA", "Age", "Marital", "Rep_Name", "Sex", "Tenure", "Rep_Visits", "Rep_Calls", "Num_Children"))
))

for(v in vars_for_simple_step6) {
  before_n <- sum(is.na(MDdf_simple[[v]]))
  if(before_n == 0) next

  method_used <- ""
  if(is.numeric(MDdf_simple[[v]])) {
    fill <- median(MDdf_simple[[v]], na.rm = TRUE)
    MDdf_simple[[v]][is.na(MDdf_simple[[v]])] <- fill
    method_used <- "Median"
  } else if(is.factor(MDdf_simple[[v]]) || is.character(MDdf_simple[[v]])) {
    fill <- mode_value_step6(MDdf_simple[[v]])
    MDdf_simple[[v]][is.na(MDdf_simple[[v]])] <- fill
    if(is.factor(MDdf_simple[[v]])) MDdf_simple[[v]] <- droplevels(MDdf_simple[[v]])
    method_used <- "Mode"
  }

  after_n <- sum(is.na(MDdf_simple[[v]]))
  imputation_log_step6 <- rbind(imputation_log_step6, data.frame(
    Variable = v,
    Missing_Before = before_n,
    Method = method_used,
    Missing_After = after_n,
    stringsAsFactors = FALSE
  ))
}

remaining_missing_by_var_step6 <- sort(colSums(is.na(MDdf_simple)), decreasing = TRUE)
remaining_missing_by_var_step6 <- remaining_missing_by_var_step6[remaining_missing_by_var_step6 > 0]

cat("Simple methods applied to variables:", length(vars_for_simple_step6), "\n")
```

```
## Simple methods applied to variables: 10
```

``` r
cat("Variables with remaining missingness:", length(remaining_missing_by_var_step6), "\n")
```

```
## Variables with remaining missingness: 11
```

``` r
cat("Total remaining missing values:", sum(is.na(MDdf_simple)), "\n\n")
```

```
## Total remaining missing values: 60047
```

``` r
if(nrow(imputation_log_step6) > 0) {
  cat("Imputation log (simple methods):\n")
  print(imputation_log_step6)
}
```

```
## Imputation log (simple methods):
##            Variable Missing_Before Method Missing_After
## 1      Num_Children            406 Median             0
## 2         Rep_Calls            433 Median             0
## 3          Rep_Name            871   Mode             0
## 4        Rep_Visits            508 Median             0
## 5               Sex            667   Mode             0
## 6            Tenure            592 Median             0
## 7  Flag_Tenure_High            592 Median             0
## 8               Age            986 Median             0
## 9             DistA           2895 Median             0
## 10          Marital           1403   Mode             0
```

``` r
if(length(remaining_missing_by_var_step6) > 0) {
  cat("\nTop remaining missing variables (for complex methods/MICE later):\n")
  print(head(remaining_missing_by_var_step6, 10))
}
```

```
## 
## Top remaining missing variables (for complex methods/MICE later):
##     Educational_Level  Favorite_Caps_Player        Favorite_Sport 
##                  6612                  6612                  6612 
##            Job_Sector     Mode_Of_Transport            Team_B_STH 
##                  6612                  6612                  6612 
##            Team_C_STH        Net_Worth_True HouseHold_Income_True 
##                  6612                  5762                  5691 
##      Marital_Original 
##                  1155
```

``` r
cat("\nNote: Step 6 applies simple univariate methods first; remaining high-missing variables are intentionally left for complex imputation.\n")
```

```
## 
## Note: Step 6 applies simple univariate methods first; remaining high-missing variables are intentionally left for complex imputation.
```

**Step 6 Results and Interpretation:**

- Variables handled with simple methods: **10**
- Remaining missing values after simple techniques: **60,047**
- Variables still requiring complex methods: **11**
- Interpretation: simple techniques are applied where appropriate (low/moderate missingness), while high-missing variables are preserved for advanced imputation.

### Missing Data Steps 1-6 Summary


```
## === MISSING DATA SUMMARY (Steps 1-6) ===
```

```
## Step 1: Missing values identified - 20 variables with NA and 69,400 missing cells.
```

```
## Step 2: Missingness indicator variables created - 20 indicator columns.
```

```
## Step 3: Obvious coding issues cleaned - 0 blank values and 248 Marital='U' values converted to NA.
```

```
## Step 4: Row/column decisions marked - 9 columns >40% missing and 0 rows >50% missing (no hard deletes).
```

```
## Step 5: Missingness mechanism assessed - 9 possible MNAR, 8 likely MAR, 4 could be MCAR.
```

```
## Step 6: Simple techniques applied - 10 variables imputed with simple methods; remaining missing values: 60,047 .
```

# Keep a draft modeling copy after simple techniques

```
## Draft modeling file created in-session: MDdf_draft_modeling
```

```
## Dimensions: 9447 x 80
```

```
## Remaining missing values in draft modeling file: 60047
```

