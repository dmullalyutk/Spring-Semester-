}
}
}
}
return(nzv_info)
}
# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable
if(length(nzv_variables) > 0) {
cat("Near-zero variance variables to monitor:\n")
print(nzv_variables)
cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
cat("No significant near-zero variance issues found\n")
}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
# Check if they're perfectly correlated
state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")
# If one-to-one mapping, they're redundant
cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
cat("Removing State_Loc (keeping State_Name)\n")
excluded_cols <- c(excluded_cols, "State_Loc")
FFdf$State_Loc <- NULL
}
# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]
if(length(numeric_vars) > 1) {
# Calculate correlation matrix (handling NAs)
cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")
# Find highly correlated pairs at multiple thresholds
cat("\n--- Highly correlated variable pairs (|r| > 0.85) ---\n")
cat("These pairs may cause multicollinearity in regression models\n\n")
high_cor_85 <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor_85) > 0) {
cor_pairs <- data.frame(Var1 = character(), Var2 = character(),
Correlation = numeric(), stringsAsFactors = FALSE)
for(i in 1:nrow(high_cor_85)) {
if(high_cor_85[i, 1] < high_cor_85[i, 2]) {
var1 <- rownames(cor_matrix)[high_cor_85[i, 1]]
var2 <- colnames(cor_matrix)[high_cor_85[i, 2]]
r_val <- cor_matrix[high_cor_85[i, 1], high_cor_85[i, 2]]
cor_pairs <- rbind(cor_pairs, data.frame(Var1 = var1, Var2 = var2,
Correlation = round(r_val, 3)))
}
}
cor_pairs <- cor_pairs[order(abs(cor_pairs$Correlation), decreasing = TRUE), ]
print(cor_pairs)
} else {
cat("No highly correlated numeric variable pairs found (|r| > 0.85)\n")
}
}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
# Subset to variables with fewer missing values for cleaner plot
complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]
if(length(complete_vars) > 2) {
cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
corrplot(cor_subset, method = "color", type = "upper",
tl.cex = 0.7, tl.col = "black",
title = "Correlation Matrix - Numeric Variables",
mar = c(0, 0, 2, 0))
}
}
# Address multicollinearity based on correlation matrix analysis
cat("=== MULTICOLLINEARITY ANALYSIS ===\n\n")
# Identify correlated variable clusters from the correlation matrix
cat("Cluster 1: Spending & Visit variables\n")
cat("  - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)\n")
cat("  - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)\n")
cat("  Recommendation: Consider removing Rep_Visits or Total_Spent\n\n")
cat("Cluster 2: Concession & Seating\n")
cat("  - Concession_Total <-> NumSeats: r = 0.915 (strong positive)\n")
cat("  Recommendation: Makes business sense - more seats = more concessions\n\n")
cat("Cluster 3: Attendance pairs\n")
cat("  - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)\n")
cat("  Note: Negative correlation suggests inverse relationship\n")
cat("  Recommendation: Keep both - they capture different behaviors\n\n")
# Create list of variables to flag for multicollinearity
multicollinear_vars <- c("Rep_Visits", "Team_Store_Total")
cat("Variables flagged for potential removal due to multicollinearity:\n")
print(multicollinear_vars)
# Option: Remove highly correlated variables (uncomment to execute)
# FFdf <- FFdf[, !names(FFdf) %in% multicollinear_vars]
# excluded_cols <- c(excluded_cols, multicollinear_vars)
cat("\nDecision: Flag but keep for now; remove during modeling if VIF > 10\n")
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))
# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")
# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")
# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)
# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)
# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")
# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)
par(mfrow = c(1, 1))
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")
# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
"(", round(total_missing/total_cells * 100, 2), "%)\n\n")
# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)
cat("Columns with missing values:\n")
na_summary <- data.frame(
Variable = names(na_by_col),
Missing_Count = as.numeric(na_by_col),
Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")
# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")
# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")
# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")
# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)
cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
# Prepare data for decision tree
# Exclude ID columns, flag variables, and high-cardinality variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
"Marital_Original", "Last_Contact", "Contact_Year",
"Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour",
"State_Name", "Zip_Codes")
tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]
# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]
# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)
# Build a simple decision tree (max 2-3 splits for sanity check)
# Uses Region variable (from Step 6) instead of high-cardinality State_Name/Zip_Codes
sanity_tree <- rpart(Y01 ~ .,
data = tree_data,
method = "class",
control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))
# Plot the tree
rpart.plot(sanity_tree,
main = "Sanity Check Decision Tree (max depth = 3)",
extra = 104,  # Show percentage and count
box.palette = "RdYlGn")
# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
print(head(var_imp, 10))
} else {
cat("No variables selected by the tree\n")
}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")
# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")
if(accuracy > 0.90) {
cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")
cat("Step 6 - Handle Categorical Variables:\n")
cat("  - Rare factor levels (< 5%) lumped into 'Other' via fct_lump_prop()\n")
cat("  - Marital 'U' kept as separate category\n")
cat("  - State_Name grouped into US Census regions\n\n")
cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")
cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n")
cat("  - Decision: Keep for now but monitor during modeling\n\n")
cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n")
cat("  - Correlation matrix reviewed for multicollinearity\n\n")
cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n")
cat("  - Missing data summary table generated\n\n")
cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Tree accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Review variable importance for potential data leakage\n\n")
cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
# Save cleaned dataset for next phase (Missing Data)
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
cat("=== STEP 1: IDENTIFY MISSING DATA ===\n\n")
# Start Missing Data workflow from cleaned dataset
MDdf <- FFdf
missing_counts_step1 <- colSums(is.na(MDdf))
missing_counts_step1 <- sort(missing_counts_step1[missing_counts_step1 > 0], decreasing = TRUE)
missing_table_step1 <- data.frame(
Variable = names(missing_counts_step1),
Missing_Count = as.numeric(missing_counts_step1),
Missing_Pct = round(as.numeric(missing_counts_step1) / nrow(MDdf) * 100, 2)
)
cat("Variables with missing values:", length(missing_counts_step1), "\n")
cat("Total missing cells:", sum(is.na(MDdf)), "\n\n")
print(missing_table_step1)
# Optional pattern table from mice
md_pattern_step1 <- tryCatch(mice::md.pattern(MDdf, plot = FALSE), error = function(e) NULL)
if(!is.null(md_pattern_step1)) {
cat("\nTop missingness patterns (mice::md.pattern):\n")
print(head(md_pattern_step1, 8))
}
cat("=== STEP 2: MARK MISSING DATA ===\n\n")
# Create indicator columns for variables with missingness
vars_with_na_step2 <- names(MDdf)[colSums(is.na(MDdf)) > 0]
indicator_names_step2 <- paste0(vars_with_na_step2, "_Missing")
for(v in vars_with_na_step2) {
MDdf[[paste0(v, "_Missing")]] <- ifelse(is.na(MDdf[[v]]), 1, 0)
}
cat("Missingness indicators created:", length(vars_with_na_step2), "\n")
cat("Example indicators:\n")
print(head(indicator_names_step2, 10))
cat("\nIdentifier columns retained:",
all(c("ID", "Cust_ID") %in% names(MDdf)), "\n")
cat("=== STEP 4: ROW/COLUMN DECISIONS ===\n\n")
# Missingness by column
na_pct_step4 <- colSums(is.na(MDdf)) / nrow(MDdf)
high_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.40])
extreme_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.65])
cat("Columns > 40% missing:", length(high_missing_cols_step4), "\n")
if(length(high_missing_cols_step4) > 0) {
print(high_missing_cols_step4)
} else {
cat("No columns exceed 40% missingness threshold.\n")
}
# Missingness by row
row_missing_pct_step4 <- rowSums(is.na(MDdf)) / ncol(MDdf)
rows_high_missing_step4 <- which(row_missing_pct_step4 > 0.50)
cat("\nRows > 50% missing:", length(rows_high_missing_step4), "\n")
# Mark (do not delete) for modeling decisions
excluded_cols_md_step4 <- unique(c("ID", "Cust_ID", extreme_missing_cols_step4))
cat("Columns marked for potential exclusion from modeling (not deleted):\n")
print(excluded_cols_md_step4)
cat("Decision: mark rows/columns for modeling strategy; avoid hard deletes at this stage.\n")
cat("=== STEP 4: ROW/COLUMN DECISIONS ===\n\n")
# Missingness by column
na_pct_step4 <- colSums(is.na(MDdf)) / nrow(MDdf)
high_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.40])
extreme_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.65])
cat("Columns > 40% missing:", length(high_missing_cols_step4), "\n")
if(length(high_missing_cols_step4) > 0) {
print(high_missing_cols_step4)
} else {
cat("No columns exceed 40% missingness threshold.\n")
}
# Missingness by row
row_missing_pct_step4 <- rowSums(is.na(MDdf)) / ncol(MDdf)
rows_high_missing_step4 <- which(row_missing_pct_step4 > 0.50)
cat("\nRows > 50% missing:", length(rows_high_missing_step4), "\n")
# Mark (do not delete) for modeling decisions
excluded_cols_md_step4 <- unique(c("ID", "Cust_ID", extreme_missing_cols_step4))
cat("Columns marked for potential exclusion from modeling (not deleted):\n")
print(excluded_cols_md_step4)
cat("Decision: mark rows/columns for modeling strategy; avoid hard deletes at this stage.\n")
cat("=== STEP 5: ASSESS MISSINGNESS PATTERNS ===\n\n")
# Pairwise missingness summary
missing_matrix_step5 <- is.na(MDdf)
cat("Any complete rows:", any(rowSums(missing_matrix_step5) == 0), "\n")
cat("Rows with at least one missing value:", sum(rowSums(missing_matrix_step5) > 0), "\n")
# Bar chart of top missing variables
na_count_step5 <- sort(colSums(missing_matrix_step5), decreasing = TRUE)
na_count_step5 <- na_count_step5[na_count_step5 > 0]
if(length(na_count_step5) > 0) {
top_na_step5 <- head(na_count_step5, 15)
barplot(top_na_step5,
las = 2,
col = "steelblue",
main = "Top Variables by Missing Count",
ylab = "Missing Count")
} else {
plot.new()
text(0.5, 0.5, "No missing values detected")
}
cat("\n=== MCAR / MAR / MNAR ASSESSMENT (PRACTICAL) ===\n\n")
# Practical mechanism check:
# - Manual association tests + simple decision tree used for judgment (per course guidance)
# - If missingness relates to observed data -> likely MAR
# - If no relationships detected -> could be MCAR
# - MNAR is flagged as possible for sensitive/high-missing variables, then SME validated
vars_with_na_step5 <- names(MDdf)[colSums(is.na(MDdf)) > 0]
assessment_step5 <- data.frame(
Variable = character(),
Missing_Count = integer(),
Missing_Pct = numeric(),
Manual_Tests_Run = integer(),
Significant_Assoc = integer(),
Tree_Has_Splits = character(),
Likely_Mechanism = character(),
MNAR_Risk = character(),
Judgement_Basis = character(),
stringsAsFactors = FALSE
)
# Variables where nonresponse may depend on latent/unobserved value (domain risk)
mnar_sensitive_vars <- c(
"Educational_Level", "Favorite_Caps_Player", "Favorite_Sport",
"HouseHold_Income_True", "Job_Sector", "Mode_Of_Transport",
"Net_Worth_True", "Team_B_STH", "Team_C_STH"
)
tree_predictors_step5 <- intersect(c(
"Age", "Tenure", "Total_Spent", "NumSeats", "Survey_Comp", "DistA",
"Sex", "Marital", "Account_Type", "Region", "Rep_Visits", "Rep_Calls"
), names(MDdf))
for(v in vars_with_na_step5) {
miss_ind <- is.na(MDdf[[v]])
if(sum(miss_ind) == 0 || sum(!miss_ind) == 0) next
tests_run <- 0
sig_assoc <- 0
# Manual tests against a stable predictor set
candidate_preds <- setdiff(tree_predictors_step5, v)
for(p in candidate_preds) {
pred <- MDdf[[p]]
if(all(is.na(pred))) next
ok <- !is.na(pred)
if(sum(ok) < 30) next
pval <- NA
if(is.numeric(pred)) {
# Numeric predictor vs missingness group
if(length(unique(miss_ind[ok])) == 2) {
pval <- tryCatch(wilcox.test(pred[ok] ~ miss_ind[ok])$p.value, error = function(e) NA)
}
} else if(is.factor(pred) || is.character(pred) || is.logical(pred)) {
# Categorical predictor vs missingness group
tab <- table(miss_ind[ok], pred[ok])
if(all(dim(tab) >= c(2, 2))) {
pval <- tryCatch(suppressWarnings(chisq.test(tab)$p.value), error = function(e) NA)
}
}
if(!is.na(pval)) {
tests_run <- tests_run + 1
if(pval < 0.05) sig_assoc <- sig_assoc + 1
}
}
# Decision tree check for non-random missingness structure
tree_has_splits <- FALSE
tree_df <- MDdf[, c(setdiff(tree_predictors_step5, v)), drop = FALSE]
if(ncol(tree_df) > 0) {
tree_df$miss_flag <- as.factor(ifelse(is.na(MDdf[[v]]), "Missing", "Observed"))
tree_df <- tree_df[complete.cases(tree_df), , drop = FALSE]
if(nrow(tree_df) >= 200 && length(unique(tree_df$miss_flag)) == 2) {
tree_fit <- tryCatch(
rpart(miss_flag ~ ., data = tree_df, method = "class",
control = rpart.control(maxdepth = 2, minsplit = 100, cp = 0.01)),
error = function(e) NULL
)
if(!is.null(tree_fit) && nrow(tree_fit$frame) > 1) {
tree_has_splits <- TRUE
}
}
}
missing_pct <- round(mean(miss_ind) * 100, 2)
mnar_risk <- FALSE
judgement_basis <- "Observed-data checks only"
if(v %in% mnar_sensitive_vars && missing_pct >= 40) {
mnar_risk <- TRUE
judgement_basis <- "Sensitive/self-report field with high missingness"
}
mech <- if(mnar_risk) {
"Possible MNAR (needs SME validation)"
} else if(sig_assoc > 0 || tree_has_splits) {
"Likely MAR (observed-data relationship)"
} else if(tests_run > 0) {
"Could be MCAR"
} else {
"Insufficient evidence"
}
assessment_step5 <- rbind(assessment_step5, data.frame(
Variable = v,
Missing_Count = sum(miss_ind),
Missing_Pct = missing_pct,
Manual_Tests_Run = tests_run,
Significant_Assoc = sig_assoc,
Tree_Has_Splits = ifelse(tree_has_splits, "Yes", "No"),
Likely_Mechanism = mech,
MNAR_Risk = ifelse(mnar_risk, "Yes", "No"),
Judgement_Basis = judgement_basis,
stringsAsFactors = FALSE
))
}
if(nrow(assessment_step5) > 0) {
assessment_step5 <- assessment_step5[order(assessment_step5$Missing_Pct, decreasing = TRUE), ]
print(assessment_step5)
cat("\nMechanism counts:\n")
mechanism_counts_step5 <- table(assessment_step5$Likely_Mechanism)
print(mechanism_counts_step5)
cat("\nInterpretation guide:\n")
cat("- MCAR: no detected relationship between missingness and observed variables.\n")
cat("- MAR: missingness related to observed data (manual tests and/or tree split patterns).\n")
cat("- MNAR: flagged as POSSIBLE for sensitive/high-missing fields; confirm with SME and sensitivity analysis.\n")
cat("- Course note: judgment is prioritized over strict significance in this stage.\n")
} else {
cat("No variables with missing values to assess.\n")
}
cat("=== STEP 6: APPLY SIMPLE TECHNIQUES ===\n\n")
# Create simple-imputation working copy (do not overwrite MDdf)
MDdf_simple <- MDdf
# Missingness tiers for simple-technique decisions
missing_pct_step6 <- colSums(is.na(MDdf_simple)) / nrow(MDdf_simple)
small_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0 & missing_pct_step6 <= 0.10])
moderate_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0.10 & missing_pct_step6 <= 0.40])
high_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0.40])
cat("Small-missing vars (<=10%):", length(small_missing_vars_step6), "\n")
cat("Moderate-missing vars (10-40%):", length(moderate_missing_vars_step6), "\n")
cat("High-missing vars (>40%):", length(high_missing_vars_step6), "\n\n")
# Apply simple methods first to low-missing variables
mode_value_step6 <- function(x) {
ux <- unique(x[!is.na(x)])
ux[which.max(tabulate(match(x, ux)))]
}
imputation_log_step6 <- data.frame(
Variable = character(),
Missing_Before = integer(),
Method = character(),
Missing_After = integer(),
stringsAsFactors = FALSE
)
vars_for_simple_step6 <- unique(c(
small_missing_vars_step6,
intersect(moderate_missing_vars_step6,
c("DistA", "Age", "Marital", "Rep_Name", "Sex", "Tenure", "Rep_Visits", "Rep_Calls", "Num_Children"))
))
for(v in vars_for_simple_step6) {
before_n <- sum(is.na(MDdf_simple[[v]]))
if(before_n == 0) next
method_used <- ""
if(is.numeric(MDdf_simple[[v]])) {
fill <- median(MDdf_simple[[v]], na.rm = TRUE)
MDdf_simple[[v]][is.na(MDdf_simple[[v]])] <- fill
method_used <- "Median"
} else if(is.factor(MDdf_simple[[v]]) || is.character(MDdf_simple[[v]])) {
fill <- mode_value_step6(MDdf_simple[[v]])
MDdf_simple[[v]][is.na(MDdf_simple[[v]])] <- fill
if(is.factor(MDdf_simple[[v]])) MDdf_simple[[v]] <- droplevels(MDdf_simple[[v]])
method_used <- "Mode"
}
after_n <- sum(is.na(MDdf_simple[[v]]))
imputation_log_step6 <- rbind(imputation_log_step6, data.frame(
Variable = v,
Missing_Before = before_n,
Method = method_used,
Missing_After = after_n,
stringsAsFactors = FALSE
))
}
remaining_missing_by_var_step6 <- sort(colSums(is.na(MDdf_simple)), decreasing = TRUE)
remaining_missing_by_var_step6 <- remaining_missing_by_var_step6[remaining_missing_by_var_step6 > 0]
cat("Simple methods applied to variables:", length(vars_for_simple_step6), "\n")
cat("Variables with remaining missingness:", length(remaining_missing_by_var_step6), "\n")
cat("Total remaining missing values:", sum(is.na(MDdf_simple)), "\n\n")
if(nrow(imputation_log_step6) > 0) {
cat("Imputation log (simple methods):\n")
print(imputation_log_step6)
}
if(length(remaining_missing_by_var_step6) > 0) {
cat("\nTop remaining missing variables (for complex methods/MICE later):\n")
print(head(remaining_missing_by_var_step6, 10))
}
cat("\nNote: Step 6 applies simple univariate methods first; remaining high-missing variables are intentionally left for complex imputation.\n")
cat("=== MISSING DATA SUMMARY (Steps 1-6) ===\n\n")
cat("Step 1: Missing values identified -", length(missing_counts_step1), "variables with NA and",
format(sum(is.na(MDdf)), big.mark = ","), "missing cells.\n")
cat("Step 2: Missingness indicator variables created -", length(vars_with_na_step2), "indicator columns.\n")
cat("Step 3: Obvious coding issues cleaned -", added_na_from_blanks_step3,
"blank values and", marital_u_to_na_step3, "Marital='U' values converted to NA.\n")
cat("Step 4: Row/column decisions marked -", length(high_missing_cols_step4),
"columns >40% missing and", length(rows_high_missing_step4), "rows >50% missing (no hard deletes).\n")
cat("Step 5: Missingness mechanism assessed -",
sum(assessment_step5$Likely_Mechanism == "Possible MNAR (needs SME validation)"), "possible MNAR,",
sum(assessment_step5$Likely_Mechanism == "Likely MAR (observed-data relationship)"), "likely MAR,",
sum(assessment_step5$Likely_Mechanism == "Could be MCAR"), "could be MCAR.\n")
cat("Step 6: Simple techniques applied -", nrow(imputation_log_step6),
"variables imputed with simple methods; remaining missing values:",
format(sum(is.na(MDdf_simple)), big.mark = ","), ".\n")
View(MDdf)
View(MDdf_simple)
