cat("\nTree accuracy (with Region):", round(accuracy_region * 100, 2), "%\n")
# Setup Chunk - Load Required Libraries
library(mice)
library(VIM)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(corrplot)
library(lubridate)
library(flextable)
library(rpart)       # Decision Tree package
library(rpart.plot)  # Visualization of Decision Trees
setwd("C:/Users/david/Desktop/Spring-Semester-/Missing Data")
# Load the raw data files
MainDF <- read.csv("Raw.csv")
StoreDF <- read.csv("StoreTable.csv")
ConcessDF <- read.csv("ConcessTable.csv")
CustomerDF <- read.csv("CustomerTable.csv")
# Check dimensions of each file
cat("MainDF:", dim(MainDF), "| StoreDF:", dim(StoreDF),
"| ConcessDF:", dim(ConcessDF), "| CustomerDF:", dim(CustomerDF), "\n")
# Check for duplicate Cust_IDs in each source table BEFORE merging
cat("Duplicates in MainDF:", sum(duplicated(MainDF$Cust_ID)), "\n")
cat("Duplicates in StoreDF:", sum(duplicated(StoreDF$Cust_ID)), "\n")
cat("Duplicates in ConcessDF:", sum(duplicated(ConcessDF$Cust_ID)), "\n")
cat("Duplicates in CustomerDF:", sum(duplicated(CustomerDF$Cust_ID)), "\n")
# Create flat file by merging all tables on Cust_ID
FFdf <- MainDF
FFdf <- merge(FFdf, StoreDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, ConcessDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, CustomerDF, by = "Cust_ID", all.x = TRUE)
cat("Final flat file dimensions - Rows:", nrow(FFdf), "Columns:", ncol(FFdf), "\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "| Duplicate rows:", nrow(FFdf) - length(unique(FFdf$Cust_ID)), "\n")
# Create an identifier variable
FFdf$ID <- 1:nrow(FFdf)
# Reorder columns: Y-variable first, then alphabetically
y_var <- "Y01"
other_vars <- sort(setdiff(names(FFdf), y_var))
FFdf <- FFdf[, c(y_var, other_vars)]
# Histograms for key numeric variables
par(mfrow = c(2, 2))
hist(FFdf$Age, main = "Distribution of Age", xlab = "Age", col = "steelblue", breaks = 20)
hist(FFdf$tenure, main = "Distribution of Tenure", xlab = "Tenure (Years)", col = "steelblue", breaks = 20)
hist(FFdf$NumSeats, main = "Distribution of NumSeats", xlab = "Number of Seats", col = "steelblue")
hist(FFdf$Total.Spent, main = "Distribution of Total Spent", xlab = "Total Spent ($)", col = "steelblue", breaks = 30)
par(mfrow = c(1, 1))
# Response variable distribution
cat("Response Variable (Y01):", table(FFdf$Y01), "\n")
barplot(table(FFdf$Y01), main = "Distribution of Y01 (Response Variable)",
col = c("coral", "steelblue"), names.arg = c("0 (No)", "1 (Yes)"))
# Clean up variable names - replace dots/spaces with underscores, capitalize
names(FFdf) <- gsub("\\.", "_", names(FFdf))
names(FFdf) <- gsub(" ", "_", names(FFdf))
names(FFdf) <- gsub("(^|_)([a-z])", "\\1\\U\\2", names(FFdf), perl = TRUE)
cat("Dataset:", nrow(FFdf), "rows,", ncol(FFdf), "columns\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "out of", nrow(FFdf), "rows\n")
# Identify character variables
chr_vars <- names(FFdf)[sapply(FFdf, is.character)]
cat("Character variables to convert:", length(chr_vars), "\n")
# Convert character variables to factors
for(var in chr_vars) {
FFdf[[var]] <- as.factor(FFdf[[var]])
}
# Show key factor levels
cat("\nSex levels:", levels(FFdf$Sex), "\n")
cat("Marital levels:", levels(FFdf$Marital), "\n")
cat("Account_Type levels:", levels(FFdf$Account_Type), "\n")
# Check numeric variable ranges
cat("Age range:", range(FFdf$Age, na.rm = TRUE), "\n")
cat("Tenure range:", range(FFdf$Tenure, na.rm = TRUE), "\n")
cat("NumSeats range:", range(FFdf$NumSeats, na.rm = TRUE), "\n")
# Check for 999 placeholder values in DistA
cat("\nDistA values = 999:", sum(FFdf$DistA == 999, na.rm = TRUE), "\n")
# Convert 999 to NA (placeholder for missing)
FFdf$DistA[FFdf$DistA == 999] <- NA
cat("DistA NA count after conversion:", sum(is.na(FFdf$DistA)), "\n")
# Check Survey_Comp for outliers (expected range 0-1)
cat("\nSurvey_Comp range:", range(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("Survey_Comp values > 1:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
# Check for redundant columns
cat("\nState_Name unique:", length(unique(FFdf$State_Name)),
"| State_Loc unique:", length(unique(FFdf$State_Loc)), "\n")
# Convert Last_Contact to datetime format
FFdf$Last_Contact <- ymd_hms(as.character(FFdf$Last_Contact))
# Extract date components
FFdf$Contact_Year <- year(FFdf$Last_Contact)
FFdf$Contact_Month <- month(FFdf$Last_Contact)
FFdf$Contact_Day <- day(FFdf$Last_Contact)
FFdf$Contact_Weekday <- wday(FFdf$Last_Contact, label = TRUE)
FFdf$Contact_Hour <- hour(FFdf$Last_Contact)
cat("Date components extracted: Contact_Year, Contact_Month, Contact_Day, Contact_Weekday, Contact_Hour\n")
cat("Contact Year range:", range(FFdf$Contact_Year, na.rm = TRUE), "\n")
cat("Contact Hour range:", range(FFdf$Contact_Hour, na.rm = TRUE), "\n")
cat("Final Dataset:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
cat("Total Missing Values:", sum(is.na(FFdf)), "\n")
# Show columns with missing values
na_counts <- colSums(is.na(FFdf))
na_cols <- na_counts[na_counts > 0]
cat("\nColumns with missing values:\n")
print(sort(na_cols, decreasing = TRUE))
# Get all factor variables
factor_vars <- names(FFdf)[sapply(FFdf, is.factor)]
cat("Factor variables:", length(factor_vars), "\n")
# Function to check for rare levels (less than 5% of data)
check_rare_levels <- function(df, threshold = 0.05) {
factor_vars <- names(df)[sapply(df, is.factor)]
rare_info <- list()
for(var in factor_vars) {
tbl <- table(df[[var]], useNA = "ifany")
pct <- prop.table(tbl)
rare_levels <- names(pct[pct < threshold])
if(length(rare_levels) > 0) {
rare_info[[var]] <- data.frame(
Level = rare_levels,
Count = as.numeric(tbl[rare_levels]),
Percent = round(as.numeric(pct[rare_levels]) * 100, 2)
)
}
}
return(rare_info)
}
# Check for rare levels
rare_levels <- check_rare_levels(FFdf, threshold = 0.05)
cat("\nVariables with rare levels (< 5%):\n")
print(names(rare_levels))
# Display level distributions for key categorical variables
cat("\n--- Sex Distribution ---\n")
print(table(FFdf$Sex, useNA = "ifany"))
cat("\n--- Marital Distribution ---\n")
print(table(FFdf$Marital, useNA = "ifany"))
cat("\n--- Account_Type Distribution ---\n")
print(table(FFdf$Account_Type, useNA = "ifany"))
cat("\n--- Educational_Level Distribution ---\n")
print(table(FFdf$Educational_Level, useNA = "ifany"))
# Handle Marital "U" (Unknown) - treat as NA for analysis purposes
# Create backup first
FFdf$Marital_Original <- FFdf$Marital
# Option: Convert "U" to NA (uncomment if desired)
# FFdf$Marital[FFdf$Marital == "U"] <- NA
# FFdf$Marital <- droplevels(FFdf$Marital)
cat("Marital 'U' (Unknown) count:", sum(FFdf$Marital == "U", na.rm = TRUE), "\n")
cat("Decision: Keep 'U' as separate level for now - may represent meaningful unknown status\n")
# Check for levels that might mean the same thing
cat("\n--- State_Name levels ---\n")
print(length(levels(FFdf$State_Name)))
cat("Number of unique states:", length(unique(FFdf$State_Name)), "\n")
# Group states into US Census Bureau regions to reduce cardinality
# This addresses the high-cardinality issue identified in Step 11
# Define region mappings
northeast <- c("Connecticut", "Maine", "Massachusetts", "New Hampshire",
"Rhode Island", "Vermont", "New Jersey", "New York", "Pennsylvania")
midwest <- c("Illinois", "Indiana", "Michigan", "Ohio", "Wisconsin",
"Iowa", "Kansas", "Minnesota", "Missouri", "Nebraska",
"North Dakota", "South Dakota")
south <- c("Delaware", "Florida", "Georgia", "Maryland", "North Carolina",
"South Carolina", "Virginia", "District of Columbia", "West Virginia",
"Alabama", "Kentucky", "Mississippi", "Tennessee",
"Arkansas", "Louisiana", "Oklahoma", "Texas")
west <- c("Arizona", "Colorado", "Idaho", "Montana", "Nevada", "New Mexico",
"Utah", "Wyoming", "Alaska", "California", "Hawaii", "Oregon", "Washington")
# Create Region variable
FFdf$Region <- case_when(
FFdf$State_Name %in% northeast ~ "Northeast",
FFdf$State_Name %in% midwest ~ "Midwest",
FFdf$State_Name %in% south ~ "South",
FFdf$State_Name %in% west ~ "West",
TRUE ~ "Other"  # Catch any unmatched states
)
# Convert to factor
FFdf$Region <- as.factor(FFdf$Region)
# Display region distribution
cat("--- Region Distribution (grouped from State_Name) ---\n")
print(table(FFdf$Region, useNA = "ifany"))
cat("\nRegion percentages:\n")
print(round(prop.table(table(FFdf$Region)) * 100, 2))
# Check if any states weren't mapped
unmatched_states <- unique(FFdf$State_Name[FFdf$Region == "Other"])
if(length(unmatched_states) > 0) {
cat("\nWarning - Unmatched states assigned to 'Other':\n")
print(unmatched_states)
} else {
cat("\nAll states successfully mapped to regions.\n")
}
# Function to identify zero-variance columns
find_zero_variance <- function(df) {
zv_cols <- c()
for(col in names(df)) {
unique_vals <- length(unique(na.omit(df[[col]])))
if(unique_vals <= 1) {
zv_cols <- c(zv_cols, col)
}
}
return(zv_cols)
}
# Find zero-variance columns
zero_var_cols <- find_zero_variance(FFdf)
cat("Zero-variance columns found:", length(zero_var_cols), "\n")
if(length(zero_var_cols) > 0) {
cat("Columns with zero variance:\n")
print(zero_var_cols)
# Store in excluded columns list
excluded_cols <- zero_var_cols
# Remove zero-variance columns
FFdf <- FFdf[, !names(FFdf) %in% zero_var_cols]
cat("\nRemoved", length(zero_var_cols), "zero-variance columns\n")
} else {
cat("No zero-variance columns found\n")
excluded_cols <- c()
}
cat("Remaining columns:", ncol(FFdf), "\n")
# Function to find near-zero variance columns
# A column is NZV if one value dominates (e.g., >95% of values)
find_near_zero_variance <- function(df, threshold = 0.95) {
nzv_info <- data.frame(
Variable = character(),
DominantValue = character(),
DominantPct = numeric(),
UniqueValues = integer(),
stringsAsFactors = FALSE
)
for(col in names(df)) {
if(is.numeric(df[[col]]) || is.factor(df[[col]])) {
tbl <- table(df[[col]], useNA = "no")
if(length(tbl) > 0) {
max_pct <- max(tbl) / sum(tbl)
if(max_pct >= threshold) {
dominant_val <- names(tbl)[which.max(tbl)]
nzv_info <- rbind(nzv_info, data.frame(
Variable = col,
DominantValue = as.character(dominant_val),
DominantPct = round(max_pct * 100, 2),
UniqueValues = length(tbl),
stringsAsFactors = FALSE
))
}
}
}
}
return(nzv_info)
}
# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable
if(length(nzv_variables) > 0) {
cat("Near-zero variance variables to monitor:\n")
print(nzv_variables)
cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
cat("No significant near-zero variance issues found\n")
}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
# Check if they're perfectly correlated
state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")
# If one-to-one mapping, they're redundant
cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
cat("Removing State_Loc (keeping State_Name)\n")
excluded_cols <- c(excluded_cols, "State_Loc")
FFdf$State_Loc <- NULL
}
# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]
if(length(numeric_vars) > 1) {
# Calculate correlation matrix (handling NAs)
cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")
# Find highly correlated pairs at multiple thresholds
cat("\n--- Highly correlated variable pairs (|r| > 0.85) ---\n")
cat("These pairs may cause multicollinearity in regression models\n\n")
high_cor_85 <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor_85) > 0) {
cor_pairs <- data.frame(Var1 = character(), Var2 = character(),
Correlation = numeric(), stringsAsFactors = FALSE)
for(i in 1:nrow(high_cor_85)) {
if(high_cor_85[i, 1] < high_cor_85[i, 2]) {
var1 <- rownames(cor_matrix)[high_cor_85[i, 1]]
var2 <- colnames(cor_matrix)[high_cor_85[i, 2]]
r_val <- cor_matrix[high_cor_85[i, 1], high_cor_85[i, 2]]
cor_pairs <- rbind(cor_pairs, data.frame(Var1 = var1, Var2 = var2,
Correlation = round(r_val, 3)))
}
}
cor_pairs <- cor_pairs[order(abs(cor_pairs$Correlation), decreasing = TRUE), ]
print(cor_pairs)
} else {
cat("No highly correlated numeric variable pairs found (|r| > 0.85)\n")
}
}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
# Subset to variables with fewer missing values for cleaner plot
complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]
if(length(complete_vars) > 2) {
cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
corrplot(cor_subset, method = "color", type = "upper",
tl.cex = 0.7, tl.col = "black",
title = "Correlation Matrix - Numeric Variables",
mar = c(0, 0, 2, 0))
}
}
# Address multicollinearity based on correlation matrix analysis
cat("=== MULTICOLLINEARITY ANALYSIS ===\n\n")
# Identify correlated variable clusters from the correlation matrix
cat("Cluster 1: Spending & Visit variables\n")
cat("  - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)\n")
cat("  - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)\n")
cat("  Recommendation: Consider removing Rep_Visits or Total_Spent\n\n")
cat("Cluster 2: Concession & Seating\n")
cat("  - Concession_Total <-> NumSeats: r = 0.915 (strong positive)\n")
cat("  Recommendation: Makes business sense - more seats = more concessions\n\n")
cat("Cluster 3: Attendance pairs\n")
cat("  - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)\n")
cat("  Note: Negative correlation suggests inverse relationship\n")
cat("  Recommendation: Keep both - they capture different behaviors\n\n")
# Create list of variables to flag for multicollinearity
multicollinear_vars <- c("Rep_Visits", "Team_Store_Total")
cat("Variables flagged for potential removal due to multicollinearity:\n")
print(multicollinear_vars)
# Option: Remove highly correlated variables (uncomment to execute)
# FFdf <- FFdf[, !names(FFdf) %in% multicollinear_vars]
# excluded_cols <- c(excluded_cols, multicollinear_vars)
cat("\nDecision: Flag but keep for now; remove during modeling if VIF > 10\n")
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))
# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")
# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")
# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)
# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)
# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")
# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)
par(mfrow = c(1, 1))
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")
# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
"(", round(total_missing/total_cells * 100, 2), "%)\n\n")
# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)
cat("Columns with missing values:\n")
na_summary <- data.frame(
Variable = names(na_by_col),
Missing_Count = as.numeric(na_by_col),
Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")
# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")
# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")
# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")
# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)
cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
# Prepare data for decision tree
# Exclude ID columns and flag variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
"Marital_Original", "Last_Contact", "Contact_Year",
"Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour")
tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]
# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]
# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)
# Build a simple decision tree (max 2-3 splits for sanity check)
sanity_tree <- rpart(Y01 ~ .,
data = tree_data,
method = "class",
control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))
# Plot the tree
rpart.plot(sanity_tree,
main = "Sanity Check Decision Tree (max depth = 3)",
extra = 104,  # Show percentage and count
box.palette = "RdYlGn")
# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
print(head(var_imp, 10))
} else {
cat("No variables selected by the tree\n")
}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")
# Check for suspiciously good predictors
cat("Tree Summary:\n")
summary(sanity_tree)
cat("\n--- Interpretation ---\n")
cat("1. If a single variable achieves >90% accuracy: investigate for data leakage\n")
cat("2. If an ID-like variable appears: exclude from modeling\n")
cat("3. Top predictors should make business sense\n\n")
# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")
if(accuracy > 0.90) {
cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
print(sanity_tree)
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")
cat("\n--- Interpretation ---\n")
cat("1. If a single variable achieves >90% accuracy: investigate for data leakage\n")
cat("2. If an ID-like variable appears: exclude from modeling\n")
cat("3. Top predictors should make business sense\n\n")
# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")
if(accuracy > 0.90) {
cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
# Build a second tree using Region instead of State_Name to reduce cardinality issues
# Exclude high-cardinality variables that may cause overfitting
exclude_high_card <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
"Marital_Original", "Last_Contact", "Contact_Year",
"Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour",
"State_Name", "Zip_Codes")  # Exclude high-cardinality vars
tree_vars_region <- names(FFdf)[!names(FFdf) %in% exclude_high_card]
tree_data_region <- FFdf[, tree_vars_region]
# Remove rows with NA in response variable
tree_data_region <- tree_data_region[!is.na(tree_data_region$Y01), ]
# Convert Y01 to factor for classification tree
tree_data_region$Y01 <- as.factor(tree_data_region$Y01)
# Build decision tree with Region instead of State_Name
sanity_tree_region <- rpart(Y01 ~ .,
data = tree_data_region,
method = "class",
control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))
# Plot the tree
rpart.plot(sanity_tree_region,
main = "Sanity Check Tree (Using Region instead of State_Name)",
extra = 104,
box.palette = "RdYlGn")
# Variable importance for region-based tree
cat("\n=== VARIABLE IMPORTANCE (Region-based Tree) ===\n")
if(length(sanity_tree_region$variable.importance) > 0) {
var_imp_region <- sort(sanity_tree_region$variable.importance, decreasing = TRUE)
print(head(var_imp_region, 10))
} else {
cat("No variables selected by the tree\n")
}
# Check accuracy
predictions_region <- predict(sanity_tree_region, tree_data_region, type = "class")
accuracy_region <- mean(predictions_region == tree_data_region$Y01, na.rm = TRUE)
cat("\nTree accuracy (with Region):", round(accuracy_region * 100, 2), "%\n")
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")
cat("Step 6 - Handle Categorical Variables:\n")
cat("  - Reviewed level distributions for rare levels\n")
cat("  - Marital 'U' kept as separate category\n")
cat("  - State_Name grouped into 4 US Census regions (Northeast, Midwest, South, West)\n\n")
cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")
cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n")
cat("  - Decision: Keep for now but monitor during modeling\n\n")
cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n")
cat("  - Correlation matrix reviewed for multicollinearity\n\n")
cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n")
cat("  - VIM aggr() visualization generated\n\n")
cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Tree accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Review variable importance for potential data leakage\n\n")
cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
# Save cleaned dataset for next phase (Missing Data)
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
