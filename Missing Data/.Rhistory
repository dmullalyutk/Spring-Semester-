UniqueValues = length(tbl),
stringsAsFactors = FALSE
))
}
}
}
}
return(nzv_info)
}
# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable
if(length(nzv_variables) > 0) {
cat("Near-zero variance variables to monitor:\n")
print(nzv_variables)
cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
cat("No significant near-zero variance issues found\n")
}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
# Check if they're perfectly correlated
state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")
# If one-to-one mapping, they're redundant
cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
cat("Removing State_Loc (keeping State_Name)\n")
excluded_cols <- c(excluded_cols, "State_Loc")
FFdf$State_Loc <- NULL
}
# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]
if(length(numeric_vars) > 1) {
# Calculate correlation matrix (handling NAs)
cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")
# Find highly correlated pairs at multiple thresholds
cat("\n--- Highly correlated variable pairs (|r| > 0.85) ---\n")
cat("These pairs may cause multicollinearity in regression models\n\n")
high_cor_85 <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor_85) > 0) {
cor_pairs <- data.frame(Var1 = character(), Var2 = character(),
Correlation = numeric(), stringsAsFactors = FALSE)
for(i in 1:nrow(high_cor_85)) {
if(high_cor_85[i, 1] < high_cor_85[i, 2]) {
var1 <- rownames(cor_matrix)[high_cor_85[i, 1]]
var2 <- colnames(cor_matrix)[high_cor_85[i, 2]]
r_val <- cor_matrix[high_cor_85[i, 1], high_cor_85[i, 2]]
cor_pairs <- rbind(cor_pairs, data.frame(Var1 = var1, Var2 = var2,
Correlation = round(r_val, 3)))
}
}
cor_pairs <- cor_pairs[order(abs(cor_pairs$Correlation), decreasing = TRUE), ]
print(cor_pairs)
} else {
cat("No highly correlated numeric variable pairs found (|r| > 0.85)\n")
}
}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
# Subset to variables with fewer missing values for cleaner plot
complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]
if(length(complete_vars) > 2) {
cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
corrplot(cor_subset, method = "color", type = "upper",
tl.cex = 0.7, tl.col = "black",
title = "Correlation Matrix - Numeric Variables",
mar = c(0, 0, 2, 0))
}
}
# Address multicollinearity based on correlation matrix analysis
cat("=== MULTICOLLINEARITY ANALYSIS ===\n\n")
# Identify correlated variable clusters from the correlation matrix
cat("Cluster 1: Spending & Visit variables\n")
cat("  - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)\n")
cat("  - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)\n")
cat("  Recommendation: Consider removing Rep_Visits or Total_Spent\n\n")
cat("Cluster 2: Concession & Seating\n")
cat("  - Concession_Total <-> NumSeats: r = 0.915 (strong positive)\n")
cat("  Recommendation: Makes business sense - more seats = more concessions\n\n")
cat("Cluster 3: Attendance pairs\n")
cat("  - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)\n")
cat("  Note: Negative correlation suggests inverse relationship\n")
cat("  Recommendation: Keep both - they capture different behaviors\n\n")
# Create list of variables to flag for multicollinearity
multicollinear_vars <- c("Rep_Visits", "Team_Store_Total")
cat("Variables flagged for potential removal due to multicollinearity:\n")
print(multicollinear_vars)
# Option: Remove highly correlated variables (uncomment to execute)
# FFdf <- FFdf[, !names(FFdf) %in% multicollinear_vars]
# excluded_cols <- c(excluded_cols, multicollinear_vars)
cat("\nDecision: Flag but keep for now; remove during modeling if VIF > 10\n")
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))
# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")
# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")
# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)
# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)
# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")
# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)
par(mfrow = c(1, 1))
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")
# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
"(", round(total_missing/total_cells * 100, 2), "%)\n\n")
# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)
cat("Columns with missing values:\n")
na_summary <- data.frame(
Variable = names(na_by_col),
Missing_Count = as.numeric(na_by_col),
Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")
# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")
# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")
# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")
# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)
cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
# Prepare data for decision tree
# Exclude ID columns, flag variables, and high-cardinality variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
"Marital_Original", "Last_Contact", "Contact_Year",
"Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour",
"State_Name", "Zip_Codes")
tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]
# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]
# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)
# Build a simple decision tree (max 2-3 splits for sanity check)
# Uses Region variable (from Step 6) instead of high-cardinality State_Name/Zip_Codes
sanity_tree <- rpart(Y01 ~ .,
data = tree_data,
method = "class",
control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))
# Plot the tree
rpart.plot(sanity_tree,
main = "Sanity Check Decision Tree (max depth = 3)",
extra = 104,  # Show percentage and count
box.palette = "RdYlGn")
# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
print(head(var_imp, 10))
} else {
cat("No variables selected by the tree\n")
}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")
# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")
if(accuracy > 0.90) {
cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")
cat("Step 6 - Handle Categorical Variables:\n")
cat("  - Rare factor levels (< 5%) lumped into 'Other' via fct_lump_prop()\n")
cat("  - Marital 'U' kept as separate category\n")
cat("  - State_Name grouped into US Census regions\n\n")
cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")
cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n")
cat("  - Decision: Keep for now but monitor during modeling\n\n")
cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n")
cat("  - Correlation matrix reviewed for multicollinearity\n\n")
cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n")
cat("  - Missing data summary table generated\n\n")
cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Tree accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Review variable importance for potential data leakage\n\n")
cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
# Save cleaned dataset for next phase (Missing Data)
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
# Rename to MDdf to follow course convention
MDdf <- FFdf
cat("=== MISSING DATA IDENTIFICATION ===\n")
cat("Dataset:", nrow(MDdf), "rows x", ncol(MDdf), "columns\n")
cat("Total missing values:", sum(is.na(MDdf)), "\n")
cat("Total cells:", nrow(MDdf) * ncol(MDdf), "\n")
cat("Overall missing %:", round(sum(is.na(MDdf)) / (nrow(MDdf) * ncol(MDdf)) * 100, 2), "%\n\n")
# Missing values by column - sorted descending
na_counts <- colSums(is.na(MDdf))
na_cols <- na_counts[na_counts > 0]
na_sorted <- sort(na_cols, decreasing = TRUE)
# Create a summary table
missing_summary <- data.frame(
Variable = names(na_sorted),
Missing_Count = as.numeric(na_sorted),
Missing_Pct = round(as.numeric(na_sorted) / nrow(MDdf) * 100, 1),
Present_Count = nrow(MDdf) - as.numeric(na_sorted)
)
cat("=== COLUMNS WITH MISSING DATA ===\n")
print(missing_summary, row.names = FALSE)
# Flextable for formatted output
flextable(missing_summary) %>%
set_header_labels(Variable = "Variable", Missing_Count = "Missing (n)",
Missing_Pct = "Missing (%)", Present_Count = "Present (n)") %>%
colformat_double(j = "Missing_Pct", digits = 1) %>%
autofit()
# md.pattern from mice - shows patterns of missingness across variables
cat("=== MISSING DATA PATTERN (mice::md.pattern) ===\n")
md.pattern(MDdf, plot = TRUE, rotate.names = TRUE)
# VIM aggr plot - visual aggregation of missing data
aggr(MDdf,
col = c("steelblue", "red"),
numbers = TRUE,
sortVars = TRUE,
labels = names(MDdf),
cex.axis = 0.5,
gap = 3,
ylab = c("Proportion Missing", "Pattern"))
cat("=== CREATING MISSING DATA INDICATORS ===\n\n")
# Get variables with missing values
vars_with_na <- names(MDdf)[colSums(is.na(MDdf)) > 0]
# Exclude Marital_Original (backup column) and Flag_Tenure_High (derived from Tenure)
vars_with_na <- vars_with_na[!vars_with_na %in% c("Marital_Original", "Flag_Tenure_High")]
cat("Creating indicators for", length(vars_with_na), "variables:\n")
cat(vars_with_na, sep = ", ")
cat("\n\n")
# Create indicator variables: 1 = missing, 0 = present
for(var in vars_with_na) {
indicator_name <- paste0("M_", var)
MDdf[[indicator_name]] <- ifelse(is.na(MDdf[[var]]), 1, 0)
}
# Display indicator summary
indicator_vars <- grep("^M_", names(MDdf), value = TRUE)
cat("Indicator variables created:", length(indicator_vars), "\n\n")
# Verify indicators match original NA counts
indicator_check <- data.frame(
Variable = vars_with_na,
Original_NA = sapply(vars_with_na, function(v) sum(is.na(MDdf[[v]]))),
Indicator_Sum = sapply(vars_with_na, function(v) sum(MDdf[[paste0("M_", v)]]))
)
cat("=== VERIFICATION: Indicators match original NA counts ===\n")
print(indicator_check, row.names = FALSE)
cat("\nAll match:", all(indicator_check$Original_NA == indicator_check$Indicator_Sum), "\n")
# Check correlations between missing indicators
# High correlation = variables tend to be missing together
indicator_matrix <- MDdf[, indicator_vars]
# Only use indicators with variance > 0
non_constant <- sapply(indicator_matrix, function(x) var(x) > 0)
indicator_matrix <- indicator_matrix[, non_constant]
cor_indicators <- cor(indicator_matrix)
cat("=== CORRELATION OF MISSING INDICATORS ===\n")
cat("High correlations suggest variables are missing as a block\n\n")
# Find pairs with correlation > 0.8
high_cor <- which(abs(cor_indicators) > 0.8 & abs(cor_indicators) < 1, arr.ind = TRUE)
if(nrow(high_cor) > 0) {
pairs_shown <- c()
for(i in 1:nrow(high_cor)) {
if(high_cor[i,1] < high_cor[i,2]) {
v1 <- rownames(cor_indicators)[high_cor[i,1]]
v2 <- colnames(cor_indicators)[high_cor[i,2]]
r_val <- cor_indicators[high_cor[i,1], high_cor[i,2]]
cat(sprintf("  %s <-> %s : r = %.3f\n", v1, v2, r_val))
}
}
}
corrplot(cor_indicators, method = "color", type = "upper",
tl.cex = 0.7, tl.col = "black",
title = "Correlation of Missing Data Indicators",
mar = c(0, 0, 2, 0))
cat("=== CLEANING UP OBVIOUS MISTAKES ===\n\n")
# --- 3b. Survey_Comp values > 1 (expected 0-1 proportion) ---
cat("3b. Survey_Comp values > 1 (expected range 0-1):\n")
cat("    Count > 1:", sum(MDdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("    Max value:", max(MDdf$Survey_Comp, na.rm = TRUE), "\n")
cat("    Decision: Cap at 1.0 -- values above 1 on a proportion scale are data entry errors\n")
MDdf$Survey_Comp[MDdf$Survey_Comp > 1] <- 1.0
cat("    Survey_Comp range after cap:", range(MDdf$Survey_Comp, na.rm = TRUE), "\n\n")
# --- 3c. Mult_Loc has "Y" and "Yes" (same meaning) ---
cat("3c. Mult_Loc has inconsistent levels:\n")
print(table(MDdf$Mult_Loc, useNA = "ifany"))
cat("    Decision: Combine 'Y' into 'Yes'\n")
MDdf$Mult_Loc[MDdf$Mult_Loc == "Y"] <- "Yes"
MDdf$Mult_Loc <- droplevels(MDdf$Mult_Loc)
cat("    After fix:\n")
print(table(MDdf$Mult_Loc, useNA = "ifany"))
cat("\n")
# --- 3d. Most_Purch_Concession has blank entries ---
cat("3d. Most_Purch_Concession blank entries:\n")
blank_count <- sum(MDdf$Most_Purch_Concession == "", na.rm = TRUE)
cat("    Blank entries:", blank_count, "\n")
if(blank_count > 0) {
cat("    Decision: Convert blanks to NA\n")
MDdf$Most_Purch_Concession[MDdf$Most_Purch_Concession == ""] <- NA
MDdf$Most_Purch_Concession <- droplevels(MDdf$Most_Purch_Concession)
# Create/update indicator for this newly-missing variable
MDdf$M_Most_Purch_Concession <- ifelse(is.na(MDdf$Most_Purch_Concession), 1, 0)
indicator_vars <- grep("^M_", names(MDdf), value = TRUE)
}
cat("\n")
# --- 3e. Tenure = 400 (impossible if years) ---
cat("3e. Tenure extreme values:\n")
cat("    Values > 100:", sum(MDdf$Tenure > 100, na.rm = TRUE), "\n")
cat("    Max tenure:", max(MDdf$Tenure, na.rm = TRUE), "\n")
cat("    Decision: Leave as-is for now. Units may be months (400 months = 33 years, plausible).\n")
cat("    Flagged via Flag_Tenure_High for SME review.\n\n")
# --- 3f. Age = 99 check ---
cat("3f. Age extreme values:\n")
cat("    Values = 99:", sum(MDdf$Age == 99, na.rm = TRUE), "\n")
cat("    Age range:", range(MDdf$Age, na.rm = TRUE), "\n")
cat("    Decision: Leave as-is -- could be legitimate. SME review needed.\n\n")
cat("=== STEP 3 COMPLETE ===\n")
cat("Total missing after cleanup:", sum(is.na(MDdf)), "\n")
cat("=== EASY DECISIONS ON ROWS/COLUMNS ===\n\n")
# --- 4a. Column Decisions ---
cat("--- COLUMN DECISIONS ---\n\n")
# Recalculate missing after Step 3 cleanup
na_counts_updated <- colSums(is.na(MDdf))
na_cols_updated <- na_counts_updated[na_counts_updated > 0]
na_sorted_updated <- sort(na_cols_updated, decreasing = TRUE)
for(i in seq_along(na_sorted_updated)) {
pct <- round(na_sorted_updated[i] / nrow(MDdf) * 100, 1)
cat(sprintf("%-25s : %4d (%5.1f%%)\n", names(na_sorted_updated)[i], na_sorted_updated[i], pct))
}
# Identify columns to EXCLUDE from imputation
cat("\n--- COLUMNS TO EXCLUDE FROM IMPUTATION ---\n\n")
# Identifier columns - not predictors
id_cols <- c("ID", "Cust_ID")
cat("Identifier columns (not predictors):", paste(id_cols, collapse = ", "), "\n")
# Backup/derived columns
backup_cols <- c("Marital_Original", "Flag_Tenure_High", "Flag_Survey_Invalid")
cat("Backup/derived columns:", paste(backup_cols, collapse = ", "), "\n")
# Date components (derived from Last_Contact)
date_cols <- c("Last_Contact", "Contact_Year", "Contact_Month", "Contact_Day",
"Contact_Weekday", "Contact_Hour")
cat("Date columns (derived):", paste(date_cols, collapse = ", "), "\n")
# High-cardinality columns not useful for imputation
high_card_cols <- c("State_Name", "Zip_Codes", "Seating_Location", "Favorite_Caps_Player")
cat("High-cardinality columns:", paste(high_card_cols, collapse = ", "), "\n")
# Indicator columns (used for testing, not for imputation)
indicator_cols <- grep("^M_", names(MDdf), value = TRUE)
cat("Indicator columns:", length(indicator_cols), "M_ variables\n")
# All columns to exclude
all_exclude <- c(id_cols, backup_cols, date_cols, high_card_cols, indicator_cols)
cat("\nTotal columns excluded from imputation:", length(all_exclude), "\n")
# --- 4b. Column decisions on high-missingness variables ---
cat("\n--- HIGH MISSINGNESS COLUMN DECISIONS ---\n\n")
cat("Net_Worth_True (61.0% missing):\n")
cat("  Decision: EXCLUDE from imputation. Too much missing to impute reliably.\n")
cat("  Will keep in dataset but not impute.\n\n")
cat("HouseHold_Income_True (60.2% missing):\n")
cat("  Decision: EXCLUDE from imputation. Too much missing to impute reliably.\n")
cat("  Will keep in dataset but not impute.\n\n")
cat("CustomerDF block (70% missing each - 7 variables):\n")
cat("  Educational_Level, Favorite_Sport, Job_Sector, Mode_Of_Transport,\n")
cat("  Team_B_STH, Team_C_STH, Favorite_Caps_Player\n")
cat("  Decision: EXCLUDE from imputation. These 7 variables are missing as a block\n")
cat("  (customers who never completed the supplemental survey). Imputing 70% of a\n")
cat("  variable would be manufacturing data, not recovering it.\n\n")
high_miss_exclude <- c("Net_Worth_True", "HouseHold_Income_True",
"Educational_Level", "Favorite_Sport", "Job_Sector",
"Mode_Of_Transport", "Team_B_STH", "Team_C_STH",
"Favorite_Caps_Player")
cat("High-missingness columns excluded:", length(high_miss_exclude), "\n")
# --- 4c. Row Decisions ---
cat("\n--- ROW DECISIONS ---\n\n")
# Check how many rows have excessive missing (>50% of columns missing)
total_cols <- ncol(MDdf) - length(all_exclude) - length(high_miss_exclude)
row_na_counts <- rowSums(is.na(MDdf[, !names(MDdf) %in% c(all_exclude, high_miss_exclude)]))
cat("Among the", total_cols, "imputation-eligible columns:\n")
cat("  Rows with 0 missing:", sum(row_na_counts == 0), "\n")
cat("  Rows with 1-3 missing:", sum(row_na_counts >= 1 & row_na_counts <= 3), "\n")
cat("  Rows with 4-6 missing:", sum(row_na_counts >= 4 & row_na_counts <= 6), "\n")
cat("  Rows with 7+ missing:", sum(row_na_counts >= 7), "\n")
cat("\n  Decision: No rows excluded. Even rows with several missing values\n")
cat("  can be handled by MICE. Only exclude rows if >50% of columns are missing.\n")
# --- 4d. Build the imputation-eligible variable list ---
impute_vars <- names(MDdf)[!names(MDdf) %in% c(all_exclude, high_miss_exclude)]
cat("\n=== VARIABLES ELIGIBLE FOR IMPUTATION ===\n")
cat("Count:", length(impute_vars), "\n")
cat(impute_vars, sep = ", ")
cat("\n")
# Check which of these actually have missing values
impute_with_na <- impute_vars[colSums(is.na(MDdf[, impute_vars])) > 0]
cat("\nOf these,", length(impute_with_na), "have missing values to impute:\n")
for(v in impute_with_na) {
ct <- sum(is.na(MDdf[[v]]))
pct <- round(ct / nrow(MDdf) * 100, 1)
cat(sprintf("  %-20s : %4d (%5.1f%%)\n", v, ct, pct))
}
cat("=== STEP 5: ASSESS MISSINGNESS PATTERNS ===\n\n")
cat("Method: Decision Trees and Logistic Regression\n")
cat("Goal: Determine if missingness is predictable from other variables (MAR)\n")
cat("      or unpredictable (MCAR)\n\n")
# Variables to test: those with missing values that we plan to impute
test_vars <- impute_with_na
cat("Variables to assess:", paste(test_vars, sep = ", "), "\n\n")
# Predictors for the test: complete or near-complete variables
# Use the indicator variables we created in Step 2
predictor_candidates <- impute_vars[colSums(is.na(MDdf[, impute_vars])) == 0]
cat("Predictor candidates (no missing):", length(predictor_candidates), "\n")
cat(predictor_candidates, sep = ", ")
cat("\n")
# Use decision trees to predict missingness indicator for each variable
# If the tree finds meaningful splits, missingness is predictable -> MAR
# If the tree is trivial (no splits or very low accuracy), -> likely MCAR
cat("=== DECISION TREE TESTS FOR MISSINGNESS ===\n\n")
# Build a data frame of complete predictors for testing
complete_predictors <- MDdf[, predictor_candidates]
# Remove any factor columns with too many levels for rpart
for(col in names(complete_predictors)) {
if(is.factor(complete_predictors[[col]]) && nlevels(complete_predictors[[col]]) > 30) {
complete_predictors[[col]] <- NULL
}
}
# Also remove logical columns (convert to numeric)
for(col in names(complete_predictors)) {
if(is.logical(complete_predictors[[col]])) {
complete_predictors[[col]] <- as.integer(complete_predictors[[col]])
}
}
results <- data.frame(
Variable = character(),
Tree_Splits = integer(),
Accuracy = numeric(),
Top_Predictor = character(),
Assessment = character(),
stringsAsFactors = FALSE
)
for(var in test_vars) {
# Target: missing indicator
target <- factor(MDdf[[paste0("M_", var)]], levels = c(0, 1), labels = c("Present", "Missing"))
# Combine with predictors
tree_data <- cbind(Target = target, complete_predictors)
# Build decision tree
tree_model <- rpart(Target ~ .,
data = tree_data,
method = "class",
control = rpart.control(maxdepth = 3, minsplit = 50, cp = 0.01))
# Get results
n_splits <- nrow(tree_model$frame[tree_model$frame$var != "<leaf>", ])
preds <- predict(tree_model, tree_data, type = "class")
acc <- round(mean(preds == target) * 100, 1)
# Top predictor
if(length(tree_model$variable.importance) > 0) {
top_pred <- names(tree_model$variable.importance)[1]
} else {
top_pred <- "None"
}
# Assessment
if(n_splits == 0) {
assessment <- "Likely MCAR"
} else if(acc > 85) {
assessment <- "MAR"
} else {
assessment <- "Possibly MAR"
}
results <- rbind(results, data.frame(
Variable = var, Tree_Splits = n_splits, Accuracy = acc,
Top_Predictor = top_pred, Assessment = assessment, stringsAsFactors = FALSE
))
}
cat("=== MISSINGNESS ASSESSMENT RESULTS ===\n\n")
print(results, row.names = FALSE)
flextable(results) %>%
set_header_labels(Variable = "Variable", Tree_Splits = "Splits",
Accuracy = "Accuracy (%)", Top_Predictor = "Top Predictor",
Assessment = "Assessment") %>%
autofit()
