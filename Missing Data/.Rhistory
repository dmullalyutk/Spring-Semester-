# Option: Convert "U" to NA (uncomment if desired)
# FFdf$Marital[FFdf$Marital == "U"] <- NA
# FFdf$Marital <- droplevels(FFdf$Marital)
cat("Marital 'U' (Unknown) count:", sum(FFdf$Marital == "U", na.rm = TRUE), "\n")
cat("Decision: Keep 'U' as separate level for now - may represent meaningful unknown status\n")
# Check for levels that might mean the same thing
cat("\n--- State_Name levels ---\n")
print(length(levels(FFdf$State_Name)))
cat("Number of unique states:", length(unique(FFdf$State_Name)), "\n")
# Fix Mult_Loc "Y" -> "Yes" BEFORE lumping, so both merge into "Yes" (not separately into "Other")
levels(FFdf$Mult_Loc)[levels(FFdf$Mult_Loc) == "Y"] <- "Yes"
# Lump rare factor levels into "Other" using forcats::fct_lump_prop()
# Threshold: levels appearing in < 5% of non-NA observations are combined
# Store original levels for reference
factor_vars_to_lump <- names(FFdf)[sapply(FFdf, is.factor)]
# Exclude variables we handle separately (State_Name -> Region, Marital already reviewed)
skip_vars <- c("State_Name", "State_Loc", "Marital", "Marital_Original","Educational_Level","Seating_Location","Favorite_Caps_Player","Job_Sector")
lump_vars <- setdiff(factor_vars_to_lump, skip_vars)
cat("=== LUMPING RARE LEVELS (< 5%) INTO 'Other' ===\n\n")
for(var in lump_vars) {
original_levels <- nlevels(FFdf[[var]])
FFdf[[var]] <- fct_lump_prop(FFdf[[var]], prop = 0.05, other_level = "Other")
new_levels <- nlevels(FFdf[[var]])
if(original_levels != new_levels) {
cat(var, ": ", original_levels, " -> ", new_levels, " levels\n", sep = "")
print(table(FFdf[[var]], useNA = "ifany"))
cat("\n")
}
}
cat("Lumping complete.\n")
# Define region mappings
northeast <- c("Connecticut", "Maine", "Massachusetts", "New Hampshire",
"Rhode Island", "Vermont", "New Jersey", "New York", "Pennsylvania")
midwest <- c("Illinois", "Indiana", "Michigan", "Ohio", "Wisconsin",
"Iowa", "Kansas", "Minnesota", "Missouri", "Nebraska",
"North Dakota", "South Dakota")
south <- c("Delaware", "Florida", "Georgia", "North Carolina",
"South Carolina", "West Virginia",
"Alabama", "Kentucky", "Mississippi", "Tennessee",
"Arkansas", "Louisiana", "Oklahoma", "Texas")
west <- c("Arizona", "Colorado", "Idaho", "Montana", "Nevada", "New Mexico",
"Utah", "Wyoming", "Alaska", "California", "Hawaii", "Oregon", "Washington")
dcarea <- c("Maryland","Virginia", "District of Columbia")
# Create Region variable
FFdf$Region <- case_when(
FFdf$State_Name %in% northeast ~ "Northeast",
FFdf$State_Name %in% midwest ~ "Midwest",
FFdf$State_Name %in% south ~ "South",
FFdf$State_Name %in% west ~ "West",
FFdf$State_Name %in% dcarea ~ "DCArea",
TRUE ~ "Other"  # Catch any unmatched states
)
# Convert to factor
FFdf$Region <- as.factor(FFdf$Region)
# Display region distribution
cat("--- Region Distribution (grouped from State_Name) ---\n")
print(table(FFdf$Region, useNA = "ifany"))
cat("\nRegion percentages:\n")
print(round(prop.table(table(FFdf$Region)) * 100, 2))
# Check if any states weren't mapped
unmatched_states <- unique(FFdf$State_Name[FFdf$Region == "Other"])
if(length(unmatched_states) > 0) {
cat("\nWarning - Unmatched states assigned to 'Other':\n")
print(unmatched_states)
} else {
cat("\nAll states successfully mapped to regions.\n")
}
# Function to identify zero-variance columns
find_zero_variance <- function(df) {
zv_cols <- c()
for(col in names(df)) {
unique_vals <- length(unique(na.omit(df[[col]])))
if(unique_vals <= 1) {
zv_cols <- c(zv_cols, col)
}
}
return(zv_cols)
}
# Find zero-variance columns
zero_var_cols <- find_zero_variance(FFdf)
cat("Zero-variance columns found:", length(zero_var_cols), "\n")
if(length(zero_var_cols) > 0) {
cat("Columns with zero variance:\n")
print(zero_var_cols)
# Store in excluded columns list
excluded_cols <- zero_var_cols
# Remove zero-variance columns
FFdf <- FFdf[, !names(FFdf) %in% zero_var_cols]
cat("\nRemoved", length(zero_var_cols), "zero-variance columns\n")
} else {
cat("No zero-variance columns found\n")
excluded_cols <- c()
}
cat("Remaining columns:", ncol(FFdf), "\n")
# Function to find near-zero variance columns
# A column is NZV if one value dominates (e.g., >95% of values)
find_near_zero_variance <- function(df, threshold = 0.95) {
nzv_info <- data.frame(
Variable = character(),
DominantValue = character(),
DominantPct = numeric(),
UniqueValues = integer(),
stringsAsFactors = FALSE
)
for(col in names(df)) {
if(is.numeric(df[[col]]) || is.factor(df[[col]])) {
tbl <- table(df[[col]], useNA = "no")
if(length(tbl) > 0) {
max_pct <- max(tbl) / sum(tbl)
if(max_pct >= threshold) {
dominant_val <- names(tbl)[which.max(tbl)]
nzv_info <- rbind(nzv_info, data.frame(
Variable = col,
DominantValue = as.character(dominant_val),
DominantPct = round(max_pct * 100, 2),
UniqueValues = length(tbl),
stringsAsFactors = FALSE
))
}
}
}
}
return(nzv_info)
}
# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable
if(length(nzv_variables) > 0) {
cat("Near-zero variance variables to monitor:\n")
print(nzv_variables)
cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
cat("No significant near-zero variance issues found\n")
}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
# Check if they're perfectly correlated
state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")
# If one-to-one mapping, they're redundant
cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
cat("Removing State_Loc (keeping State_Name)\n")
excluded_cols <- c(excluded_cols, "State_Loc")
FFdf$State_Loc <- NULL
}
# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]
if(length(numeric_vars) > 1) {
# Calculate correlation matrix (handling NAs)
cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")
# Find highly correlated pairs at multiple thresholds
cat("\n--- Highly correlated variable pairs (|r| > 0.85) ---\n")
cat("These pairs may cause multicollinearity in regression models\n\n")
high_cor_85 <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor_85) > 0) {
cor_pairs <- data.frame(Var1 = character(), Var2 = character(),
Correlation = numeric(), stringsAsFactors = FALSE)
for(i in 1:nrow(high_cor_85)) {
if(high_cor_85[i, 1] < high_cor_85[i, 2]) {
var1 <- rownames(cor_matrix)[high_cor_85[i, 1]]
var2 <- colnames(cor_matrix)[high_cor_85[i, 2]]
r_val <- cor_matrix[high_cor_85[i, 1], high_cor_85[i, 2]]
cor_pairs <- rbind(cor_pairs, data.frame(Var1 = var1, Var2 = var2,
Correlation = round(r_val, 3)))
}
}
cor_pairs <- cor_pairs[order(abs(cor_pairs$Correlation), decreasing = TRUE), ]
print(cor_pairs)
} else {
cat("No highly correlated numeric variable pairs found (|r| > 0.85)\n")
}
}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
# Subset to variables with fewer missing values for cleaner plot
complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]
if(length(complete_vars) > 2) {
cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
corrplot(cor_subset, method = "color", type = "upper",
tl.cex = 0.7, tl.col = "black",
title = "Correlation Matrix - Numeric Variables",
mar = c(0, 0, 2, 0))
}
}
# Address multicollinearity based on correlation matrix analysis
cat("=== MULTICOLLINEARITY ANALYSIS ===\n\n")
# Identify correlated variable clusters from the correlation matrix
cat("Cluster 1: Spending & Visit variables\n")
cat("  - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)\n")
cat("  - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)\n")
cat("  Recommendation: Consider removing Rep_Visits or Total_Spent\n\n")
cat("Cluster 2: Concession & Seating\n")
cat("  - Concession_Total <-> NumSeats: r = 0.915 (strong positive)\n")
cat("  Recommendation: Makes business sense - more seats = more concessions\n\n")
cat("Cluster 3: Attendance pairs\n")
cat("  - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)\n")
cat("  Note: Negative correlation suggests inverse relationship\n")
cat("  Recommendation: Keep both - they capture different behaviors\n\n")
# Create list of variables to flag for multicollinearity
multicollinear_vars <- c("Rep_Visits", "Team_Store_Total")
cat("Variables flagged for potential removal due to multicollinearity:\n")
print(multicollinear_vars)
# Option: Remove highly correlated variables (uncomment to execute)
# FFdf <- FFdf[, !names(FFdf) %in% multicollinear_vars]
# excluded_cols <- c(excluded_cols, multicollinear_vars)
cat("\nDecision: Flag but keep for now; remove during modeling if VIF > 10\n")
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))
# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")
# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")
# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)
# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)
# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")
# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)
par(mfrow = c(1, 1))
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")
# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
"(", round(total_missing/total_cells * 100, 2), "%)\n\n")
# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)
cat("Columns with missing values:\n")
na_summary <- data.frame(
Variable = names(na_by_col),
Missing_Count = as.numeric(na_by_col),
Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")
# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")
# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")
# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")
# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)
cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
# Prepare data for decision tree
# Exclude ID columns, flag variables, and high-cardinality variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
"Marital_Original", "Last_Contact", "Contact_Year",
"Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour",
"State_Name", "Zip_Codes")
tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]
# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]
# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)
# Build a simple decision tree (max 2-3 splits for sanity check)
# Uses Region variable (from Step 6) instead of high-cardinality State_Name/Zip_Codes
sanity_tree <- rpart(Y01 ~ .,
data = tree_data,
method = "class",
control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))
# Plot the tree
rpart.plot(sanity_tree,
main = "Sanity Check Decision Tree (max depth = 3)",
extra = 104,  # Show percentage and count
box.palette = "RdYlGn")
# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
print(head(var_imp, 10))
} else {
cat("No variables selected by the tree\n")
}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")
# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")
if(accuracy > 0.90) {
cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")
cat("Step 6 - Handle Categorical Variables:\n")
cat("  - Rare factor levels (< 5%) lumped into 'Other' via fct_lump_prop()\n")
cat("  - Marital 'U' kept as separate category\n")
cat("  - State_Name grouped into US Census regions\n\n")
cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")
cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n")
cat("  - Decision: Keep for now but monitor during modeling\n\n")
cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n")
cat("  - Correlation matrix reviewed for multicollinearity\n\n")
cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n")
cat("  - Missing data summary table generated\n\n")
cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Tree accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Review variable importance for potential data leakage\n\n")
cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
# Save cleaned dataset for next phase (Missing Data)
tryCatch({
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
}, error = function(e) cat("\nNote: Could not save FFdf_cleaned.csv (file may be open):", conditionMessage(e), "\n"))
MDdf <- FFdf
missing_summary <- data.frame(
Variable     = names(MDdf),
Missing_Count = colSums(is.na(MDdf)),
Missing_Pct  = round(colSums(is.na(MDdf)) / nrow(MDdf) * 100, 1)
) %>% filter(Missing_Count > 0) %>% arrange(desc(Missing_Count))
cat("Total missing:", sum(is.na(MDdf)), "of", nrow(MDdf)*ncol(MDdf), "cells (",
round(sum(is.na(MDdf))/(nrow(MDdf)*ncol(MDdf))*100,1), "%)\n")
flextable(missing_summary) %>%
set_header_labels(Variable="Variable", Missing_Count="Missing (n)", Missing_Pct="Missing (%)") %>%
autofit()
vars_with_na <- names(MDdf)[colSums(is.na(MDdf)) > 0]
vars_with_na <- vars_with_na[!vars_with_na %in% c("Marital_Original", "Flag_Tenure_High")]
for(var in vars_with_na) {
MDdf[[paste0("M_", var)]] <- ifelse(is.na(MDdf[[var]]), 1, 0)
}
indicator_vars <- grep("^M_", names(MDdf), value = TRUE)
cat("Indicator variables created:", length(indicator_vars), "\n")
# Correlation of indicators — high r = missing as a block
indicator_matrix <- MDdf[, indicator_vars]
indicator_matrix <- indicator_matrix[, sapply(indicator_matrix, stats::var) > 0]
cor_indicators <- cor(indicator_matrix)
corrplot(cor_indicators, method = "color", type = "upper",
tl.cex = 0.7, tl.col = "black",
title = "Correlation of Missing Data Indicators",
mar = c(0, 0, 2, 0))
# Survey_Comp > 1: cap at 1.0 (proportion cannot exceed 100%)
cat("Survey_Comp > 1 count:", sum(MDdf$Survey_Comp > 1, na.rm = TRUE), "| max:", max(MDdf$Survey_Comp, na.rm = TRUE), "\n")
MDdf$Survey_Comp[MDdf$Survey_Comp > 1] <- 1.0
cat("Survey_Comp after cap:", range(MDdf$Survey_Comp, na.rm = TRUE), "\n\n")
# Marital "U": keep as valid level — unknown status is informative
cat("Marital 'U' count:", sum(MDdf$Marital == "U", na.rm = TRUE), "— KEPT as distinct level\n")
cat("Marital true NAs:", sum(is.na(MDdf$Marital)), "\n\n")
# Tenure / Age extremes: flag only, leave for SME
cat("Tenure > 100:", sum(MDdf$Tenure > 100, na.rm = TRUE), "| Age == 99:", sum(MDdf$Age == 99, na.rm = TRUE), "— flagged, left as-is\n")
cat("Total missing after cleanup:", sum(is.na(MDdf)), "\n")
id_cols       <- c("ID", "Cust_ID")
backup_cols   <- c("Marital_Original", "Flag_Tenure_High", "Flag_Survey_Invalid")
date_cols     <- c("Last_Contact","Contact_Year","Contact_Month","Contact_Day","Contact_Weekday","Contact_Hour")
high_card_cols <- c("State_Name", "Zip_Codes", "Seating_Location")
indicator_cols <- grep("^M_", names(MDdf), value = TRUE)
all_exclude <- c(id_cols, backup_cols, date_cols, high_card_cols, indicator_cols)
impute_vars  <- names(MDdf)[!names(MDdf) %in% all_exclude]
impute_with_na <- impute_vars[colSums(is.na(MDdf[, impute_vars])) > 0]
cat("Columns excluded from imputation:", length(all_exclude), "\n")
cat("Imputation-eligible variables:", length(impute_vars), "| with NAs:", length(impute_with_na), "\n\n")
# Row missing summary
row_na <- rowSums(is.na(MDdf[, impute_vars]))
cat("Row NA distribution among eligible columns:\n")
cat("  0 missing:", sum(row_na == 0), "| 1-3:", sum(row_na >= 1 & row_na <= 3),
"| 4-6:", sum(row_na >= 4 & row_na <= 6), "| 7+:", sum(row_na > 6), "\n")
cat("Decision: No rows excluded.\n")
predictor_candidates <- impute_vars[colSums(is.na(MDdf[, impute_vars])) == 0]
complete_predictors  <- MDdf[, predictor_candidates]
# Drop high-cardinality factors and logical columns
complete_predictors <- complete_predictors[, sapply(complete_predictors, function(x)
!(is.factor(x) && nlevels(x) > 30))]
complete_predictors <- as.data.frame(lapply(complete_predictors, function(x)
if(is.logical(x)) as.integer(x) else x))
results <- data.frame(Variable=character(), Splits=integer(), Accuracy=numeric(),
Top_Predictor=character(), Assessment=character(), stringsAsFactors=FALSE)
for(var in impute_with_na) {
target <- factor(MDdf[[paste0("M_", var)]], levels=c(0,1), labels=c("Present","Missing"))
tree_data <- cbind(Target=target, complete_predictors)
tree_model <- rpart(Target ~ ., data=tree_data, method="class",
control=rpart.control(maxdepth=3, minsplit=50, cp=0.01))
n_splits <- nrow(tree_model$frame[tree_model$frame$var != "<leaf>", ])
acc      <- round(mean(predict(tree_model, tree_data, type="class") == target) * 100, 1)
top_pred <- if(length(tree_model$variable.importance) > 0) names(tree_model$variable.importance)[1] else "None"
assessment <- if(n_splits == 0) "Likely MCAR" else if(acc > 85) "MAR" else "Possibly MAR"
results <- rbind(results, data.frame(Variable=var, Splits=n_splits, Accuracy=acc,
Top_Predictor=top_pred, Assessment=assessment,
stringsAsFactors=FALSE))
}
flextable(results) %>%
set_header_labels(Variable="Variable", Splits="Splits", Accuracy="Accuracy (%)",
Top_Predictor="Top Predictor", Assessment="Assessment") %>%
color(~ Assessment == "MAR",         ~ Assessment, color="red") %>%
color(~ Assessment == "Likely MCAR", ~ Assessment, color="darkgreen") %>%
color(~ Assessment == "Possibly MAR",~ Assessment, color="orange") %>%
autofit()
vars_with_splits <- results$Variable[results$Splits > 0]
for(var in vars_with_splits[1:min(4, length(vars_with_splits))]) {
target     <- factor(MDdf[[paste0("M_", var)]], levels=c(0,1), labels=c("Present","Missing"))
tree_model <- rpart(Target ~ ., data=cbind(Target=target, complete_predictors), method="class",
control=rpart.control(maxdepth=3, minsplit=50, cp=0.01))
rpart.plot(tree_model, main=paste("Missingness:", var, "—", results$Assessment[results$Variable==var]),
extra=104, box.palette="RdYlGn")
}
get_mode <- function(x) { x <- x[!is.na(x)]; ux <- unique(x); ux[which.max(tabulate(match(x, ux)))] }
# --- Median imputation ---
median_nc <- median(MDdf$Num_Children, na.rm=TRUE)
MDdf$Num_Children[is.na(MDdf$Num_Children)] <- median_nc
cat("Num_Children: imputed with median =", median_nc, "| NAs remaining:", sum(is.na(MDdf$Num_Children)), "\n")
median_rv <- median(MDdf$Rep_Visits, na.rm=TRUE)
MDdf$Rep_Visits[is.na(MDdf$Rep_Visits)] <- median_rv
cat("Rep_Visits: imputed with median =", round(median_rv,2), "| NAs remaining:", sum(is.na(MDdf$Rep_Visits)), "\n\n")
# --- Mode imputation ---
mode_mpc <- get_mode(MDdf$Most_Purch_Concession)
MDdf$Most_Purch_Concession[is.na(MDdf$Most_Purch_Concession)] <- mode_mpc
cat("Most_Purch_Concession: imputed with mode =", as.character(mode_mpc), "| NAs:", sum(is.na(MDdf$Most_Purch_Concession)), "\n")
mode_sex <- get_mode(MDdf$Sex)
MDdf$Sex[is.na(MDdf$Sex)] <- mode_sex
cat("Sex: imputed with mode =", as.character(mode_sex), "| NAs:", sum(is.na(MDdf$Sex)), "\n\n")
# --- Mgmt Dictate: Stochastic (independent, sample with replacement) ---
set.seed(42)
observed_rc <- MDdf$Rep_Calls[!is.na(MDdf$Rep_Calls)]
MDdf$Rep_Calls[is.na(MDdf$Rep_Calls)] <- sample(observed_rc, sum(is.na(MDdf$Rep_Calls)), replace=TRUE)
cat("Rep_Calls: stochastic imputation | NAs:", sum(is.na(MDdf$Rep_Calls)), "\n")
set.seed(123)
observed_rn    <- MDdf$Rep_Name[!is.na(MDdf$Rep_Name)]
level_props    <- prop.table(table(observed_rn))
MDdf$Rep_Name[is.na(MDdf$Rep_Name)] <- sample(names(level_props), sum(is.na(MDdf$Rep_Name)),
replace=TRUE, prob=as.numeric(level_props))
cat("Rep_Name: stochastic imputation (proportional) | NAs:", sum(is.na(MDdf$Rep_Name)), "\n")
# --- Mgmt Dictate: Age via linear regression on other variables ---
age_predictors  <- c("Y01","Total_Spent","NumSeats","Tenure","First_Year_STH",
"Num_Children","Rep_Calls","Rep_Visits","Concession_Total","Survey_Comp")
available_preds <- age_predictors[sapply(age_predictors, function(v) v %in% names(MDdf) && sum(is.na(MDdf[[v]]))==0)]
train_idx  <- !is.na(MDdf$Age)
age_model  <- lm(Age ~ ., data=MDdf[train_idx, c("Age", available_preds)])
pred_age   <- predict(age_model, newdata=MDdf[!train_idx, available_preds])
pred_age   <- pmin(pmax(round(pred_age), min(MDdf$Age, na.rm=TRUE)), max(MDdf$Age, na.rm=TRUE))
MDdf$Age[!train_idx] <- pred_age
cat("Age: regression imputation (", length(available_preds), "predictors) | NAs:", sum(is.na(MDdf$Age)), "\n")
cat("Predicted Age — Mean:", round(mean(pred_age),1), "| SD:", round(sd(pred_age),1),
"| Range:", min(pred_age), "-", max(pred_age), "\n")
mice_data <- MDdf[, impute_vars]
# Initialize to get default method vector
init <- mice(mice_data, maxit=0, print=FALSE)
meth <- init$method
# Build smarter predictor matrix: only use predictors with >= 10% correlation
# AND >= 50% usable cases (avoids multicollinearity from the 70%-missing block)
pred <- quickpred(mice_data, mincor=0.1, minpuc=0.5)
# Assign methods by variable type for variables still needing imputation
for(v in impute_with_na) {
cls <- class(mice_data[[v]])[1]
if(cls %in% c("numeric","integer")) {
meth[v] <- "pmm"
} else if(cls == "factor") {
meth[v] <- if(nlevels(mice_data[[v]]) == 2) "logreg" else "polyreg"
}
}
cat("Methods assigned:\n")
print(meth[meth != ""])
# Run MICE — MANAGEMENT DICTATE: m=10 imputation cycles
mice_imp <- mice(mice_data, method=meth, predictorMatrix=pred,
m=10, maxit=10, seed=42, print=FALSE)
cat("\nMICE complete: m=10 datasets (management dictate), maxit=10 iterations\n")
View(mice_imp)
# Convergence: chains should mix freely without drift
plot(mice_imp, layout=c(4,3))
# Density: blue = observed, red = imputed (should roughly overlap for numeric vars)
densityplot(mice_imp, ~ DistA + Tenure + Net_Worth_True + HouseHold_Income_True)
MDdf_imputed1 <- complete(mice_imp, 1)
MDdf_imputed1 <- complete(mice_imp, 1)
MDdf_imputed2 <- complete(mice_imp, 2)
MDdf_imputed3 <- complete(mice_imp, 3)
MDdf_imputed4 <- complete(mice_imp, 4)
MDdf_imputed5 <- complete(mice_imp, 5)
MDdf_imputed6 <- complete(mice_imp, 6)
MDdf_imputed7 <- complete(mice_imp, 7)
MDdf_imputed8 <- complete(mice_imp, 8)
MDdf_imputed9 <- complete(mice_imp, 9)
MDdf_imputed10 <- complete(mice_imp, 10)
View(MDdf_imputed1)
View(MDdf_imputed10)
sum(is.na(MDdf_imputed1))
View(MainDF)
View(MDdf)
View(MDdf_imputed6)
