---
title: "Data Wrangling and Cleaning: Steps 1-11"
author: "David Mullaly"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

## Introduction

This document covers the data wrangling process and all eleven data cleaning steps for the problem. We load multiple relational tables, merge them into a single flat file, and perform comprehensive data cleaning to prepare for missing data analysis.

### Data Cleaning Steps Covered:
**Steps 1-5 (Initial Cleaning)**

1. Open data in your software of choice
2. Review variables for common sense based on SME knowledge
3. Review how the software coded the variables (nominal, continuous)
4. Perform data integrity/validation checks
5. Handle dates

**Steps 6-7 (Categorical & Zero-Variance)**

6. Handle categorical variables - keep as is, combine rare levels, combine similar levels
7. Handle zero-variance predictors

**Steps 8-11 (Advanced Cleaning)**

8. Handle near zero-variance predictors
9. Eliminate redundant columns and linear combination columns
10. Search for outliers and initial search for missing values
11. Sanity check using Decision Tree (1 to 2 splits)

```{r setup, include=FALSE}
# Setup Chunk - Load Required Libraries
library(mice)
library(VIM)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(corrplot)
library(lubridate)
library(flextable)
library(rpart)       # Decision Tree package
library(rpart.plot)  # Visualization of Decision Trees

setwd("C:/Users/david/Desktop/Spring-Semester-/Missing Data")
```

## Data Loading and Wrangling

Load four relational tables and merge into a single flat file (FFdf) using left joins on Cust_ID.

```{r load_data, echo=FALSE}
# Load the raw data files
MainDF <- read.csv("Raw.csv")
StoreDF <- read.csv("StoreTable.csv")
ConcessDF <- read.csv("ConcessTable.csv")
CustomerDF <- read.csv("CustomerTable.csv")

# Check dimensions of each file
cat("MainDF:", dim(MainDF), "| StoreDF:", dim(StoreDF),
    "| ConcessDF:", dim(ConcessDF), "| CustomerDF:", dim(CustomerDF), "\n")
```

```{r check_duplicates, echo=FALSE}
# Check for duplicate Cust_IDs in each source table BEFORE merging
cat("Duplicates in MainDF:", sum(duplicated(MainDF$Cust_ID)), "\n")
cat("Duplicates in StoreDF:", sum(duplicated(StoreDF$Cust_ID)), "\n")
cat("Duplicates in ConcessDF:", sum(duplicated(ConcessDF$Cust_ID)), "\n")
cat("Duplicates in CustomerDF:", sum(duplicated(CustomerDF$Cust_ID)), "\n")
```

```{r merge_data, echo=FALSE}
# Create flat file by merging all tables on Cust_ID
FFdf <- MainDF
FFdf <- merge(FFdf, StoreDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, ConcessDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, CustomerDF, by = "Cust_ID", all.x = TRUE)

cat("Final flat file dimensions - Rows:", nrow(FFdf), "Columns:", ncol(FFdf), "\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "| Duplicate rows:", nrow(FFdf) - length(unique(FFdf$Cust_ID)), "\n")
```

## Step 1: Open Data in Your Software of Choice

Create identifier variable, arrange columns, and explore distributions visually.

```{r step1_open_data, echo=FALSE, fig.width=8, fig.height=6}
# Create an identifier variable
FFdf$ID <- 1:nrow(FFdf)

# Reorder columns: Y-variable first, then alphabetically
y_var <- "Y01"
other_vars <- sort(setdiff(names(FFdf), y_var))
FFdf <- FFdf[, c(y_var, other_vars)]

# Histograms for key numeric variables
par(mfrow = c(2, 2))
hist(FFdf$Age, main = "Distribution of Age", xlab = "Age", col = "steelblue", breaks = 20)
hist(FFdf$tenure, main = "Distribution of Tenure", xlab = "Tenure (Years)", col = "steelblue", breaks = 20)
hist(FFdf$NumSeats, main = "Distribution of NumSeats", xlab = "Number of Seats", col = "steelblue")
hist(FFdf$Total.Spent, main = "Distribution of Total Spent", xlab = "Total Spent ($)", col = "steelblue", breaks = 30)
par(mfrow = c(1, 1))

# Response variable distribution
cat("Response Variable (Y01):", table(FFdf$Y01), "\n")
barplot(table(FFdf$Y01), main = "Distribution of Y01 (Response Variable)",
        col = c("coral", "steelblue"), names.arg = c("0 (No)", "1 (Yes)"))
```

**Observations:** Age is roughly normal (30-60), Tenure is right-skewed, NumSeats clusters around 2-4, Total Spent is heavily right-skewed. Response variable Y01 is reasonably balanced.

## Step 2: Review Variables for Common Sense (SME Knowledge)

Standardize variable names and check for unique identifiers.

```{r step2_common_sense, echo=FALSE}
# Clean up variable names - replace dots/spaces with underscores, capitalize
names(FFdf) <- gsub("\\.", "_", names(FFdf))
names(FFdf) <- gsub(" ", "_", names(FFdf))
names(FFdf) <- gsub("(^|_)([a-z])", "\\1\\U\\2", names(FFdf), perl = TRUE)

cat("Dataset:", nrow(FFdf), "rows,", ncol(FFdf), "columns\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "out of", nrow(FFdf), "rows\n")
```

**Result:** Each row does not necessarily represent a unique customer (175 duplicate Cust_IDs found in MainDF source data).

## Step 3: Review How Software Coded Variables

Convert character variables to factors for proper categorical analysis.

```{r step3_variable_types, echo=FALSE}
# Identify character variables
chr_vars <- names(FFdf)[sapply(FFdf, is.character)]
cat("Character variables to convert:", length(chr_vars), "\n")

# Convert character variables to factors
for(var in chr_vars) {
  FFdf[[var]] <- as.factor(FFdf[[var]])
}

# Show key factor levels
cat("\nSex levels:", levels(FFdf$Sex), "\n")
cat("Marital levels:", levels(FFdf$Marital), "\n")
cat("Account_Type levels:", levels(FFdf$Account_Type), "\n")
```

**Note:** Marital has levels D (Divorced), M (Married), S (Single), U (Unknown). The "U" for Unknown may need special handling.

## Step 4: Data Integrity/Validation Checks

Check for anomalies, bogus values, and data quality issues.

```{r step4_checks, echo=FALSE}
# Check numeric variable ranges
cat("Age range:", range(FFdf$Age, na.rm = TRUE), "\n")
cat("Tenure range:", range(FFdf$Tenure, na.rm = TRUE), "\n")
cat("NumSeats range:", range(FFdf$NumSeats, na.rm = TRUE), "\n")

# Check for 999 placeholder values in DistA
cat("\nDistA values = 999:", sum(FFdf$DistA == 999, na.rm = TRUE), "\n")

# Convert 999 to NA (placeholder for missing)
FFdf$DistA[FFdf$DistA == 999] <- NA
cat("DistA NA count after conversion:", sum(is.na(FFdf$DistA)), "\n")

# Check Survey_Comp for outliers (expected range 0-1)
cat("\nSurvey_Comp range:", range(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("Survey_Comp values > 1:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")

# Check for redundant columns
cat("\nState_Name unique:", length(unique(FFdf$State_Name)),
    "| State_Loc unique:", length(unique(FFdf$State_Loc)), "\n")
```

**Issues Found:**

- **DistA = 999:** Placeholder values converted to NA
- **Survey_Comp > 1:** Outlier found when expected range is 0-1
- **Age max = 99:** May be placeholder or extreme value - verify with SME
- **Tenure max = 400:** Suspicious value if measured in years - verify units with SME
- **State_Name/State_Loc:** Redundant columns (same info, different format)
- **Marital = "U":** Unknown status - consider treating as NA
- **Cust_ID duplicates:** 175 duplicate IDs found in MainDF source data
- **Address, Name, PhoneNum:** 100% missing - likely removed from CustomerDF for privacy

## Step 5: Handle Dates

Convert Last_Contact datetime and extract useful components.

```{r step5_handle_dates, echo=FALSE}
# Convert Last_Contact to datetime format
FFdf$Last_Contact <- ymd_hms(as.character(FFdf$Last_Contact))

# Extract date components
FFdf$Contact_Year <- year(FFdf$Last_Contact)
FFdf$Contact_Month <- month(FFdf$Last_Contact)
FFdf$Contact_Day <- day(FFdf$Last_Contact)
FFdf$Contact_Weekday <- wday(FFdf$Last_Contact, label = TRUE)
FFdf$Contact_Hour <- hour(FFdf$Last_Contact)

cat("Date components extracted: Contact_Year, Contact_Month, Contact_Day, Contact_Weekday, Contact_Hour\n")
cat("Contact Year range:", range(FFdf$Contact_Year, na.rm = TRUE), "\n")
cat("Contact Hour range:", range(FFdf$Contact_Hour, na.rm = TRUE), "\n")
```

## Summary

```{r summary, echo=FALSE}
cat("Final Dataset:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
cat("Total Missing Values:", sum(is.na(FFdf)), "\n")

# Show columns with missing values
na_counts <- colSums(is.na(FFdf))
na_cols <- na_counts[na_counts > 0]
cat("\nColumns with missing values:\n")
print(sort(na_cols, decreasing = TRUE))
```

### Data Quality Issues for Future Steps:

| Issue | Possible action / Action Taken |
|-------|--------------|
| DistA = 999 | Converted to NA |
| Survey_Comp > 1 | Flag for investigation (110 values, max 7.4) |
| Age = 99 | Verify with SME |
| Tenure = 400 | Verify units with SME (400 years unlikely) |
| Marital = "U" | Consider as NA or keep |
| State redundancy | Drop one column |
| ID columns | Exclude from modeling |
| Cust_ID duplicates | 175 duplicates in MainDF - investigate or deduplicate |
| PII columns | Address, Name, PhoneNum 100% missing - exclude |



**Step 6: Handle Categorical Variables - keep as is, combine rare levels, combine similar levels**


```{r step6_categorical, echo=TRUE}
# Get all factor variables
factor_vars <- names(FFdf)[sapply(FFdf, is.factor)]
cat("Factor variables:", length(factor_vars), "\n")

# Function to check for rare levels (less than 5% of data)
check_rare_levels <- function(df, threshold = 0.05) {
  factor_vars <- names(df)[sapply(df, is.factor)]
  rare_info <- list()

  for(var in factor_vars) {
    tbl <- table(df[[var]], useNA = "ifany")
    pct <- prop.table(tbl)
    rare_levels <- names(pct[pct < threshold])
    if(length(rare_levels) > 0) {
      rare_info[[var]] <- data.frame(
        Level = rare_levels,
        Count = as.numeric(tbl[rare_levels]),
        Percent = round(as.numeric(pct[rare_levels]) * 100, 2)
      )
    }
  }
  return(rare_info)
}

# Check for rare levels
rare_levels <- check_rare_levels(FFdf, threshold = 0.05)
cat("\nVariables with rare levels (< 5%):\n")
print(names(rare_levels))

# Display level distributions for key categorical variables
cat("\n--- Sex Distribution ---\n")
print(table(FFdf$Sex, useNA = "ifany"))

cat("\n--- Marital Distribution ---\n")
print(table(FFdf$Marital, useNA = "ifany"))

cat("\n--- Account_Type Distribution ---\n")
print(table(FFdf$Account_Type, useNA = "ifany"))

cat("\n--- Educational_Level Distribution ---\n")
print(table(FFdf$Educational_Level, useNA = "ifany"))
```

```{r step6_handle_categorical, echo=TRUE}
# Handle Marital "U" (Unknown) - treat as NA for analysis purposes
# Create backup first
FFdf$Marital_Original <- FFdf$Marital

# Option: Convert "U" to NA (uncomment if desired)
# FFdf$Marital[FFdf$Marital == "U"] <- NA
# FFdf$Marital <- droplevels(FFdf$Marital)

cat("Marital 'U' (Unknown) count:", sum(FFdf$Marital == "U", na.rm = TRUE), "\n")
cat("Decision: Keep 'U' as separate level for now - may represent meaningful unknown status\n")

# Check for levels that might mean the same thing
cat("\n--- State_Name levels ---\n")
print(length(levels(FFdf$State_Name)))
cat("Number of unique states:", length(unique(FFdf$State_Name)), "\n")
```

```{r step6_lump_rare_levels, echo=TRUE}
# Lump rare factor levels into "Other" using forcats::fct_lump_prop()
# Threshold: levels appearing in < 5% of non-NA observations are combined

# Store original levels for reference
factor_vars_to_lump <- names(FFdf)[sapply(FFdf, is.factor)]

# Exclude variables we handle separately (State_Name -> Region, Marital already reviewed)
skip_vars <- c("State_Name", "State_Loc", "Marital", "Marital_Original","Educational_Level","Seating_Location","Favorite_Caps_Player","Job_Sector")
lump_vars <- setdiff(factor_vars_to_lump, skip_vars)

cat("=== LUMPING RARE LEVELS (< 5%) INTO 'Other' ===\n\n")

for(var in lump_vars) {
  original_levels <- nlevels(FFdf[[var]])
  FFdf[[var]] <- fct_lump_prop(FFdf[[var]], prop = 0.05, other_level = "Other")
  new_levels <- nlevels(FFdf[[var]])

  if(original_levels != new_levels) {
    cat(var, ": ", original_levels, " -> ", new_levels, " levels\n", sep = "")
    print(table(FFdf[[var]], useNA = "ifany"))
    cat("\n")
  }
}

cat("Lumping complete.\n")
```

```{r step6_group_states_by_region, echo=TRUE}


# Define region mappings
northeast <- c("Connecticut", "Maine", "Massachusetts", "New Hampshire",
               "Rhode Island", "Vermont", "New Jersey", "New York", "Pennsylvania")

midwest <- c("Illinois", "Indiana", "Michigan", "Ohio", "Wisconsin",
             "Iowa", "Kansas", "Minnesota", "Missouri", "Nebraska",
             "North Dakota", "South Dakota")

south <- c("Delaware", "Florida", "Georgia", "North Carolina",
           "South Carolina", "West Virginia",
           "Alabama", "Kentucky", "Mississippi", "Tennessee",
           "Arkansas", "Louisiana", "Oklahoma", "Texas")

west <- c("Arizona", "Colorado", "Idaho", "Montana", "Nevada", "New Mexico",
          "Utah", "Wyoming", "Alaska", "California", "Hawaii", "Oregon", "Washington")

dcarea <- c("Maryland","Virginia", "District of Columbia")

# Create Region variable
FFdf$Region <- case_when(
  FFdf$State_Name %in% northeast ~ "Northeast",
  FFdf$State_Name %in% midwest ~ "Midwest",
  FFdf$State_Name %in% south ~ "South",
  FFdf$State_Name %in% west ~ "West",
  FFdf$State_Name %in% dcarea ~ "DCArea",
  TRUE ~ "Other"  # Catch any unmatched states
)

# Convert to factor
FFdf$Region <- as.factor(FFdf$Region)

# Display region distribution
cat("--- Region Distribution (grouped from State_Name) ---\n")
print(table(FFdf$Region, useNA = "ifany"))
cat("\nRegion percentages:\n")
print(round(prop.table(table(FFdf$Region)) * 100, 2))

# Check if any states weren't mapped
unmatched_states <- unique(FFdf$State_Name[FFdf$Region == "Other"])
if(length(unmatched_states) > 0) {
  cat("\nWarning - Unmatched states assigned to 'Other':\n")
  print(unmatched_states)
} else {
  cat("\nAll states successfully mapped to regions.\n")
}
```

**Step 6 Observations:**

- Marital has "U" (Unknown) level - kept as separate category for now
- Educational_Level could be made ordinal if needed for certain models
- **State_Name grouped into US regions** - reduces cardinality from 50 levels to 5 (Northeast, Midwest, South, West, DCArea)
- **Rare levels (< 5%) lumped into "Other"** using `fct_lump_prop()` for all applicable factor variables

---

**Step 7: Remove Zero-Variance Predictors**



```{r step7_zero_variance, echo=TRUE}
# Function to identify zero-variance columns
find_zero_variance <- function(df) {
  zv_cols <- c()
  for(col in names(df)) {
    unique_vals <- length(unique(na.omit(df[[col]])))
    if(unique_vals <= 1) {
      zv_cols <- c(zv_cols, col)
    }
  }
  return(zv_cols)
}

# Find zero-variance columns
zero_var_cols <- find_zero_variance(FFdf)
cat("Zero-variance columns found:", length(zero_var_cols), "\n")

if(length(zero_var_cols) > 0) {
  cat("Columns with zero variance:\n")
  print(zero_var_cols)

  # Store in excluded columns list
  excluded_cols <- zero_var_cols

  # Remove zero-variance columns
  FFdf <- FFdf[, !names(FFdf) %in% zero_var_cols]
  cat("\nRemoved", length(zero_var_cols), "zero-variance columns\n")
} else {
  cat("No zero-variance columns found\n")
  excluded_cols <- c()
}

cat("Remaining columns:", ncol(FFdf), "\n")
```

**Step 7 Results:**

The following 8 zero-variance columns were identified and removed:

- **Address, Name, PhoneNum**: PII columns - 100% missing (intentionally scrubbed)
- **InfRate, UnempRate**: Economic indicators - likely constant for this snapshot
- **Last_Team_Championship, NHL_Team_Record, Playoffs**: Team-related constants

These columns provide no predictive value since every observation has the same value (or all NA).

---

## Class 4: Data Cleaning Process: Steps 8-11

**Step 8: Handle Near Zero-Variance Predictors**


```{r step8_near_zero_variance, echo=TRUE}
# Function to find near-zero variance columns
# A column is NZV if one value dominates (e.g., >95% of values)
find_near_zero_variance <- function(df, threshold = 0.95) {
  nzv_info <- data.frame(
    Variable = character(),
    DominantValue = character(),
    DominantPct = numeric(),
    UniqueValues = integer(),
    stringsAsFactors = FALSE
  )

  for(col in names(df)) {
    if(is.numeric(df[[col]]) || is.factor(df[[col]])) {
      tbl <- table(df[[col]], useNA = "no")
      if(length(tbl) > 0) {
        max_pct <- max(tbl) / sum(tbl)
        if(max_pct >= threshold) {
          dominant_val <- names(tbl)[which.max(tbl)]
          nzv_info <- rbind(nzv_info, data.frame(
            Variable = col,
            DominantValue = as.character(dominant_val),
            DominantPct = round(max_pct * 100, 2),
            UniqueValues = length(tbl),
            stringsAsFactors = FALSE
          ))
        }
      }
    }
  }
  return(nzv_info)
}

# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)
```

```{r step8_handle_nzv, echo=TRUE}
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable

if(length(nzv_variables) > 0) {
  cat("Near-zero variance variables to monitor:\n")
  print(nzv_variables)
  cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
  cat("No significant near-zero variance issues found\n")
}
```

**Step 8 Results:**

Near-zero variance columns identified (>95% one value):

| Variable | Dominant Value | Dominant % |
|----------|----------------|------------|
| Additional_Seats | 0 | 96.99% |
| Mult_Loc | No | 96.99% |

**Observations:**

- **Additional_Seats**: 97% of customers have 0 additional seats - consider binning (0 vs >0)
- **Mult_Loc**: 97% are "No" - low information but may still be predictive for the 3% minority
- Decision: Keep for now but flag for potential exclusion during modeling
- May cause issues with some modeling techniques (especially regression-based)

---

**Step 9: Remove Redundant Columns and Linear Combination Columns**


```{r step9_redundant, echo=TRUE}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
  # Check if they're perfectly correlated
  state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
  cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
  cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")

  # If one-to-one mapping, they're redundant
  cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
  cat("Removing State_Loc (keeping State_Name)\n")

  excluded_cols <- c(excluded_cols, "State_Loc")
  FFdf$State_Loc <- NULL
}

# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]

if(length(numeric_vars) > 1) {
  # Calculate correlation matrix (handling NAs)
  cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")

  # Find highly correlated pairs at multiple thresholds
  cat("\n--- Highly correlated variable pairs (|r| > 0.85) ---\n")
  cat("These pairs may cause multicollinearity in regression models\n\n")

  high_cor_85 <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind = TRUE)

  if(nrow(high_cor_85) > 0) {
    cor_pairs <- data.frame(Var1 = character(), Var2 = character(),
                            Correlation = numeric(), stringsAsFactors = FALSE)
    for(i in 1:nrow(high_cor_85)) {
      if(high_cor_85[i, 1] < high_cor_85[i, 2]) {
        var1 <- rownames(cor_matrix)[high_cor_85[i, 1]]
        var2 <- colnames(cor_matrix)[high_cor_85[i, 2]]
        r_val <- cor_matrix[high_cor_85[i, 1], high_cor_85[i, 2]]
        cor_pairs <- rbind(cor_pairs, data.frame(Var1 = var1, Var2 = var2,
                                                  Correlation = round(r_val, 3)))
      }
    }
    cor_pairs <- cor_pairs[order(abs(cor_pairs$Correlation), decreasing = TRUE), ]
    print(cor_pairs)
  } else {
    cat("No highly correlated numeric variable pairs found (|r| > 0.85)\n")
  }
}
```

```{r step9_correlation_plot, echo=TRUE, fig.width=10, fig.height=8}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
  # Subset to variables with fewer missing values for cleaner plot
  complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]

  if(length(complete_vars) > 2) {
    cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
    corrplot(cor_subset, method = "color", type = "upper",
             tl.cex = 0.7, tl.col = "black",
             title = "Correlation Matrix - Numeric Variables",
             mar = c(0, 0, 2, 0))
  }
}
```

```{r step9_handle_multicollinearity, echo=TRUE}
# Address multicollinearity based on correlation matrix analysis
cat("=== MULTICOLLINEARITY ANALYSIS ===\n\n")

# Identify correlated variable clusters from the correlation matrix
cat("Cluster 1: Spending & Visit variables\n")
cat("  - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)\n")
cat("  - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)\n")
cat("  Recommendation: Consider removing Rep_Visits or Total_Spent\n\n")

cat("Cluster 2: Concession & Seating\n")
cat("  - Concession_Total <-> NumSeats: r = 0.915 (strong positive)\n")
cat("  Recommendation: Makes business sense - more seats = more concessions\n\n")

cat("Cluster 3: Attendance pairs\n")
cat("  - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)\n")
cat("  Note: Negative correlation suggests inverse relationship\n")
cat("  Recommendation: Keep both - they capture different behaviors\n\n")

# Create list of variables to flag for multicollinearity
multicollinear_vars <- c("Rep_Visits", "Team_Store_Total")
cat("Variables flagged for potential removal due to multicollinearity:\n")
print(multicollinear_vars)

# Option: Remove highly correlated variables (uncomment to execute)
# FFdf <- FFdf[, !names(FFdf) %in% multicollinear_vars]
# excluded_cols <- c(excluded_cols, multicollinear_vars)
cat("\nDecision: Flag but keep for now; remove during modeling if VIF > 10\n")
```

**Step 9 Observations:**

Based on the correlation matrix analysis (actual results from output above):

| Cluster | Variables | Correlation | Recommendation |
|---------|-----------|-------------|----------------|
| 1 | Rep_Visits vs Total_Spent | r = 0.947 | Remove Rep_Visits |
| 1 | Team_Store_Total vs Total_Spent | r = 0.853 | Monitor for VIF |
| 2 | Concession_Total vs NumSeats | r = 0.915 | Keep - business logic |
| 3 | Weekday_Attended vs Weekday_Sold | r = -0.946 | Keep both - inverse relationship |
| - | State_Loc vs State_Name | Redundant | **REMOVED** |

**Action Items:**
- State_Loc removed (redundant with State_Name)
- Flagged 2 variables for potential removal: Rep_Visits, Team_Store_Total
- Will check VIF during modeling phase and remove if VIF > 10

---

**Step 10: Search for Outliers and Initial Search for Missing Values**

```{r step10_outliers, echo=TRUE, fig.width=10, fig.height=8}
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))

# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")

# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")

# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)

# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)

# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")

# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)

par(mfrow = c(1, 1))
```

```{r step10_missing_assessment, echo=TRUE, fig.width=10, fig.height=6}
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")

# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
    "(", round(total_missing/total_cells * 100, 2), "%)\n\n")

# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)

cat("Columns with missing values:\n")
na_summary <- data.frame(
  Variable = names(na_by_col),
  Missing_Count = as.numeric(na_by_col),
  Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)

```

```{r step10_outlier_decisions, echo=TRUE}
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")

# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")

# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")

# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")

# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)

cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
```

---

**Step 11: Sanity Check Using Decision Tree (1 to 2 splits)**


```{r step11_decision_tree, echo=TRUE, fig.width=10, fig.height=8}
# Prepare data for decision tree
# Exclude ID columns, flag variables, and high-cardinality variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
                       "Marital_Original", "Last_Contact", "Contact_Year",
                       "Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour",
                       "State_Name", "Zip_Codes")

tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]

# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]

# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)

# Build a simple decision tree (max 2-3 splits for sanity check)
# Uses Region variable (from Step 6) instead of high-cardinality State_Name/Zip_Codes
sanity_tree <- rpart(Y01 ~ .,
                     data = tree_data,
                     method = "class",
                     control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))

# Plot the tree
rpart.plot(sanity_tree,
           main = "Sanity Check Decision Tree (max depth = 3)",
           extra = 104,  # Show percentage and count
           box.palette = "RdYlGn")

# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
  var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
  print(head(var_imp, 10))
} else {
  cat("No variables selected by the tree\n")
}
```

```{r step11_tree_analysis, echo=TRUE}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")

# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")

if(accuracy > 0.90) {
  cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
  cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
```

**Step 11 Results:**

The decision tree uses Region (from Step 6) instead of raw State_Name/Zip_Codes to avoid overfitting from high-cardinality variables.

Top Variable Importance:

| Variable | Importance |
|----------|------------|
| Region | 1505.2 |
| Favorite_Team | 253.5 |
| PerUsed | 245.2 |
| DistA | 196.8 |
| Tenure | 187.7 |

**Analysis:**

- **Accuracy ~88%** - reasonable, no obvious data leakage
- **Region is the dominant predictor** - geographic location strongly predicts Y01
- **No high-cardinality variables** causing artificial inflation of accuracy

---

## Summary of Steps 6-11 (Data Cleaning Complete)

```{r final_summary, echo=TRUE}
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")

cat("Step 6 - Handle Categorical Variables:\n")
cat("  - Rare factor levels (< 5%) lumped into 'Other' via fct_lump_prop()\n")
cat("  - Marital 'U' kept as separate category\n")
cat("  - State_Name grouped into US Census regions\n\n")

cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")

cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n")
cat("  - Decision: Keep for now but monitor during modeling\n\n")

cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n")
cat("  - Correlation matrix reviewed for multicollinearity\n\n")

cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n")
cat("  - Missing data summary table generated\n\n")

cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Tree accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Review variable importance for potential data leakage\n\n")

cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")

# Save cleaned dataset for next phase (Missing Data)
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
```

### Issues Identified for Further Action:

| Step | Issue | Recommendation |
|------|-------|----------------|
| 6 | Marital "U" unknown | Keep as category or convert to NA during imputation |
| 6 | Rare factor levels (< 5%) | **RESOLVED:** Lumped into "Other" via fct_lump_prop() |
| 6 | State_Name high cardinality | **RESOLVED:** Grouped into US Census regions |
| 7 | Zero-variance columns | Removed from dataset |
| 8 | Near-zero variance | Monitor during modeling; consider binning |
| 9 | State_Loc redundant | Removed |
| 10 | Tenure = 400 | Verify units with SME (years vs months?) |
| 10 | Survey_Comp > 1 | Investigate scale/cap values at 1 |
| 10 | Missing data patterns | Address in Missing Data phase (Class 5+) |
| 11 | Tree predictors | **RESOLVED:** Using Region variable instead of State_Name |

---

## Missing Data Analysis: Steps 1-5

We join this program already in progress with a cleaned flat file (FFdf). The Missing Data process follows a separate set of steps from Data Cleaning. These steps prepare the dataset for imputation (Steps 6-7 of the MD process, covered later).

**Missing Data Steps:**

1. Identify missing data
2. "Mark" missing data (create indicator variables)
3. Clean up any obvious mistakes
4. Make easy decisions on Rows/Columns
5. Assess missingness patterns (MCAR vs MAR)

---

### MD Step 1: Identify Missing Data

We summarize the extent and location of missing values across all variables in the cleaned flat file. This uses `is.na()`, `colSums()`, `md.pattern()` from the `mice` package, and `aggr()` from the `VIM` package.

```{r md_step1_identify, echo=TRUE, fig.width=10, fig.height=8}
# Rename to MDdf to follow course convention
MDdf <- FFdf

cat("=== MISSING DATA IDENTIFICATION ===\n")
cat("Dataset:", nrow(MDdf), "rows x", ncol(MDdf), "columns\n")
cat("Total missing values:", sum(is.na(MDdf)), "\n")
cat("Total cells:", nrow(MDdf) * ncol(MDdf), "\n")
cat("Overall missing %:", round(sum(is.na(MDdf)) / (nrow(MDdf) * ncol(MDdf)) * 100, 2), "%\n\n")

# Missing values by column - sorted descending
na_counts <- colSums(is.na(MDdf))
na_cols <- na_counts[na_counts > 0]
na_sorted <- sort(na_cols, decreasing = TRUE)

# Create a summary table
missing_summary <- data.frame(
  Variable = names(na_sorted),
  Missing_Count = as.numeric(na_sorted),
  Missing_Pct = round(as.numeric(na_sorted) / nrow(MDdf) * 100, 1),
  Present_Count = nrow(MDdf) - as.numeric(na_sorted)
)

cat("=== COLUMNS WITH MISSING DATA ===\n")
print(missing_summary, row.names = FALSE)

# Flextable for formatted output
flextable(missing_summary) %>%
  set_header_labels(Variable = "Variable", Missing_Count = "Missing (n)",
                    Missing_Pct = "Missing (%)", Present_Count = "Present (n)") %>%
  colformat_double(j = "Missing_Pct", digits = 1) %>%
  autofit()
```

```{r md_step1_pattern, echo=TRUE, fig.width=12, fig.height=8}
# md.pattern from mice - shows patterns of missingness across variables
cat("=== MISSING DATA PATTERN (mice::md.pattern) ===\n")
md.pattern(MDdf, plot = TRUE, rotate.names = TRUE)
```

```{r md_step1_aggr, echo=TRUE, fig.width=12, fig.height=8}
# VIM aggr plot - visual aggregation of missing data
aggr(MDdf,
     col = c("steelblue", "red"),
     numbers = TRUE,
     sortVars = TRUE,
     labels = names(MDdf),
     cex.axis = 0.5,
     gap = 3,
     ylab = c("Proportion Missing", "Pattern"))
```

**MD Step 1 Observations:**

- **Total missing:** 67,997 values out of 557,373 cells (12.2% overall)
- **High missingness (>50%):** 7 variables from the Customer table are all missing at exactly 70.0% (6,612 values each): Educational_Level, Favorite_Caps_Player, Favorite_Sport, Job_Sector, Mode_Of_Transport, Team_B_STH, Team_C_STH. This identical count strongly suggests these are missing as a block -- likely from customers who did not complete a supplemental survey.
- **Net_Worth_True (61.0%)** and **HouseHold_Income_True (60.2%)** are also very high -- sensitive financial information that many customers may not disclose.
- **DistA (30.6%):** Distance variable, partially from 999 placeholder conversion in Step 4 of cleaning plus original NAs.
- **Moderate missingness (5-15%):** Marital (12.2%), Age (10.4%), Rep_Name (9.2%), Sex (7.1%), Tenure (6.3%), Rep_Visits (5.4%), Rep_Calls (4.6%), Num_Children (4.3%).
- The `md.pattern()` output shows the patterns of co-occurrence of missingness -- the 7 CustomerDF variables always go missing together, confirming a block pattern.

---

### MD Step 2: Mark Missing Data (Create Indicator Variables)

We create binary indicator variables (0 = present, 1 = missing) for every variable with missing data. These indicators serve three purposes: (1) allow testing for MCAR and MAR, (2) can sometimes be used as predictors themselves, and (3) maintain integrity of the original variables after imputation.

```{r md_step2_mark, echo=TRUE}
cat("=== CREATING MISSING DATA INDICATORS ===\n\n")

# Get variables with missing values
vars_with_na <- names(MDdf)[colSums(is.na(MDdf)) > 0]

# Exclude Marital_Original (backup column) and Flag_Tenure_High (derived from Tenure)
vars_with_na <- vars_with_na[!vars_with_na %in% c("Marital_Original", "Flag_Tenure_High")]

cat("Creating indicators for", length(vars_with_na), "variables:\n")
cat(vars_with_na, sep = ", ")
cat("\n\n")

# Create indicator variables: 1 = missing, 0 = present
for(var in vars_with_na) {
  indicator_name <- paste0("M_", var)
  MDdf[[indicator_name]] <- ifelse(is.na(MDdf[[var]]), 1, 0)
}

# Display indicator summary
indicator_vars <- grep("^M_", names(MDdf), value = TRUE)
cat("Indicator variables created:", length(indicator_vars), "\n\n")

# Verify indicators match original NA counts
indicator_check <- data.frame(
  Variable = vars_with_na,
  Original_NA = sapply(vars_with_na, function(v) sum(is.na(MDdf[[v]]))),
  Indicator_Sum = sapply(vars_with_na, function(v) sum(MDdf[[paste0("M_", v)]]))
)
cat("=== VERIFICATION: Indicators match original NA counts ===\n")
print(indicator_check, row.names = FALSE)
cat("\nAll match:", all(indicator_check$Original_NA == indicator_check$Indicator_Sum), "\n")
```

```{r md_step2_correlations, echo=TRUE, fig.width=10, fig.height=8}
# Check correlations between missing indicators
# High correlation = variables tend to be missing together
indicator_matrix <- MDdf[, indicator_vars]

# Only use indicators with variance > 0
non_constant <- sapply(indicator_matrix, function(x) var(x) > 0)
indicator_matrix <- indicator_matrix[, non_constant]

cor_indicators <- cor(indicator_matrix)

cat("=== CORRELATION OF MISSING INDICATORS ===\n")
cat("High correlations suggest variables are missing as a block\n\n")

# Find pairs with correlation > 0.8
high_cor <- which(abs(cor_indicators) > 0.8 & abs(cor_indicators) < 1, arr.ind = TRUE)
if(nrow(high_cor) > 0) {
  pairs_shown <- c()
  for(i in 1:nrow(high_cor)) {
    if(high_cor[i,1] < high_cor[i,2]) {
      v1 <- rownames(cor_indicators)[high_cor[i,1]]
      v2 <- colnames(cor_indicators)[high_cor[i,2]]
      r_val <- cor_indicators[high_cor[i,1], high_cor[i,2]]
      cat(sprintf("  %s <-> %s : r = %.3f\n", v1, v2, r_val))
    }
  }
}

corrplot(cor_indicators, method = "color", type = "upper",
         tl.cex = 0.7, tl.col = "black",
         title = "Correlation of Missing Data Indicators",
         mar = c(0, 0, 2, 0))
```

**MD Step 2 Observations:**

- Created 18 binary indicator variables (M_VariableName) for each variable with missing data.
- The 7 CustomerDF variables (Educational_Level, Favorite_Caps_Player, Favorite_Sport, Job_Sector, Mode_Of_Transport, Team_B_STH, Team_C_STH) have perfect correlation (r = 1.0) among their indicators, confirming they are missing as a single block.
- Net_Worth_True and HouseHold_Income_True indicators also show moderate-to-high correlation, suggesting they tend to be missing together (both are financial variables).
- This block pattern is important: it means these 7 variables are likely missing because a subset of customers never completed a secondary survey, not because of random chance.

---

### MD Step 3: Clean Up Obvious Mistakes

Review variables for values that should actually be coded as NA or corrected. This includes placeholder values, impossible values, and categorical codes that represent "unknown" or "missing."

```{r md_step3_cleanup, echo=TRUE}
cat("=== CLEANING UP OBVIOUS MISTAKES ===\n\n")

# --- 3a. Marital "U" = Unknown ---
cat("3a. Marital 'U' (Unknown):\n")
cat("    Count of 'U':", sum(MDdf$Marital == "U", na.rm = TRUE), "\n")
cat("    Decision: KEEP 'U' as a valid level. Although 'Unknown' could be treated as\n")
cat("    missing, we retain it as a distinct category to preserve information and\n")
cat("    see how it behaves in modeling.\n")
cat("    Marital levels:", levels(MDdf$Marital), "\n")
cat("    Marital NA count:", sum(is.na(MDdf$Marital)), "\n\n")

# --- 3b. Survey_Comp values > 1 (expected 0-1 proportion) ---
cat("3b. Survey_Comp values > 1 (expected range 0-1):\n")
cat("    Count > 1:", sum(MDdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("    Max value:", max(MDdf$Survey_Comp, na.rm = TRUE), "\n")
cat("    Decision: Cap at 1.0 -- values above 1 on a proportion scale are data entry errors\n")
MDdf$Survey_Comp[MDdf$Survey_Comp > 1] <- 1.0
cat("    Survey_Comp range after cap:", range(MDdf$Survey_Comp, na.rm = TRUE), "\n\n")

# --- 3c. Mult_Loc has "Y" and "Yes" (same meaning) ---
cat("3c. Mult_Loc has inconsistent levels:\n")
print(table(MDdf$Mult_Loc, useNA = "ifany"))
cat("    Decision: Combine 'Y' into 'Yes'\n")
MDdf$Mult_Loc[MDdf$Mult_Loc == "Y"] <- "Yes"
MDdf$Mult_Loc <- droplevels(MDdf$Mult_Loc)
cat("    After fix:\n")
print(table(MDdf$Mult_Loc, useNA = "ifany"))
cat("\n")

# --- 3d. Most_Purch_Concession has blank entries ---
cat("3d. Most_Purch_Concession blank entries:\n")
blank_count <- sum(MDdf$Most_Purch_Concession == "", na.rm = TRUE)
cat("    Blank entries:", blank_count, "\n")
if(blank_count > 0) {
  cat("    Decision: Convert blanks to NA\n")
  MDdf$Most_Purch_Concession[MDdf$Most_Purch_Concession == ""] <- NA
  MDdf$Most_Purch_Concession <- droplevels(MDdf$Most_Purch_Concession)
  # Create/update indicator for this newly-missing variable
  MDdf$M_Most_Purch_Concession <- ifelse(is.na(MDdf$Most_Purch_Concession), 1, 0)
  indicator_vars <- grep("^M_", names(MDdf), value = TRUE)
}
cat("\n")

# --- 3e. Tenure = 400 (impossible if years) ---
cat("3e. Tenure extreme values:\n")
cat("    Values > 100:", sum(MDdf$Tenure > 100, na.rm = TRUE), "\n")
cat("    Max tenure:", max(MDdf$Tenure, na.rm = TRUE), "\n")
cat("    Decision: Leave as-is for now. Units may be months (400 months = 33 years, plausible).\n")
cat("    Flagged via Flag_Tenure_High for SME review.\n\n")

# --- 3f. Age = 99 check ---
cat("3f. Age extreme values:\n")
cat("    Values = 99:", sum(MDdf$Age == 99, na.rm = TRUE), "\n")
cat("    Age range:", range(MDdf$Age, na.rm = TRUE), "\n")
cat("    Decision: Leave as-is -- could be legitimate. SME review needed.\n\n")

cat("=== STEP 3 COMPLETE ===\n")
cat("Total missing after cleanup:", sum(is.na(MDdf)), "\n")
```

**MD Step 3 Observations:**

- **Marital "U":** 248 values kept as "U" (Unknown) -- a valid distinct category. Retaining "U" preserves information and lets us see how unknown marital status behaves in the model. Marital still has 1,155 true NAs to impute.
- **Survey_Comp > 1:** 110 values capped at 1.0. A completion proportion cannot exceed 100%, so these were data entry errors.
- **Mult_Loc "Y" vs "Yes":** 72 "Y" entries combined with "Yes" -- these represent the same category with inconsistent coding.
- **Most_Purch_Concession blanks:** 85 blank entries converted to NA -- blanks in a categorical variable are missing data, not a valid level.
- **Tenure = 400 and Age = 99:** Left as-is pending SME verification. These are flagged but not corrected without domain knowledge.

---

### MD Step 4: Make Easy Decisions on Rows/Columns

Evaluate which rows or columns should be excluded from imputation based on irrelevance. We do NOT delete data -- we exclude non-analytical columns from the imputation process while keeping the original intact. **Decision: We will impute ALL variables with missing data**, including those with high missingness (>50%), to see the full effect on the model.

```{r md_step4_decisions, echo=TRUE}
cat("=== EASY DECISIONS ON ROWS/COLUMNS ===\n\n")

# --- 4a. Column Decisions ---
cat("--- COLUMN DECISIONS ---\n\n")

# Recalculate missing after Step 3 cleanup
na_counts_updated <- colSums(is.na(MDdf))
na_cols_updated <- na_counts_updated[na_counts_updated > 0]
na_sorted_updated <- sort(na_cols_updated, decreasing = TRUE)

for(i in seq_along(na_sorted_updated)) {
  pct <- round(na_sorted_updated[i] / nrow(MDdf) * 100, 1)
  cat(sprintf("%-25s : %4d (%5.1f%%)\n", names(na_sorted_updated)[i], na_sorted_updated[i], pct))
}

# Identify columns to EXCLUDE from imputation (only non-analytical columns)
cat("\n--- COLUMNS EXCLUDED FROM IMPUTATION (non-analytical only) ---\n\n")

# Identifier columns - not predictors
id_cols <- c("ID", "Cust_ID")
cat("Identifier columns (not predictors):", paste(id_cols, collapse = ", "), "\n")

# Backup/derived columns
backup_cols <- c("Marital_Original", "Flag_Tenure_High", "Flag_Survey_Invalid")
cat("Backup/derived columns:", paste(backup_cols, collapse = ", "), "\n")

# Date components (derived from Last_Contact)
date_cols <- c("Last_Contact", "Contact_Year", "Contact_Month", "Contact_Day",
               "Contact_Weekday", "Contact_Hour")
cat("Date columns (derived):", paste(date_cols, collapse = ", "), "\n")

# High-cardinality columns not useful for imputation
high_card_cols <- c("State_Name", "Zip_Codes", "Seating_Location")
cat("High-cardinality columns:", paste(high_card_cols, collapse = ", "), "\n")

# Indicator columns (used for testing, not for imputation)
indicator_cols <- grep("^M_", names(MDdf), value = TRUE)
cat("Indicator columns:", length(indicator_cols), "M_ variables\n")

# All columns to exclude
all_exclude <- c(id_cols, backup_cols, date_cols, high_card_cols, indicator_cols)
cat("\nTotal columns excluded from imputation:", length(all_exclude), "\n")

# --- 4b. HIGH MISSINGNESS: IMPUTE ANYWAY ---
cat("\n--- HIGH MISSINGNESS COLUMN DECISIONS ---\n\n")

cat("NOTE: Normally variables with >50% missing would be excluded from imputation.\n")
cat("However, we are choosing to impute ALL missing values to observe the full\n")
cat("effect on the modeling results. This is not best practice but is instructive.\n\n")

cat("CustomerDF block (70% missing each - 7 variables):\n")
cat("  Educational_Level, Favorite_Caps_Player, Favorite_Sport, Job_Sector,\n")
cat("  Mode_Of_Transport, Team_B_STH, Team_C_STH\n")
cat("  Decision: IMPUTE. These are missing as a block (supplemental survey).\n")
cat("  Imputing 70% is essentially manufacturing data, but we proceed to see\n")
cat("  the impact on the model.\n\n")

cat("Net_Worth_True (61.0% missing):\n")
cat("  Decision: IMPUTE. High missingness but we proceed as planned.\n\n")

cat("HouseHold_Income_True (60.2% missing):\n")
cat("  Decision: IMPUTE. High missingness but we proceed as planned.\n\n")

# NO high_miss_exclude -- impute everything
high_miss_exclude <- c()

# --- 4c. Row Decisions ---
cat("\n--- ROW DECISIONS ---\n\n")

# Check how many rows have excessive missing
total_cols <- ncol(MDdf) - length(all_exclude)
row_na_counts <- rowSums(is.na(MDdf[, !names(MDdf) %in% all_exclude]))
cat("Among the", total_cols, "imputation-eligible columns:\n")
cat("  Rows with 0 missing:", sum(row_na_counts == 0), "\n")
cat("  Rows with 1-3 missing:", sum(row_na_counts >= 1 & row_na_counts <= 3), "\n")
cat("  Rows with 4-6 missing:", sum(row_na_counts >= 4 & row_na_counts <= 6), "\n")
cat("  Rows with 7-10 missing:", sum(row_na_counts >= 7 & row_na_counts <= 10), "\n")
cat("  Rows with 10+ missing:", sum(row_na_counts > 10), "\n")
cat("\n  Decision: No rows excluded.\n")

# --- 4d. Build the imputation-eligible variable list ---
impute_vars <- names(MDdf)[!names(MDdf) %in% c(all_exclude, high_miss_exclude)]
cat("\n=== VARIABLES ELIGIBLE FOR IMPUTATION ===\n")
cat("Count:", length(impute_vars), "\n")
cat(impute_vars, sep = ", ")
cat("\n")

# Check which of these actually have missing values
impute_with_na <- impute_vars[colSums(is.na(MDdf[, impute_vars])) > 0]
cat("\nOf these,", length(impute_with_na), "have missing values to impute:\n")
for(v in impute_with_na) {
  ct <- sum(is.na(MDdf[[v]]))
  pct <- round(ct / nrow(MDdf) * 100, 1)
  cat(sprintf("  %-25s : %4d (%5.1f%%)\n", v, ct, pct))
}
```

**MD Step 4 Observations:**

- **Columns excluded from imputation (non-analytical only):** ID, Cust_ID, Marital_Original, Flag variables, Last_Contact and date components, high-cardinality columns (State_Name, Zip_Codes, Seating_Location), all M_ indicator variables.
- **No variables excluded for high missingness.** We are imputing ALL variables including the 7 CustomerDF block variables (70%), Net_Worth_True (61%), and HouseHold_Income_True (60.2%). This is not typical best practice -- imputing 70% of a variable means we are largely manufacturing data -- but we proceed to see the full effect on the model.
- **Row decisions:** No rows excluded.

---

### MD Step 5: Assess Missingness Patterns (MCAR vs MAR)

We now determine whether the missing data is MCAR (Missing Completely at Random) or MAR (Missing at Random). This matters because it determines which imputation methods are appropriate. We use decision trees to test whether missingness in each variable can be predicted by other observed variables. If it CAN be predicted, the data is MAR. If it CANNOT, the data may be MCAR.

Per the instructor: "I find a Decision Tree to be just fine and nice visually" and "I use judgment, not statistical significance." The consequences of misclassifying MCAR as MAR are low because advanced methods (MICE) assume MAR and handle MCAR just fine.

```{r md_step5_assess_setup, echo=TRUE}
cat("=== STEP 5: ASSESS MISSINGNESS PATTERNS ===\n\n")
cat("Method: Decision Trees and Logistic Regression\n")
cat("Goal: Determine if missingness is predictable from other variables (MAR)\n")
cat("      or unpredictable (MCAR)\n\n")

# Variables to test: those with missing values that we plan to impute
test_vars <- impute_with_na
cat("Variables to assess:", paste(test_vars, sep = ", "), "\n\n")

# Predictors for the test: complete or near-complete variables
# Use the indicator variables we created in Step 2
predictor_candidates <- impute_vars[colSums(is.na(MDdf[, impute_vars])) == 0]
cat("Predictor candidates (no missing):", length(predictor_candidates), "\n")
cat(predictor_candidates, sep = ", ")
cat("\n")
```

```{r md_step5_trees, echo=TRUE, fig.width=10, fig.height=7}
# Use decision trees to predict missingness indicator for each variable
# If the tree finds meaningful splits, missingness is predictable -> MAR
# If the tree is trivial (no splits or very low accuracy), -> likely MCAR

cat("=== DECISION TREE TESTS FOR MISSINGNESS ===\n\n")

# Build a data frame of complete predictors for testing
complete_predictors <- MDdf[, predictor_candidates]

# Remove any factor columns with too many levels for rpart
for(col in names(complete_predictors)) {
  if(is.factor(complete_predictors[[col]]) && nlevels(complete_predictors[[col]]) > 30) {
    complete_predictors[[col]] <- NULL
  }
}

# Also remove logical columns (convert to numeric)
for(col in names(complete_predictors)) {
  if(is.logical(complete_predictors[[col]])) {
    complete_predictors[[col]] <- as.integer(complete_predictors[[col]])
  }
}

results <- data.frame(
  Variable = character(),
  Tree_Splits = integer(),
  Accuracy = numeric(),
  Top_Predictor = character(),
  Assessment = character(),
  stringsAsFactors = FALSE
)

for(var in test_vars) {
  # Target: missing indicator
  target <- factor(MDdf[[paste0("M_", var)]], levels = c(0, 1), labels = c("Present", "Missing"))

  # Combine with predictors
  tree_data <- cbind(Target = target, complete_predictors)

  # Build decision tree
  tree_model <- rpart(Target ~ .,
                      data = tree_data,
                      method = "class",
                      control = rpart.control(maxdepth = 3, minsplit = 50, cp = 0.01))

  # Get results
  n_splits <- nrow(tree_model$frame[tree_model$frame$var != "<leaf>", ])
  preds <- predict(tree_model, tree_data, type = "class")
  acc <- round(mean(preds == target) * 100, 1)

  # Top predictor
  if(length(tree_model$variable.importance) > 0) {
    top_pred <- names(tree_model$variable.importance)[1]
  } else {
    top_pred <- "None"
  }

  # Assessment
  if(n_splits == 0) {
    assessment <- "Likely MCAR"
  } else if(acc > 85) {
    assessment <- "MAR"
  } else {
    assessment <- "Possibly MAR"
  }

  results <- rbind(results, data.frame(
    Variable = var, Tree_Splits = n_splits, Accuracy = acc,
    Top_Predictor = top_pred, Assessment = assessment, stringsAsFactors = FALSE
  ))
}

cat("=== MISSINGNESS ASSESSMENT RESULTS ===\n\n")
print(results, row.names = FALSE)

flextable(results) %>%
  set_header_labels(Variable = "Variable", Tree_Splits = "Splits",
                    Accuracy = "Accuracy (%)", Top_Predictor = "Top Predictor",
                    Assessment = "Assessment") %>%
  autofit()
```

```{r md_step5_tree_plots, echo=TRUE, fig.width=10, fig.height=7}
# Plot the most informative trees (those with splits)
vars_with_splits <- results$Variable[results$Tree_Splits > 0]

if(length(vars_with_splits) > 0) {
  for(var in vars_with_splits[1:min(4, length(vars_with_splits))]) {
    target <- factor(MDdf[[paste0("M_", var)]], levels = c(0, 1), labels = c("Present", "Missing"))
    tree_data <- cbind(Target = target, complete_predictors)

    tree_model <- rpart(Target ~ .,
                        data = tree_data,
                        method = "class",
                        control = rpart.control(maxdepth = 3, minsplit = 50, cp = 0.01))

    rpart.plot(tree_model,
               main = paste("Missingness in", var, "-", results$Assessment[results$Variable == var]),
               extra = 104,
               box.palette = "RdYlGn")
  }
}
```

```{r md_step5_logistic, echo=TRUE}
# Supplementary: Logistic regression tests for key variables
cat("=== LOGISTIC REGRESSION TESTS (Supplementary) ===\n\n")

for(var in test_vars) {
  target <- MDdf[[paste0("M_", var)]]

  # Only run if we have both 0s and 1s
  if(length(unique(target)) < 2) next

  # Use a few key predictors
  test_data <- data.frame(
    Missing = target,
    Y01 = MDdf$Y01,
    NumSeats = MDdf$NumSeats,
    Total_Spent = MDdf$Total_Spent,
    Concession_Total = MDdf$Concession_Total,
    First_Year_STH = MDdf$First_Year_STH
  )

  model <- glm(Missing ~ ., data = test_data, family = binomial)
  sig_vars <- names(which(summary(model)$coefficients[-1, 4] < 0.05))

  if(length(sig_vars) > 0) {
    cat(var, ": Significant predictors of missingness:", paste(sig_vars, collapse = ", "), "\n")
    cat("  -> Supports MAR assessment\n\n")
  } else {
    cat(var, ": No significant predictors found -> Supports MCAR assessment\n\n")
  }
}
```

```{r md_step5_summary, echo=TRUE}
cat("=== STEP 5 SUMMARY: MISSINGNESS PATTERN ASSESSMENT ===\n\n")

# Final classification
flextable(results) %>%
  set_header_labels(Variable = "Variable", Tree_Splits = "Tree Splits",
                    Accuracy = "Tree Accuracy (%)", Top_Predictor = "Top Predictor",
                    Assessment = "Classification") %>%
  color(~ Assessment == "MAR", ~ Assessment, color = "red") %>%
  color(~ Assessment == "Likely MCAR", ~ Assessment, color = "darkgreen") %>%
  color(~ Assessment == "Possibly MAR", ~ Assessment, color = "orange") %>%
  autofit()
```

**MD Step 5 Observations:**

The decision tree and logistic regression tests reveal the missingness patterns for each variable. The results table above shows the full assessment. Key findings:

- **MAR variables** (missingness IS predictable from other observed data): Variables where the decision tree found meaningful splits. For example, DistA missingness is predicted by Y01. Most_Purch_Concession missingness is predicted by Concession_Total. The 7 CustomerDF block variables likely share a common MAR/MNAR pattern tied to survey completion.

- **MCAR variables** (missingness is NOT predictable): Variables producing trees with zero splits -- their missingness cannot be predicted by any other variable. The data appears to be missing completely at random.

- **The 7 CustomerDF block variables** (70% missing each): These are likely MNAR -- customers who didn't complete the supplemental survey are systematically different from those who did. Normally we would exclude these, but we are imputing them anyway to observe the effect. MICE will do its best with available predictors, but the results for these variables should be interpreted with caution since we are largely manufacturing data.

- **Practical implication:** Since we are imputing everything, all variables proceed to Steps 6/7. MICE assumes MAR and handles MCAR just fine. The MNAR block variables are the riskiest to impute, but the professor wants to see what happens.

---

### Summary of Missing Data Steps 1-5

```{r md_steps_1_5_summary, echo=TRUE}
cat("=== MISSING DATA STEPS 1-5: COMPLETE ===\n\n")

cat("Step 1 - Identify Missing Data:\n")
cat("  Total missing: 67,997 values (12.2% of all cells)\n")
cat("  20 variables have missing data\n")
cat("  7 CustomerDF variables missing in identical block at 70%\n\n")

cat("Step 2 - Mark Missing Data:\n")
cat("  Created", length(indicator_cols), "binary indicator variables (M_ prefix)\n")
cat("  Confirmed block pattern in CustomerDF variables (r = 1.0)\n\n")

cat("Step 3 - Clean Up Obvious Mistakes:\n")
cat("  Marital 'U' (248 values) -> KEPT as valid level\n")
cat("  Survey_Comp > 1 (110 values) -> capped at 1.0\n")
cat("  Mult_Loc 'Y' (72 values) -> 'Yes'\n")
cat("  Most_Purch_Concession blanks (85 values) -> NA\n\n")

cat("Step 4 - Easy Decisions on Rows/Columns:\n")
cat("  Excluded from imputation: ID cols, date cols, backup cols, indicator cols\n")
cat("  High-missingness variables: KEPT for imputation (imputing everything)\n")
cat("  No rows excluded\n")
cat("  Variables eligible for imputation:", length(impute_with_na), "\n\n")

cat("Step 5 - Assess Missingness Patterns:\n")
cat("  Used decision trees and logistic regression\n")
cat("  Classification: Mix of MAR, MCAR, and likely MNAR (block) patterns\n")
cat("  All variables proceed to imputation regardless\n\n")

cat("=== READY FOR STEPS 6-7 (Simple Imputation & MICE) ===\n")
cat("Dataset: MDdf with", nrow(MDdf), "rows x", ncol(MDdf), "columns\n")
cat("Variables to impute:", length(impute_with_na), "\n")
cat(paste(impute_with_na, collapse = ", "), "\n")
```

| MD Step | Action | Key Finding |
|---------|--------|-------------|
| 1. Identify | Counted and visualized all missing data | 12.2% overall; 7 variables missing as block at 70% |
| 2. Mark | Created M_ indicator variables | Block pattern confirmed via indicator correlations |
| 3. Clean | Fixed Survey_Comp, Mult_Loc, blanks; kept Marital U | 267 values corrected |
| 4. Decide | Kept ALL variables; excluded only non-analytical cols | Imputing everything including 70% missing block |
| 5. Assess | Decision tree + logistic regression tests | Mix of MAR, MCAR, and likely MNAR |

---

### MD Step 6: Apply Simple (Univariate) Imputation Techniques

These are **univariate** (one variable at a time) methods. The goal is to handle variables with a small number of missing values using simple, defensible techniques -- getting them "out of the way" before Multiple Imputation (MICE) in Step 7. Variables with larger or more complex missingness patterns are left for MICE.

**Non-modeling methods:** Mean, median, mode
**Modeling methods:** Regression, CART, pmm (via mice with m=1)

The decision of which variables to handle here vs. MICE depends on:

1. How much data is missing (small = simpler method is fine)
2. Whether the variable was classified as MCAR or MAR in Step 5
3. SME judgment on whether a univariate approach is appropriate

```{r md_step6_plan, echo=TRUE}
cat("=== STEP 6: SIMPLE UNIVARIATE IMPUTATION ===\n\n")

cat("Variables eligible for imputation and their characteristics:\n\n")

# Recap the variables to impute
for(v in impute_with_na) {
  ct <- sum(is.na(MDdf[[v]]))
  pct <- round(ct / nrow(MDdf) * 100, 1)
  cls <- class(MDdf[[v]])[1]
  assess <- results$Assessment[results$Variable == v]
  cat(sprintf("  %-25s : %4d (%5.1f%%)  [%s]  %s\n", v, ct, pct, cls, assess))
}

cat("\n--- IMPUTATION PLAN ---\n\n")
cat("SIMPLE UNIVARIATE (Step 6 - handle now):\n")
cat("  Num_Children    (4.3%, integer)  -> Median imputation\n")
cat("  Rep_Visits      (5.4%, numeric)  -> Median imputation\n")
cat("  Most_Purch_Conc (0.9%, factor)   -> Mode imputation\n")
cat("  Sex             (7.1%, factor)   -> Mode imputation\n\n")

cat("MANAGEMENT DICTATE (Step 6 - custom code, NOT MICE):\n")
cat("  Rep_Calls       (4.6%, numeric)  -> Stochastic imputation, independent\n")
cat("  Rep_Name        (9.2%, factor)   -> Stochastic imputation, independent\n")
cat("  Age             (10.4%, integer) -> Simple imputation using other variables\n\n")

cat("MICE (Step 7 - multivariate imputation for remaining):\n")
cat("  DistA                (30.6%, integer) -> pmm via MICE\n")
cat("  Marital              (12.2%, factor)  -> polyreg via MICE\n")
cat("  Tenure               (6.3%, integer)  -> pmm via MICE\n")
cat("  Educational_Level    (70.0%, factor)  -> polyreg via MICE\n")
cat("  Favorite_Caps_Player (70.0%, factor)  -> polyreg via MICE\n")
cat("  Favorite_Sport       (70.0%, factor)  -> polyreg via MICE\n")
cat("  Job_Sector           (70.0%, factor)  -> polyreg via MICE\n")
cat("  Mode_Of_Transport    (70.0%, factor)  -> polyreg via MICE\n")
cat("  Team_B_STH           (70.0%, factor)  -> logreg via MICE\n")
cat("  Team_C_STH           (70.0%, factor)  -> logreg via MICE\n")
cat("  Net_Worth_True       (61.0%, numeric) -> pmm via MICE\n")
cat("  HouseHold_Income_True(60.2%, numeric) -> pmm via MICE\n")
cat("\n  NOTE: Imputing the 70% missing block is not best practice.\n")
cat("  We are essentially manufacturing data for these variables.\n")
cat("  Results should be interpreted with extreme caution.\n")
```

```{r md_step6_helpers, echo=TRUE}
# Helper function: get the mode of a vector (most frequent value)
get_mode <- function(x) {
  x <- x[!is.na(x)]
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# Store pre-imputation counts for verification
pre_impute_na <- sapply(impute_with_na, function(v) sum(is.na(MDdf[[v]])))
```

```{r md_step6_numeric, echo=TRUE}
cat("=== 6a. NUMERIC VARIABLES: MEDIAN IMPUTATION ===\n\n")

# --- Num_Children (4.3% missing, integer) ---
cat("Num_Children:\n")
cat("  Before - NA count:", sum(is.na(MDdf$Num_Children)), "\n")
cat("  Median:", median(MDdf$Num_Children, na.rm = TRUE), "\n")
cat("  Mean:", round(mean(MDdf$Num_Children, na.rm = TRUE), 2), "\n")
median_nc <- median(MDdf$Num_Children, na.rm = TRUE)
MDdf$Num_Children[is.na(MDdf$Num_Children)] <- median_nc
cat("  After - NA count:", sum(is.na(MDdf$Num_Children)), "\n")
cat("  Imputed with median =", median_nc, "\n")
cat("  Rationale: Small % missing, integer variable, median preserves distribution better\n")
cat("  than mean for count data with a floor at 0.\n\n")

# --- Rep_Calls (4.6% missing, numeric) ---
# MANAGEMENT DICTATE: Stochastic imputation, independent of all other variables, custom code
cat("Rep_Calls (MANAGEMENT DICTATE - Stochastic, Independent):\n")
cat("  Before - NA count:", sum(is.na(MDdf$Rep_Calls)), "\n")
observed_rc <- MDdf$Rep_Calls[!is.na(MDdf$Rep_Calls)]
cat("  Observed distribution - Mean:", round(mean(observed_rc), 2),
    " SD:", round(sd(observed_rc), 2),
    " Min:", min(observed_rc), " Max:", max(observed_rc), "\n")
n_missing_rc <- sum(is.na(MDdf$Rep_Calls))
set.seed(42)
# Stochastic imputation: sample with replacement from observed values
# This preserves the empirical distribution (including skewness) without
# using any other variables -- purely independent of all other columns
stochastic_rc <- sample(observed_rc, size = n_missing_rc, replace = TRUE)
MDdf$Rep_Calls[is.na(MDdf$Rep_Calls)] <- stochastic_rc
cat("  After - NA count:", sum(is.na(MDdf$Rep_Calls)), "\n")
cat("  Method: Stochastic sampling from observed distribution (sample with replacement)\n")
cat("  Imputed", n_missing_rc, "values drawn randomly from", length(observed_rc), "observed values\n")
cat("  Rationale: Management dictates stochastic imputation independent of all other\n")
cat("  variables. Sampling from the observed distribution preserves the marginal\n")
cat("  distribution shape (including skewness) and adds natural variability, unlike\n")
cat("  deterministic median imputation which creates a spike at one value.\n\n")

# --- Rep_Visits (5.4% missing, numeric) ---
cat("Rep_Visits:\n")
cat("  Before - NA count:", sum(is.na(MDdf$Rep_Visits)), "\n")
cat("  Median:", round(median(MDdf$Rep_Visits, na.rm = TRUE), 2), "\n")
cat("  Mean:", round(mean(MDdf$Rep_Visits, na.rm = TRUE), 2), "\n")
median_rv <- median(MDdf$Rep_Visits, na.rm = TRUE)
MDdf$Rep_Visits[is.na(MDdf$Rep_Visits)] <- median_rv
cat("  After - NA count:", sum(is.na(MDdf$Rep_Visits)), "\n")
cat("  Imputed with median =", round(median_rv, 2), "\n")
cat("  Rationale: Like Rep_Calls, this is a count-like variable. Median imputation\n")
cat("  is conservative and preserves the central tendency without inflating variance.\n\n")
```

```{r md_step6_categorical, echo=TRUE}
cat("=== 6b. CATEGORICAL VARIABLES: MODE IMPUTATION ===\n\n")

# --- Most_Purch_Concession (0.9% missing, factor) ---
cat("Most_Purch_Concession:\n")
cat("  Before - NA count:", sum(is.na(MDdf$Most_Purch_Concession)), "\n")
cat("  Distribution:\n")
print(table(MDdf$Most_Purch_Concession, useNA = "ifany"))
mode_mpc <- get_mode(MDdf$Most_Purch_Concession)
cat("  Mode:", as.character(mode_mpc), "\n")
MDdf$Most_Purch_Concession[is.na(MDdf$Most_Purch_Concession)] <- mode_mpc
cat("  After - NA count:", sum(is.na(MDdf$Most_Purch_Concession)), "\n")
cat("  Imputed with mode =", as.character(mode_mpc), "\n")
cat("  Rationale: Very small % missing (<1%). Mode is the standard simple method\n")
cat("  for categorical variables. With so few values to fill, impact is minimal.\n\n")

# --- Sex (7.1% missing, factor) ---
cat("Sex:\n")
cat("  Before - NA count:", sum(is.na(MDdf$Sex)), "\n")
cat("  Distribution:\n")
print(table(MDdf$Sex, useNA = "ifany"))
mode_sex <- get_mode(MDdf$Sex)
cat("  Mode:", as.character(mode_sex), "\n")
MDdf$Sex[is.na(MDdf$Sex)] <- mode_sex
cat("  After - NA count:", sum(is.na(MDdf$Sex)), "\n")
cat("  Imputed with mode =", as.character(mode_sex), "\n")
cat("  Rationale: Binary variable with clear dominant category. Mode imputation is\n")
cat("  conservative and appropriate when one category heavily outweighs the other.\n\n")

# --- Rep_Name (9.2% missing, factor) ---
# MANAGEMENT DICTATE: Stochastic imputation, independent of all other variables, custom code
cat("Rep_Name (MANAGEMENT DICTATE - Stochastic, Independent):\n")
cat("  Before - NA count:", sum(is.na(MDdf$Rep_Name)), "\n")
cat("  Distribution:\n")
observed_rn <- MDdf$Rep_Name[!is.na(MDdf$Rep_Name)]
print(table(observed_rn))
n_missing_rn <- sum(is.na(MDdf$Rep_Name))
# Stochastic imputation: sample from observed level proportions
# This preserves the empirical distribution across categories
set.seed(123)
level_props <- prop.table(table(observed_rn))
cat("\n  Level proportions:\n")
print(round(level_props, 4))
stochastic_rn <- sample(names(level_props), size = n_missing_rn,
                        replace = TRUE, prob = as.numeric(level_props))
MDdf$Rep_Name[is.na(MDdf$Rep_Name)] <- stochastic_rn
cat("  After - NA count:", sum(is.na(MDdf$Rep_Name)), "\n")
cat("  Method: Stochastic sampling from observed level proportions\n")
cat("  Imputed", n_missing_rn, "values drawn randomly proportional to observed frequencies\n")
cat("  Rationale: Management dictates stochastic imputation independent of all other\n")
cat("  variables. Sampling proportional to observed level frequencies preserves the\n")
cat("  category distribution while adding natural variability. Unlike mode imputation\n")
cat("  (which assigns everyone the same rep), this spreads missing values across all\n")
cat("  reps in proportion to their observed frequencies.\n\n")
```

```{r md_step6_age, echo=TRUE, fig.width=10, fig.height=6}
cat("=== 6c. AGE: SIMPLE IMPUTATION USING OTHER VARIABLES (MANAGEMENT DICTATE) ===\n\n")

# MANAGEMENT DICTATE: Age must be imputed using simple imputation with other
# variables, using our own code (not MICE functions).
# Approach: Build a linear regression model on observed Age using other complete
# predictors, then predict missing values.

cat("Age (MANAGEMENT DICTATE - Simple Imputation Using Other Variables):\n")
cat("  Before - NA count:", sum(is.na(MDdf$Age)), "\n")
n_missing_age <- sum(is.na(MDdf$Age))

# Select predictors that are complete (no NAs) and analytically relevant
# We use variables already imputed in Steps 6a/6b plus originally complete ones
age_predictors <- c("Y01", "Total_Spent", "NumSeats", "Tenure",
                    "First_Year_STH", "Num_Children", "Rep_Calls",
                    "Rep_Visits", "Concession_Total", "Survey_Comp")

# Check which predictors are actually available and complete
available_preds <- age_predictors[age_predictors %in% names(MDdf)]
available_preds <- available_preds[sapply(available_preds, function(v) sum(is.na(MDdf[[v]])) == 0)]
cat("  Predictors used (complete variables):", paste(available_preds, collapse = ", "), "\n\n")

# Build training data: rows where Age is observed
train_idx <- !is.na(MDdf$Age)
pred_idx <- is.na(MDdf$Age)

train_data <- MDdf[train_idx, c("Age", available_preds)]
pred_data <- MDdf[pred_idx, available_preds]

# Fit a linear regression model
age_model <- lm(Age ~ ., data = train_data)
cat("  Regression model summary:\n")
print(summary(age_model))

# Predict missing Age values
predicted_age <- predict(age_model, newdata = pred_data)

# Round to integers (Age is whole years) and enforce reasonable bounds
predicted_age <- round(predicted_age)
predicted_age <- pmax(predicted_age, min(MDdf$Age, na.rm = TRUE))  # floor at observed min
predicted_age <- pmin(predicted_age, max(MDdf$Age, na.rm = TRUE))  # cap at observed max

cat("\n  Predicted Age distribution:\n")
cat("    Mean:", round(mean(predicted_age), 1), "\n")
cat("    SD:", round(sd(predicted_age), 1), "\n")
cat("    Range:", min(predicted_age), "-", max(predicted_age), "\n")
cat("  Observed Age distribution:\n")
cat("    Mean:", round(mean(MDdf$Age, na.rm = TRUE), 1), "\n")
cat("    SD:", round(sd(MDdf$Age, na.rm = TRUE), 1), "\n")
cat("    Range:", min(MDdf$Age, na.rm = TRUE), "-", max(MDdf$Age, na.rm = TRUE), "\n\n")

# Impute
MDdf$Age[is.na(MDdf$Age)] <- predicted_age
cat("  After - NA count:", sum(is.na(MDdf$Age)), "\n")
cat("  Method: Linear regression on", length(available_preds), "predictor variables\n")
cat("  Imputed", n_missing_age, "values using lm() predictions (rounded to integers)\n")
cat("  Rationale: Management dictates simple imputation using other variables with\n")
cat("  custom code (not MICE). Linear regression leverages relationships between Age\n")
cat("  and other observed variables to produce informed predictions. Values are rounded\n")
cat("  to whole years and clamped to the observed Age range.\n\n")

# Diagnostic plot: predicted vs actual (on training data)
train_preds <- predict(age_model, newdata = train_data)
par(mfrow = c(1, 2))
plot(train_data$Age, train_preds,
     main = "Age Model: Actual vs Predicted (Training)",
     xlab = "Actual Age", ylab = "Predicted Age",
     col = rgb(0.2, 0.4, 0.8, 0.3), pch = 16)
abline(0, 1, col = "red", lwd = 2)

hist(predicted_age, main = "Distribution of Imputed Age Values",
     col = "steelblue", xlab = "Imputed Age", breaks = 20)
abline(v = mean(MDdf$Age, na.rm = TRUE), col = "red", lwd = 2, lty = 2)
legend("topright", "Overall Mean Age", col = "red", lty = 2, lwd = 2)
par(mfrow = c(1, 1))
```

```{r md_step6_verify, echo=TRUE, fig.width=10, fig.height=6}
cat("=== STEP 6 VERIFICATION ===\n\n")

# Post-imputation counts
post_impute_na <- sapply(impute_with_na, function(v) sum(is.na(MDdf[[v]])))

verify_df <- data.frame(
  Variable = impute_with_na,
  Before = pre_impute_na,
  After = post_impute_na,
  Imputed = pre_impute_na - post_impute_na,
  Method = ifelse(pre_impute_na - post_impute_na > 0,
                  ifelse(impute_with_na %in% c("Num_Children", "Rep_Visits"), "Median",
                  ifelse(impute_with_na == "Rep_Calls", "Stochastic (Mgmt)",
                  ifelse(impute_with_na == "Rep_Name", "Stochastic (Mgmt)",
                  ifelse(impute_with_na == "Age", "Regression (Mgmt)",
                  ifelse(impute_with_na %in% c("Most_Purch_Concession", "Sex"), "Mode", "---"))))),
                  "Left for MICE")
)

print(verify_df, row.names = FALSE)

cat("\nTotal missing before Step 6:", sum(pre_impute_na), "\n")
cat("Total missing after Step 6:", sum(post_impute_na), "\n")
cat("Values imputed in Step 6:", sum(pre_impute_na) - sum(post_impute_na), "\n")

cat("\n--- REMAINING FOR MICE (Step 7) ---\n")
remaining <- verify_df[verify_df$After > 0, ]
print(remaining[, c("Variable", "After")], row.names = FALSE)

# Flextable summary
flextable(verify_df) %>%
  set_header_labels(Variable = "Variable", Before = "NA Before",
                    After = "NA After", Imputed = "Values Filled",
                    Method = "Method Used") %>%
  color(~ After == 0, ~ After, color = "darkgreen") %>%
  color(~ After > 0, ~ After, color = "red") %>%
  autofit()
```

```{r md_step6_distribution_check, echo=TRUE, fig.width=10, fig.height=10}
# Visual check: compare distributions before and after imputation
# We saved FFdf as the pre-imputation state, MDdf is post-imputation

par(mfrow = c(3, 2))

# Num_Children
hist(FFdf$Num_Children[!is.na(FFdf$Num_Children)], main = "Num_Children (Before)", col = "lightblue",
     xlab = "Number of Children", breaks = 12)
hist(MDdf$Num_Children, main = "Num_Children (After Median)", col = "steelblue",
     xlab = "Number of Children", breaks = 12)

# Rep_Calls - now stochastic
hist(FFdf$Rep_Calls[!is.na(FFdf$Rep_Calls)], main = "Rep_Calls (Before)", col = "lightblue",
     xlab = "Rep Calls", breaks = 30)
hist(MDdf$Rep_Calls, main = "Rep_Calls (After Stochastic)", col = "coral",
     xlab = "Rep Calls", breaks = 30)

# Rep_Visits
hist(FFdf$Rep_Visits[!is.na(FFdf$Rep_Visits)], main = "Rep_Visits (Before)", col = "lightblue",
     xlab = "Rep Visits", breaks = 20)
hist(MDdf$Rep_Visits, main = "Rep_Visits (After Median)", col = "steelblue",
     xlab = "Rep Visits", breaks = 20)

par(mfrow = c(1, 1))
```

**MD Step 6 Observations:**

**Numeric variables imputed with median:**

- **Num_Children** (406 values, 4.3%): Imputed with median. Small % missing, integer count variable. Median preserves distribution and avoids non-integer imputed values.
- **Rep_Visits** (508 values, 5.4%): Imputed with median. Right-skewed count data benefits from median imputation.

**Management Dictate -- Stochastic imputation (independent of all other variables):**

- **Rep_Calls** (433 values, 4.6%): Stochastic imputation via `sample()` from observed values with replacement. This preserves the full empirical distribution (including right skew) while adding natural variability. Unlike median imputation, stochastic sampling does not create an artificial spike at one value. Management required this be independent of all other variables and use custom code (not MICE).
- **Rep_Name** (871 values, 9.2%): Stochastic imputation via `sample()` from observed level proportions. Each missing value is randomly assigned a rep name proportional to the observed frequencies. This distributes missing values across all reps rather than assigning everyone the mode. Management required independence from other variables and custom code.

**Management Dictate -- Simple imputation using other variables:**

- **Age** (983 values, 10.4%): Linear regression (`lm()`) using complete predictor variables (Total_Spent, NumSeats, Tenure_Years, etc.) to predict missing Age values. Predictions are rounded to whole years and clamped to the observed Age range. Management required this use other variables but with custom code, not MICE functions.

**Categorical variables imputed with mode:**

- **Most_Purch_Concession** (~85 values, 0.9%): Imputed with mode. Very small missingness -- impact on distribution is negligible.
- **Sex** (667 values, 7.1%): Imputed with mode. Binary variable with a dominant category.

**Remaining for MICE (Step 7) -- all variables with remaining NAs:**

These include both moderate-missingness variables and the high-missingness variables we are imputing per our decision to impute everything. Age has been moved out of MICE per management dictate. The high-miss variables (70%, 61%, 60%) will be handled by MICE but results should be interpreted with caution since we are largely manufacturing data for those.

---

### Summary of Missing Data Steps 1-6

```{r md_steps_1_6_summary, echo=TRUE}
cat("=== MISSING DATA STEPS 1-6: COMPLETE ===\n\n")

cat("Step 1 - Identify Missing Data:\n")
cat("  Total missing: 67,997 values (12.2% of all cells)\n")
cat("  20 variables have missing data\n")
cat("  7 CustomerDF variables missing in identical block at 70%\n\n")

cat("Step 2 - Mark Missing Data:\n")
cat("  Created", length(indicator_cols), "binary indicator variables (M_ prefix)\n")
cat("  Confirmed block pattern in CustomerDF variables (r = 1.0)\n\n")

cat("Step 3 - Clean Up Obvious Mistakes:\n")
cat("  Marital 'U' (248 values) -> KEPT as valid level\n")
cat("  Survey_Comp > 1 (110 values) -> capped at 1.0\n")
cat("  Mult_Loc 'Y' (72 values) -> 'Yes'\n")
cat("  Most_Purch_Concession blanks (85 values) -> NA\n\n")

cat("Step 4 - Easy Decisions on Rows/Columns:\n")
cat("  Excluded from imputation: ID cols, date cols, backup cols, indicator cols\n")
cat("  High-missingness variables: KEPT (imputing everything)\n")
cat("  No rows excluded\n")
cat("  Variables eligible for imputation:", length(impute_with_na), "\n\n")

cat("Step 5 - Assess Missingness Patterns:\n")
cat("  Used decision trees and logistic regression\n")
cat("  Classification: Mix of MAR, MCAR, and likely MNAR (block)\n")
cat("  All variables proceed to imputation regardless\n\n")

cat("Step 6 - Simple Imputation + Management Dictate:\n")
cat("  Median imputation: Num_Children, Rep_Visits\n")
cat("  Mode imputation: Most_Purch_Concession, Sex\n")
cat("  MGMT DICTATE - Stochastic (independent): Rep_Calls, Rep_Name\n")
cat("  MGMT DICTATE - Regression (using other vars): Age\n")
cat("  Total values imputed:", sum(pre_impute_na) - sum(post_impute_na), "\n\n")

cat("=== REMAINING FOR STEP 7 (MICE) ===\n")
remaining_vars <- impute_with_na[sapply(impute_with_na, function(v) sum(is.na(MDdf[[v]]))) > 0]
for(v in remaining_vars) {
  ct <- sum(is.na(MDdf[[v]]))
  pct <- round(ct / nrow(MDdf) * 100, 1)
  cat(sprintf("  %-25s : %4d (%5.1f%%)\n", v, ct, pct))
}
cat("\nDataset: MDdf with", nrow(MDdf), "rows x", ncol(MDdf), "columns\n")
cat("Total remaining missing (imputation-eligible):", sum(sapply(remaining_vars, function(v) sum(is.na(MDdf[[v]])))), "\n")
```

| MD Step | Action | Key Finding |
|---------|--------|-------------|
| 1. Identify | Counted and visualized all missing data | 12.2% overall; 7 variables missing as block at 70% |
| 2. Mark | Created M_ indicator variables | Block pattern confirmed via indicator correlations |
| 3. Clean | Fixed Survey_Comp, Mult_Loc, blanks; kept Marital U | 267 values corrected |
| 4. Decide | Kept ALL variables; excluded only non-analytical cols | Imputing everything including 70% missing block |
| 5. Assess | Decision tree + logistic regression tests | Mix of MAR, MCAR, and likely MNAR |
| 6. Simple + Mgmt | Median (2), Mode (2), Stochastic (2), Regression (1) | 7 variables resolved; remaining go to MICE |

