---
title: "Data Wrangling and Cleaning: Steps 1-5"
author: "David Mullaly"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

## Introduction

This document covers the data wrangling process and the first five steps of data cleaning for the course. We load multiple relational tables, merge them into a single flat file, and perform initial cleaning steps.

### Data Cleaning Steps Covered:
1. Open data in your software of choice
2. Review variables for common sense based on SME knowledge
3. Review how the software coded the variables (nominal, continuous)
4. Perform data integrity/validation checks
5. Handle dates

```{r setup, include=FALSE}
# Setup Chunk - Load Required Libraries
library(mice)
library(VIM)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(corrplot)
library(lubridate)
library(flextable)

setwd("C:/Users/david/Desktop/Spring-Semester-/Missing Data")
```

## Data Loading and Wrangling

Load four relational tables and merge into a single flat file (FFdf) using left joins on Cust_ID.

```{r load_data, echo=TRUE}
# Load the raw data files
MainDF <- read.csv("Raw.csv")
StoreDF <- read.csv("StoreTable.csv")
ConcessDF <- read.csv("ConcessTable.csv")
CustomerDF <- read.csv("CustomerTable.csv")

# Check dimensions of each file
cat("MainDF:", dim(MainDF), "| StoreDF:", dim(StoreDF),
    "| ConcessDF:", dim(ConcessDF), "| CustomerDF:", dim(CustomerDF), "\n")
```

```{r check_duplicates, echo=TRUE}
# Check for duplicate Cust_IDs in each source table BEFORE merging
cat("Duplicates in MainDF:", sum(duplicated(MainDF$Cust_ID)), "\n")
cat("Duplicates in StoreDF:", sum(duplicated(StoreDF$Cust_ID)), "\n")
cat("Duplicates in ConcessDF:", sum(duplicated(ConcessDF$Cust_ID)), "\n")
cat("Duplicates in CustomerDF:", sum(duplicated(CustomerDF$Cust_ID)), "\n")
```

```{r merge_data, echo=TRUE}
# Create flat file by merging all tables on Cust_ID
FFdf <- MainDF
FFdf <- merge(FFdf, StoreDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, ConcessDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, CustomerDF, by = "Cust_ID", all.x = TRUE)

cat("Final flat file dimensions - Rows:", nrow(FFdf), "Columns:", ncol(FFdf), "\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "| Duplicate rows:", nrow(FFdf) - length(unique(FFdf$Cust_ID)), "\n")
```

## Step 1: Open Data in Your Software of Choice

Create identifier variable, arrange columns, and explore distributions visually.

```{r step1_open_data, echo=TRUE, fig.width=8, fig.height=6}
# Create an identifier variable
FFdf$ID <- 1:nrow(FFdf)

# Reorder columns: Y-variable first, then alphabetically
y_var <- "Y01"
other_vars <- sort(setdiff(names(FFdf), y_var))
FFdf <- FFdf[, c(y_var, other_vars)]

# Histograms for key numeric variables
par(mfrow = c(2, 2))
hist(FFdf$Age, main = "Distribution of Age", xlab = "Age", col = "steelblue", breaks = 20)
hist(FFdf$tenure, main = "Distribution of Tenure", xlab = "Tenure (Years)", col = "steelblue", breaks = 20)
hist(FFdf$NumSeats, main = "Distribution of NumSeats", xlab = "Number of Seats", col = "steelblue")
hist(FFdf$Total.Spent, main = "Distribution of Total Spent", xlab = "Total Spent ($)", col = "steelblue", breaks = 30)
par(mfrow = c(1, 1))

# Response variable distribution
cat("Response Variable (Y01):", table(FFdf$Y01), "\n")
barplot(table(FFdf$Y01), main = "Distribution of Y01 (Response Variable)",
        col = c("coral", "steelblue"), names.arg = c("0 (No)", "1 (Yes)"))
```

**Observations:** Age is roughly normal (30-60), Tenure is right-skewed, NumSeats clusters around 2-4, Total Spent is heavily right-skewed. Response variable Y01 is reasonably balanced.

## Step 2: Review Variables for Common Sense (SME Knowledge)

Standardize variable names and check for unique identifiers.

```{r step2_common_sense, echo=TRUE}
# Clean up variable names - replace dots/spaces with underscores, capitalize
names(FFdf) <- gsub("\\.", "_", names(FFdf))
names(FFdf) <- gsub(" ", "_", names(FFdf))
names(FFdf) <- gsub("(^|_)([a-z])", "\\1\\U\\2", names(FFdf), perl = TRUE)

cat("Dataset:", nrow(FFdf), "rows,", ncol(FFdf), "columns\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "out of", nrow(FFdf), "rows\n")
```

**Result:** Each row does not necessarily represent a unique customer (175 duplicate Cust_IDs found in MainDF source data).

## Step 3: Review How Software Coded Variables

Convert character variables to factors for proper categorical analysis.

```{r step3_variable_types, echo=TRUE}
# Identify character variables
chr_vars <- names(FFdf)[sapply(FFdf, is.character)]
cat("Character variables to convert:", length(chr_vars), "\n")

# Convert character variables to factors
for(var in chr_vars) {
  FFdf[[var]] <- as.factor(FFdf[[var]])
}

# Show key factor levels
cat("\nSex levels:", levels(FFdf$Sex), "\n")
cat("Marital levels:", levels(FFdf$Marital), "\n")
cat("Account_Type levels:", levels(FFdf$Account_Type), "\n")
```

**Note:** Marital has levels D (Divorced), M (Married), S (Single), U (Unknown). The "U" for Unknown may need special handling.

## Step 4: Data Integrity/Validation Checks

Check for anomalies, bogus values, and data quality issues.

```{r step4_checks, echo=TRUE}
# Check numeric variable ranges
cat("Age range:", range(FFdf$Age, na.rm = TRUE), "\n")
cat("Tenure range:", range(FFdf$Tenure, na.rm = TRUE), "\n")
cat("NumSeats range:", range(FFdf$NumSeats, na.rm = TRUE), "\n")

# Check for 999 placeholder values in DistA
cat("\nDistA values = 999:", sum(FFdf$DistA == 999, na.rm = TRUE), "\n")

# Convert 999 to NA (placeholder for missing)
FFdf$DistA[FFdf$DistA == 999] <- NA
cat("DistA NA count after conversion:", sum(is.na(FFdf$DistA)), "\n")

# Check Survey_Comp for outliers (expected range 0-1)
cat("\nSurvey_Comp range:", range(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("Survey_Comp values > 1:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")

# Check for redundant columns
cat("\nState_Name unique:", length(unique(FFdf$State_Name)),
    "| State_Loc unique:", length(unique(FFdf$State_Loc)), "\n")
```

**Issues Found:**

- **DistA = 999:** Placeholder values converted to NA
- **Survey_Comp > 1:** Outlier found when expected range is 0-1
- **Age max = 99:** May be placeholder or extreme value - verify with SME
- **Tenure max = 400:** Suspicious value if measured in years - verify units with SME
- **State_Name/State_Loc:** Redundant columns (same info, different format)
- **Marital = "U":** Unknown status - consider treating as NA
- **Cust_ID duplicates:** 175 duplicate IDs found in MainDF source data
- **Address, Name, PhoneNum:** 100% missing - likely removed from CustomerDF for privacy

## Step 5: Handle Dates

Convert Last_Contact datetime and extract useful components.

```{r step5_handle_dates, echo=TRUE}
# Convert Last_Contact to datetime format
FFdf$Last_Contact <- ymd_hms(as.character(FFdf$Last_Contact))

# Extract date components
FFdf$Contact_Year <- year(FFdf$Last_Contact)
FFdf$Contact_Month <- month(FFdf$Last_Contact)
FFdf$Contact_Day <- day(FFdf$Last_Contact)
FFdf$Contact_Weekday <- wday(FFdf$Last_Contact, label = TRUE)
FFdf$Contact_Hour <- hour(FFdf$Last_Contact)

cat("Date components extracted: Contact_Year, Contact_Month, Contact_Day, Contact_Weekday, Contact_Hour\n")
cat("Contact Year range:", range(FFdf$Contact_Year, na.rm = TRUE), "\n")
cat("Contact Hour range:", range(FFdf$Contact_Hour, na.rm = TRUE), "\n")
```

## Summary

```{r summary, echo=TRUE}
cat("Final Dataset:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
cat("Total Missing Values:", sum(is.na(FFdf)), "\n")

# Show columns with missing values
na_counts <- colSums(is.na(FFdf))
na_cols <- na_counts[na_counts > 0]
cat("\nColumns with missing values:\n")
print(sort(na_cols, decreasing = TRUE))
```

### Data Quality Issues for Future Steps:

| Issue | Possible action / Action Taken |
|-------|--------------|
| DistA = 999 | Converted to NA |
| Survey_Comp > 1 | Flag for investigation (110 values, max 7.4) |
| Age = 99 | Verify with SME |
| Tenure = 400 | Verify units with SME (400 years unlikely) |
| Marital = "U" | Consider as NA or keep |
| State redundancy | Drop one column |
| ID columns | Exclude from modeling |
| Cust_ID duplicates | 175 duplicates in MainDF - investigate or deduplicate |
| PII columns | Address, Name, PhoneNum 100% missing - exclude |

## Step 6: Handle Categorical Variables

Examine categorical variables for rare levels, combine similar levels, and decide on level ordering.

```{r step6_categorical, echo=TRUE}
# Get all factor variables
factor_vars <- names(FFdf)[sapply(FFdf, is.factor)]
cat("Factor variables:", length(factor_vars), "\n")

# Function to check for rare levels (less than 5% of data)
check_rare_levels <- function(df, threshold = 0.05) {
  factor_vars <- names(df)[sapply(df, is.factor)]
  rare_info <- list()

  for(var in factor_vars) {
    tbl <- table(df[[var]], useNA = "ifany")
    pct <- prop.table(tbl)
    rare_levels <- names(pct[pct < threshold])
    if(length(rare_levels) > 0) {
      rare_info[[var]] <- data.frame(
        Level = rare_levels,
        Count = as.numeric(tbl[rare_levels]),
        Percent = round(as.numeric(pct[rare_levels]) * 100, 2)
      )
    }
  }
  return(rare_info)
}

# Check for rare levels
rare_levels <- check_rare_levels(FFdf, threshold = 0.05)
cat("\nVariables with rare levels (< 5%):\n")
print(names(rare_levels))

# Display level distributions for key categorical variables
cat("\n--- Sex Distribution ---\n")
print(table(FFdf$Sex, useNA = "ifany"))

cat("\n--- Marital Distribution ---\n")
print(table(FFdf$Marital, useNA = "ifany"))

cat("\n--- Account_Type Distribution ---\n")
print(table(FFdf$Account_Type, useNA = "ifany"))

cat("\n--- Educational_Level Distribution ---\n")
print(table(FFdf$Educational_Level, useNA = "ifany"))
```

```{r step6_handle_categorical, echo=TRUE}
# Handle Marital "U" (Unknown) - treat as NA for analysis purposes
# Create backup first
FFdf$Marital_Original <- FFdf$Marital

# Option: Convert "U" to NA (uncomment if desired)
# FFdf$Marital[FFdf$Marital == "U"] <- NA
# FFdf$Marital <- droplevels(FFdf$Marital)

cat("Marital 'U' (Unknown) count:", sum(FFdf$Marital == "U", na.rm = TRUE), "\n")
cat("Decision: Keep 'U' as separate level for now - may represent meaningful unknown status\n")

# Check for levels that might mean the same thing
cat("\n--- State_Name levels ---\n")
print(length(levels(FFdf$State_Name)))
cat("Number of unique states:", length(unique(FFdf$State_Name)), "\n")
```

**Step 6 Observations:**

- Marital has "U" (Unknown) level - kept as separate category
- Educational_Level could be made ordinal if needed
- State variables have many levels - consider regional grouping for modeling

## Step 7: Remove Zero-Variance Predictors

Identify and remove columns that contain the same value throughout (zero variance).

```{r step7_zero_variance, echo=TRUE}
# Function to identify zero-variance columns
find_zero_variance <- function(df) {
  zv_cols <- c()
  for(col in names(df)) {
    unique_vals <- length(unique(na.omit(df[[col]])))
    if(unique_vals <= 1) {
      zv_cols <- c(zv_cols, col)
    }
  }
  return(zv_cols)
}

# Find zero-variance columns
zero_var_cols <- find_zero_variance(FFdf)
cat("Zero-variance columns found:", length(zero_var_cols), "\n")

if(length(zero_var_cols) > 0) {
  cat("Columns with zero variance:\n")
  print(zero_var_cols)

  # Store in excluded columns list
  excluded_cols <- zero_var_cols

  # Remove zero-variance columns
  FFdf <- FFdf[, !names(FFdf) %in% zero_var_cols]
  cat("\nRemoved", length(zero_var_cols), "zero-variance columns\n")
} else {
  cat("No zero-variance columns found\n")
  excluded_cols <- c()
}

cat("Remaining columns:", ncol(FFdf), "\n")
```

## Step 8: Handle Near Zero-Variance Predictors

Identify columns with very low information density (dominated by single value).

```{r step8_near_zero_variance, echo=TRUE}
# Function to find near-zero variance columns
# A column is NZV if one value dominates (e.g., >95% of values)
find_near_zero_variance <- function(df, threshold = 0.95) {
  nzv_info <- data.frame(
    Variable = character(),
    DominantValue = character(),
    DominantPct = numeric(),
    UniqueValues = integer(),
    stringsAsFactors = FALSE
  )

  for(col in names(df)) {
    if(is.numeric(df[[col]]) || is.factor(df[[col]])) {
      tbl <- table(df[[col]], useNA = "no")
      if(length(tbl) > 0) {
        max_pct <- max(tbl) / sum(tbl)
        if(max_pct >= threshold) {
          dominant_val <- names(tbl)[which.max(tbl)]
          nzv_info <- rbind(nzv_info, data.frame(
            Variable = col,
            DominantValue = as.character(dominant_val),
            DominantPct = round(max_pct * 100, 2),
            UniqueValues = length(tbl),
            stringsAsFactors = FALSE
          ))
        }
      }
    }
  }
  return(nzv_info)
}

# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)

# Also check at 90% threshold
nzv_cols_90 <- find_near_zero_variance(FFdf, threshold = 0.90)
cat("\nNear-zero variance columns (>90% one value):\n")
print(nzv_cols_90)
```

```{r step8_handle_nzv, echo=TRUE}
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable

if(length(nzv_variables) > 0) {
  cat("Near-zero variance variables to monitor:\n")
  print(nzv_variables)
  cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
  cat("No significant near-zero variance issues found\n")
}
```

**Step 8 Observations:**

- Variables with >95% single value provide low information density
- May cause issues with some modeling techniques
- Consider binning or exclusion during model-specific preprocessing

## Step 9: Remove Redundant and Linear Combination Columns

Identify columns that duplicate information or are linear combinations of others.

```{r step9_redundant, echo=TRUE}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
  # Check if they're perfectly correlated
  state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
  cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
  cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")

  # If one-to-one mapping, they're redundant
  cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
  cat("Removing State_Loc (keeping State_Name)\n")

  excluded_cols <- c(excluded_cols, "State_Loc")
  FFdf$State_Loc <- NULL
}

# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]

if(length(numeric_vars) > 1) {
  # Calculate correlation matrix (handling NAs)
  cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")

  # Find highly correlated pairs (|r| > 0.95)
  high_cor <- which(abs(cor_matrix) > 0.95 & abs(cor_matrix) < 1, arr.ind = TRUE)

  if(nrow(high_cor) > 0) {
    cat("\n--- Highly correlated variable pairs (|r| > 0.95) ---\n")
    for(i in 1:nrow(high_cor)) {
      if(high_cor[i, 1] < high_cor[i, 2]) {
        var1 <- rownames(cor_matrix)[high_cor[i, 1]]
        var2 <- colnames(cor_matrix)[high_cor[i, 2]]
        cat(var1, "-", var2, ": r =", round(cor_matrix[high_cor[i, 1], high_cor[i, 2]], 3), "\n")
      }
    }
  } else {
    cat("\nNo highly correlated numeric variable pairs found (|r| > 0.95)\n")
  }
}
```

```{r step9_correlation_plot, echo=TRUE, fig.width=10, fig.height=8}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
  # Subset to variables with fewer missing values for cleaner plot
  complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]

  if(length(complete_vars) > 2) {
    cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
    corrplot(cor_subset, method = "color", type = "upper",
             tl.cex = 0.7, tl.col = "black",
             title = "Correlation Matrix - Numeric Variables",
             mar = c(0, 0, 2, 0))
  }
}
```

**Step 9 Observations:**

- State_Loc removed as redundant with State_Name
- Check correlation plot for any additional linear dependencies
- Watch for multicollinearity during modeling

## Step 10: Search for Outliers and Initial Missing Values Assessment

Identify outliers and assess the extent of missing data.

```{r step10_outliers, echo=TRUE, fig.width=10, fig.height=8}
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))

# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")

# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")

# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)

# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)

# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")

# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)

par(mfrow = c(1, 1))
```

```{r step10_missing_assessment, echo=TRUE, fig.width=10, fig.height=6}
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")

# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
    "(", round(total_missing/total_cells * 100, 2), "%)\n\n")

# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)

cat("Columns with missing values:\n")
na_summary <- data.frame(
  Variable = names(na_by_col),
  Missing_Count = as.numeric(na_by_col),
  Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)

# Visualize missing data pattern using VIM
aggr(FFdf, col = c('steelblue', 'red'), numbers = TRUE, sortVars = TRUE,
     labels = names(FFdf), cex.axis = 0.5, gap = 2,
     ylab = c("Missing Data Pattern", "Pattern"))
```

```{r step10_outlier_decisions, echo=TRUE}
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")

# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")

# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")

# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")

# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)

cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
```

## Step 11: Sanity Check Using Decision Tree

Use a simple decision tree to identify variables that are "too good" predictors or potential identifier variables.

```{r step11_decision_tree, echo=TRUE, fig.width=10, fig.height=8}
# Prepare data for decision tree
# Exclude ID columns and flag variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
                       "Marital_Original", "Last_Contact", "Contact_Year",
                       "Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour")

tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]

# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]

# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)

# Build a simple decision tree (max 2-3 splits for sanity check)
sanity_tree <- rpart(Y01 ~ .,
                     data = tree_data,
                     method = "class",
                     control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))

# Plot the tree
rpart.plot(sanity_tree,
           main = "Sanity Check Decision Tree (max depth = 3)",
           extra = 104,  # Show percentage and count
           box.palette = "RdYlGn")

# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
  var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
  print(head(var_imp, 10))
} else {
  cat("No variables selected by the tree\n")
}
```

```{r step11_tree_analysis, echo=TRUE}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")

# Check for suspiciously good predictors
cat("Tree Summary:\n")
print(sanity_tree)

cat("\n--- Interpretation ---\n")
cat("1. If a single variable achieves >90% accuracy: investigate for data leakage\n")
cat("2. If an ID-like variable appears: exclude from modeling\n")
cat("3. Top predictors should make business sense\n\n")

# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")

if(accuracy > 0.90) {
  cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
  cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
```

## Summary of Steps 6-11

```{r final_summary, echo=TRUE}
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")

cat("Step 6 - Categorical Variables:\n")
cat("  - Reviewed level distributions\n")
cat("  - Marital 'U' kept as separate category\n\n")

cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")

cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n\n")

cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n\n")

cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n\n")

cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Completed - review variable importance\n\n")

cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")

# Save cleaned dataset
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
```

### Issues Identified for Further Action:

| Step | Issue | Recommendation |
|------|-------|----------------|
| 6 | Marital "U" unknown | Keep as category or convert to NA |
| 7 | Zero-variance columns | Removed from dataset |
| 8 | Near-zero variance | Monitor during modeling |
| 9 | State_Loc redundant | Removed |
| 10 | Tenure = 400 | Verify units with SME |
| 10 | Survey_Comp > 1 | Investigate scale/cap values |
| 10 | Missing data patterns | Address in Missing Data phase |
| 11 | Tree predictors | Verify no data leakage |
