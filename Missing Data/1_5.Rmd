---
title: "Data Wrangling and Cleaning: Steps 1-11"
author: "David Mullaly"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

## Introduction

This document covers the data wrangling process and all eleven data cleaning steps for the problem. We load multiple relational tables, merge them into a single flat file, and perform comprehensive data cleaning to prepare for missing data analysis.

### Data Cleaning Steps Covered:
**Steps 1-5 (Initial Cleaning)**

1. Open data in your software of choice
2. Review variables for common sense based on SME knowledge
3. Review how the software coded the variables (nominal, continuous)
4. Perform data integrity/validation checks
5. Handle dates

**Steps 6-7 (Categorical & Zero-Variance)**

6. Handle categorical variables - keep as is, combine rare levels, combine similar levels
7. Handle zero-variance predictors

**Steps 8-11 (Advanced Cleaning)**

8. Handle near zero-variance predictors
9. Eliminate redundant columns and linear combination columns
10. Search for outliers and initial search for missing values
11. Sanity check using Decision Tree (1 to 2 splits)

```{r setup, include=FALSE}
# Setup Chunk - Load Required Libraries
library(mice)
library(VIM)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(corrplot)
library(lubridate)
library(flextable)
library(rpart)       # Decision Tree package
library(rpart.plot)  # Visualization of Decision Trees

setwd("C:/Users/david/Desktop/Spring-Semester-/Missing Data")
```

## Data Loading and Wrangling

Load four relational tables and merge into a single flat file (FFdf) using left joins on Cust_ID.

```{r load_data, echo=FALSE}
# Load the raw data files
MainDF <- read.csv("Raw.csv")
StoreDF <- read.csv("StoreTable.csv")
ConcessDF <- read.csv("ConcessTable.csv")
CustomerDF <- read.csv("CustomerTable.csv")

# Check dimensions of each file
cat("MainDF:", dim(MainDF), "| StoreDF:", dim(StoreDF),
    "| ConcessDF:", dim(ConcessDF), "| CustomerDF:", dim(CustomerDF), "\n")
```

```{r check_duplicates, echo=FALSE}
# Check for duplicate Cust_IDs in each source table BEFORE merging
cat("Duplicates in MainDF:", sum(duplicated(MainDF$Cust_ID)), "\n")
cat("Duplicates in StoreDF:", sum(duplicated(StoreDF$Cust_ID)), "\n")
cat("Duplicates in ConcessDF:", sum(duplicated(ConcessDF$Cust_ID)), "\n")
cat("Duplicates in CustomerDF:", sum(duplicated(CustomerDF$Cust_ID)), "\n")
```

```{r merge_data, echo=FALSE}
# Create flat file by merging all tables on Cust_ID
FFdf <- MainDF
FFdf <- merge(FFdf, StoreDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, ConcessDF, by = "Cust_ID", all.x = TRUE)
FFdf <- merge(FFdf, CustomerDF, by = "Cust_ID", all.x = TRUE)

cat("Final flat file dimensions - Rows:", nrow(FFdf), "Columns:", ncol(FFdf), "\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "| Duplicate rows:", nrow(FFdf) - length(unique(FFdf$Cust_ID)), "\n")
```

## Step 1: Open Data in Your Software of Choice

Create identifier variable, arrange columns, and explore distributions visually.

```{r step1_open_data, echo=FALSE, fig.width=8, fig.height=6}
# Create an identifier variable
FFdf$ID <- 1:nrow(FFdf)

# Reorder columns: Y-variable first, then alphabetically
y_var <- "Y01"
other_vars <- sort(setdiff(names(FFdf), y_var))
FFdf <- FFdf[, c(y_var, other_vars)]

# Histograms for key numeric variables
par(mfrow = c(2, 2))
hist(FFdf$Age, main = "Distribution of Age", xlab = "Age", col = "steelblue", breaks = 20)
hist(FFdf$tenure, main = "Distribution of Tenure", xlab = "Tenure (Years)", col = "steelblue", breaks = 20)
hist(FFdf$NumSeats, main = "Distribution of NumSeats", xlab = "Number of Seats", col = "steelblue")
hist(FFdf$Total.Spent, main = "Distribution of Total Spent", xlab = "Total Spent ($)", col = "steelblue", breaks = 30)
par(mfrow = c(1, 1))

# Response variable distribution
cat("Response Variable (Y01):", table(FFdf$Y01), "\n")
barplot(table(FFdf$Y01), main = "Distribution of Y01 (Response Variable)",
        col = c("coral", "steelblue"), names.arg = c("0 (No)", "1 (Yes)"))
```

**Observations:** Age is roughly normal (30-60), Tenure is right-skewed, NumSeats clusters around 2-4, Total Spent is heavily right-skewed. Response variable Y01 is reasonably balanced.

## Step 2: Review Variables for Common Sense (SME Knowledge)

Standardize variable names and check for unique identifiers.

```{r step2_common_sense, echo=FALSE}
# Clean up variable names - replace dots/spaces with underscores, capitalize
names(FFdf) <- gsub("\\.", "_", names(FFdf))
names(FFdf) <- gsub(" ", "_", names(FFdf))
names(FFdf) <- gsub("(^|_)([a-z])", "\\1\\U\\2", names(FFdf), perl = TRUE)

cat("Dataset:", nrow(FFdf), "rows,", ncol(FFdf), "columns\n")
cat("Unique Cust_ID:", length(unique(FFdf$Cust_ID)), "out of", nrow(FFdf), "rows\n")
```

**Result:** Each row does not necessarily represent a unique customer (175 duplicate Cust_IDs found in MainDF source data).

## Step 3: Review How Software Coded Variables

Convert character variables to factors for proper categorical analysis.

```{r step3_variable_types, echo=FALSE}
# Identify character variables
chr_vars <- names(FFdf)[sapply(FFdf, is.character)]
cat("Character variables to convert:", length(chr_vars), "\n")

# Convert character variables to factors
for(var in chr_vars) {
  FFdf[[var]] <- as.factor(FFdf[[var]])
}

# Show key factor levels
cat("\nSex levels:", levels(FFdf$Sex), "\n")
cat("Marital levels:", levels(FFdf$Marital), "\n")
cat("Account_Type levels:", levels(FFdf$Account_Type), "\n")
```

**Note:** Marital has levels D (Divorced), M (Married), S (Single), U (Unknown). The "U" for Unknown may need special handling.

## Step 4: Data Integrity/Validation Checks

Check for anomalies, bogus values, and data quality issues.

```{r step4_checks, echo=FALSE}
# Check numeric variable ranges
cat("Age range:", range(FFdf$Age, na.rm = TRUE), "\n")
cat("Tenure range:", range(FFdf$Tenure, na.rm = TRUE), "\n")
cat("NumSeats range:", range(FFdf$NumSeats, na.rm = TRUE), "\n")

# Check for 999 placeholder values in DistA
cat("\nDistA values = 999:", sum(FFdf$DistA == 999, na.rm = TRUE), "\n")

# Convert 999 to NA (placeholder for missing)
FFdf$DistA[FFdf$DistA == 999] <- NA
cat("DistA NA count after conversion:", sum(is.na(FFdf$DistA)), "\n")

# Check Survey_Comp for outliers (expected range 0-1)
cat("\nSurvey_Comp range:", range(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("Survey_Comp values > 1:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")

# Check for redundant columns
cat("\nState_Name unique:", length(unique(FFdf$State_Name)),
    "| State_Loc unique:", length(unique(FFdf$State_Loc)), "\n")
```

**Issues Found:**

- **DistA = 999:** Placeholder values converted to NA
- **Survey_Comp > 1:** Outlier found when expected range is 0-1
- **Age max = 99:** May be placeholder or extreme value - verify with SME
- **Tenure max = 400:** Suspicious value if measured in years - verify units with SME
- **State_Name/State_Loc:** Redundant columns (same info, different format)
- **Marital = "U":** Unknown status - consider treating as NA
- **Cust_ID duplicates:** 175 duplicate IDs found in MainDF source data
- **Address, Name, PhoneNum:** 100% missing - likely removed from CustomerDF for privacy

## Step 5: Handle Dates

Convert Last_Contact datetime and extract useful components.

```{r step5_handle_dates, echo=FALSE}
# Convert Last_Contact to datetime format
FFdf$Last_Contact <- ymd_hms(as.character(FFdf$Last_Contact))

# Extract date components
FFdf$Contact_Year <- year(FFdf$Last_Contact)
FFdf$Contact_Month <- month(FFdf$Last_Contact)
FFdf$Contact_Day <- day(FFdf$Last_Contact)
FFdf$Contact_Weekday <- wday(FFdf$Last_Contact, label = TRUE)
FFdf$Contact_Hour <- hour(FFdf$Last_Contact)

cat("Date components extracted: Contact_Year, Contact_Month, Contact_Day, Contact_Weekday, Contact_Hour\n")
cat("Contact Year range:", range(FFdf$Contact_Year, na.rm = TRUE), "\n")
cat("Contact Hour range:", range(FFdf$Contact_Hour, na.rm = TRUE), "\n")
```

## Summary

```{r summary, echo=FALSE}
cat("Final Dataset:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")
cat("Total Missing Values:", sum(is.na(FFdf)), "\n")

# Show columns with missing values
na_counts <- colSums(is.na(FFdf))
na_cols <- na_counts[na_counts > 0]
cat("\nColumns with missing values:\n")
print(sort(na_cols, decreasing = TRUE))
```

### Data Quality Issues for Future Steps:

| Issue | Possible action / Action Taken |
|-------|--------------|
| DistA = 999 | Converted to NA |
| Survey_Comp > 1 | Flag for investigation (110 values, max 7.4) |
| Age = 99 | Verify with SME |
| Tenure = 400 | Verify units with SME (400 years unlikely) |
| Marital = "U" | Consider as NA or keep |
| State redundancy | Drop one column |
| ID columns | Exclude from modeling |
| Cust_ID duplicates | 175 duplicates in MainDF - investigate or deduplicate |
| PII columns | Address, Name, PhoneNum 100% missing - exclude |



**Step 6: Handle Categorical Variables - keep as is, combine rare levels, combine similar levels**


```{r step6_categorical, echo=FALSE}
# Get all factor variables
factor_vars <- names(FFdf)[sapply(FFdf, is.factor)]
cat("Factor variables:", length(factor_vars), "\n")

# Function to check for rare levels (less than 5% of data)
check_rare_levels <- function(df, threshold = 0.05) {
  factor_vars <- names(df)[sapply(df, is.factor)]
  rare_info <- list()

  for(var in factor_vars) {
    tbl <- table(df[[var]], useNA = "ifany")
    pct <- prop.table(tbl)
    rare_levels <- names(pct[pct < threshold])
    if(length(rare_levels) > 0) {
      rare_info[[var]] <- data.frame(
        Level = rare_levels,
        Count = as.numeric(tbl[rare_levels]),
        Percent = round(as.numeric(pct[rare_levels]) * 100, 2)
      )
    }
  }
  return(rare_info)
}

# Check for rare levels
rare_levels <- check_rare_levels(FFdf, threshold = 0.05)
cat("\nVariables with rare levels (< 5%):\n")
print(names(rare_levels))

# Display level distributions for key categorical variables
cat("\n--- Sex Distribution ---\n")
print(table(FFdf$Sex, useNA = "ifany"))

cat("\n--- Marital Distribution ---\n")
print(table(FFdf$Marital, useNA = "ifany"))

cat("\n--- Account_Type Distribution ---\n")
print(table(FFdf$Account_Type, useNA = "ifany"))

cat("\n--- Educational_Level Distribution ---\n")
print(table(FFdf$Educational_Level, useNA = "ifany"))
```

```{r step6_handle_categorical, echo=FALSE}
# Handle Marital "U" (Unknown) - treat as NA for analysis purposes
# Create backup first
FFdf$Marital_Original <- FFdf$Marital

# Option: Convert "U" to NA (uncomment if desired)
# FFdf$Marital[FFdf$Marital == "U"] <- NA
# FFdf$Marital <- droplevels(FFdf$Marital)

cat("Marital 'U' (Unknown) count:", sum(FFdf$Marital == "U", na.rm = TRUE), "\n")
cat("Decision: Keep 'U' as separate level for now - may represent meaningful unknown status\n")

# Check for levels that might mean the same thing
cat("\n--- State_Name levels ---\n")
print(length(levels(FFdf$State_Name)))
cat("Number of unique states:", length(unique(FFdf$State_Name)), "\n")
```

```{r step6_lump_rare_levels, echo=FALSE}
# Lump rare factor levels into "Other" using forcats::fct_lump_prop()
# Threshold: levels appearing in < 5% of non-NA observations are combined

# Store original levels for reference
factor_vars_to_lump <- names(FFdf)[sapply(FFdf, is.factor)]

# Exclude variables we handle separately (State_Name -> Region, Marital already reviewed)
skip_vars <- c("State_Name", "State_Loc", "Marital", "Marital_Original","Educational_Level","Seating_Location","Favorite_Caps_Player","Job_Sector")
lump_vars <- setdiff(factor_vars_to_lump, skip_vars)

cat("=== LUMPING RARE LEVELS (< 5%) INTO 'Other' ===\n\n")

for(var in lump_vars) {
  original_levels <- nlevels(FFdf[[var]])
  FFdf[[var]] <- fct_lump_prop(FFdf[[var]], prop = 0.05, other_level = "Other")
  new_levels <- nlevels(FFdf[[var]])

  if(original_levels != new_levels) {
    cat(var, ": ", original_levels, " -> ", new_levels, " levels\n", sep = "")
    print(table(FFdf[[var]], useNA = "ifany"))
    cat("\n")
  }
}

cat("Lumping complete.\n")
```

```{r step6_group_states_by_region, echo=FALSE}


# Define region mappings
northeast <- c("Connecticut", "Maine", "Massachusetts", "New Hampshire",
               "Rhode Island", "Vermont", "New Jersey", "New York", "Pennsylvania")

midwest <- c("Illinois", "Indiana", "Michigan", "Ohio", "Wisconsin",
             "Iowa", "Kansas", "Minnesota", "Missouri", "Nebraska",
             "North Dakota", "South Dakota")

south <- c("Delaware", "Florida", "Georgia", "North Carolina",
           "South Carolina", "West Virginia",
           "Alabama", "Kentucky", "Mississippi", "Tennessee",
           "Arkansas", "Louisiana", "Oklahoma", "Texas")

west <- c("Arizona", "Colorado", "Idaho", "Montana", "Nevada", "New Mexico",
          "Utah", "Wyoming", "Alaska", "California", "Hawaii", "Oregon", "Washington")

dcarea <- c("Maryland","Virginia", "District of Columbia")

# Create Region variable
FFdf$Region <- case_when(
  FFdf$State_Name %in% northeast ~ "Northeast",
  FFdf$State_Name %in% midwest ~ "Midwest",
  FFdf$State_Name %in% south ~ "South",
  FFdf$State_Name %in% west ~ "West",
  FFdf$State_Name %in% dcarea ~ "DCArea",
  TRUE ~ "Other"  # Catch any unmatched states
)

# Convert to factor
FFdf$Region <- as.factor(FFdf$Region)

# Display region distribution
cat("--- Region Distribution (grouped from State_Name) ---\n")
print(table(FFdf$Region, useNA = "ifany"))
cat("\nRegion percentages:\n")
print(round(prop.table(table(FFdf$Region)) * 100, 2))

# Check if any states weren't mapped
unmatched_states <- unique(FFdf$State_Name[FFdf$Region == "Other"])
if(length(unmatched_states) > 0) {
  cat("\nWarning - Unmatched states assigned to 'Other':\n")
  print(unmatched_states)
} else {
  cat("\nAll states successfully mapped to regions.\n")
}
```

**Step 6 Observations:**

- Marital has "U" (Unknown) level - kept as separate category for now
- Educational_Level could be made ordinal if needed for certain models
- **State_Name grouped into US regions** - reduces cardinality from 50 levels to 5 (Northeast, Midwest, South, West, DCArea)
- **Rare levels (< 5%) lumped into "Other"** using `fct_lump_prop()` for all applicable factor variables

---

**Step 7: Remove Zero-Variance Predictors**



```{r step7_zero_variance, echo=FALSE}
# Function to identify zero-variance columns
find_zero_variance <- function(df) {
  zv_cols <- c()
  for(col in names(df)) {
    unique_vals <- length(unique(na.omit(df[[col]])))
    if(unique_vals <= 1) {
      zv_cols <- c(zv_cols, col)
    }
  }
  return(zv_cols)
}

# Find zero-variance columns
zero_var_cols <- find_zero_variance(FFdf)
cat("Zero-variance columns found:", length(zero_var_cols), "\n")

if(length(zero_var_cols) > 0) {
  cat("Columns with zero variance:\n")
  print(zero_var_cols)

  # Store in excluded columns list
  excluded_cols <- zero_var_cols

  # Remove zero-variance columns
  FFdf <- FFdf[, !names(FFdf) %in% zero_var_cols]
  cat("\nRemoved", length(zero_var_cols), "zero-variance columns\n")
} else {
  cat("No zero-variance columns found\n")
  excluded_cols <- c()
}

cat("Remaining columns:", ncol(FFdf), "\n")
```

**Step 7 Results:**

The following 8 zero-variance columns were identified and removed:

- **Address, Name, PhoneNum**: PII columns - 100% missing (intentionally scrubbed)
- **InfRate, UnempRate**: Economic indicators - likely constant for this snapshot
- **Last_Team_Championship, NHL_Team_Record, Playoffs**: Team-related constants

These columns provide no predictive value since every observation has the same value (or all NA).

---

## Class 4: Data Cleaning Process: Steps 8-11

**Step 8: Handle Near Zero-Variance Predictors**


```{r step8_near_zero_variance, echo=FALSE}
# Function to find near-zero variance columns
# A column is NZV if one value dominates (e.g., >95% of values)
find_near_zero_variance <- function(df, threshold = 0.95) {
  nzv_info <- data.frame(
    Variable = character(),
    DominantValue = character(),
    DominantPct = numeric(),
    UniqueValues = integer(),
    stringsAsFactors = FALSE
  )

  for(col in names(df)) {
    if(is.numeric(df[[col]]) || is.factor(df[[col]])) {
      tbl <- table(df[[col]], useNA = "no")
      if(length(tbl) > 0) {
        max_pct <- max(tbl) / sum(tbl)
        if(max_pct >= threshold) {
          dominant_val <- names(tbl)[which.max(tbl)]
          nzv_info <- rbind(nzv_info, data.frame(
            Variable = col,
            DominantValue = as.character(dominant_val),
            DominantPct = round(max_pct * 100, 2),
            UniqueValues = length(tbl),
            stringsAsFactors = FALSE
          ))
        }
      }
    }
  }
  return(nzv_info)
}

# Find near-zero variance columns (>95% one value)
nzv_cols <- find_near_zero_variance(FFdf, threshold = 0.95)
cat("Near-zero variance columns (>95% one value):\n")
print(nzv_cols)
```

```{r step8_handle_nzv, echo=FALSE}
# Decision: Flag NZV columns but don't remove yet
# These may still be useful predictors depending on modeling goals
nzv_variables <- nzv_cols$Variable

if(length(nzv_variables) > 0) {
  cat("Near-zero variance variables to monitor:\n")
  print(nzv_variables)
  cat("\nDecision: Keep for now but flag for potential exclusion during modeling\n")
} else {
  cat("No significant near-zero variance issues found\n")
}
```

**Step 8 Results:**

Near-zero variance columns identified (>95% one value):

| Variable | Dominant Value | Dominant % |
|----------|----------------|------------|
| Additional_Seats | 0 | 96.99% |
| Mult_Loc | No | 96.99% |

**Observations:**

- **Additional_Seats**: 97% of customers have 0 additional seats - consider binning (0 vs >0)
- **Mult_Loc**: 97% are "No" - low information but may still be predictive for the 3% minority
- Decision: Keep for now but flag for potential exclusion during modeling
- May cause issues with some modeling techniques (especially regression-based)

---

**Step 9: Remove Redundant Columns and Linear Combination Columns**


```{r step9_redundant, echo=FALSE}
# Check for redundant categorical columns (State_Name vs State_Loc)
cat("--- Checking State_Name vs State_Loc redundancy ---\n")
if("State_Name" %in% names(FFdf) && "State_Loc" %in% names(FFdf)) {
  # Check if they're perfectly correlated
  state_comparison <- table(FFdf$State_Name, FFdf$State_Loc)
  cat("State_Name unique values:", length(unique(FFdf$State_Name)), "\n")
  cat("State_Loc unique values:", length(unique(FFdf$State_Loc)), "\n")

  # If one-to-one mapping, they're redundant
  cat("\nDecision: State_Name and State_Loc appear to be the same information.\n")
  cat("Removing State_Loc (keeping State_Name)\n")

  excluded_cols <- c(excluded_cols, "State_Loc")
  FFdf$State_Loc <- NULL
}

# Check correlation matrix for numeric variables
numeric_vars <- names(FFdf)[sapply(FFdf, is.numeric)]
# Exclude ID columns from correlation check
numeric_vars <- numeric_vars[!numeric_vars %in% c("ID", "Cust_ID")]

if(length(numeric_vars) > 1) {
  # Calculate correlation matrix (handling NAs)
  cor_matrix <- cor(FFdf[, numeric_vars], use = "pairwise.complete.obs")

  # Find highly correlated pairs at multiple thresholds
  cat("\n--- Highly correlated variable pairs (|r| > 0.85) ---\n")
  cat("These pairs may cause multicollinearity in regression models\n\n")

  high_cor_85 <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind = TRUE)

  if(nrow(high_cor_85) > 0) {
    cor_pairs <- data.frame(Var1 = character(), Var2 = character(),
                            Correlation = numeric(), stringsAsFactors = FALSE)
    for(i in 1:nrow(high_cor_85)) {
      if(high_cor_85[i, 1] < high_cor_85[i, 2]) {
        var1 <- rownames(cor_matrix)[high_cor_85[i, 1]]
        var2 <- colnames(cor_matrix)[high_cor_85[i, 2]]
        r_val <- cor_matrix[high_cor_85[i, 1], high_cor_85[i, 2]]
        cor_pairs <- rbind(cor_pairs, data.frame(Var1 = var1, Var2 = var2,
                                                  Correlation = round(r_val, 3)))
      }
    }
    cor_pairs <- cor_pairs[order(abs(cor_pairs$Correlation), decreasing = TRUE), ]
    print(cor_pairs)
  } else {
    cat("No highly correlated numeric variable pairs found (|r| > 0.85)\n")
  }
}
```

```{r step9_correlation_plot, echo=FALSE, fig.width=10, fig.height=8}
# Visualize correlation matrix for numeric variables
if(length(numeric_vars) > 2) {
  # Subset to variables with fewer missing values for cleaner plot
  complete_vars <- numeric_vars[colSums(is.na(FFdf[, numeric_vars])) < nrow(FFdf) * 0.5]

  if(length(complete_vars) > 2) {
    cor_subset <- cor(FFdf[, complete_vars], use = "pairwise.complete.obs")
    corrplot(cor_subset, method = "color", type = "upper",
             tl.cex = 0.7, tl.col = "black",
             title = "Correlation Matrix - Numeric Variables",
             mar = c(0, 0, 2, 0))
  }
}
```

```{r step9_handle_multicollinearity, echo=FALSE}
# Address multicollinearity based on correlation matrix analysis
cat("=== MULTICOLLINEARITY ANALYSIS ===\n\n")

# Identify correlated variable clusters from the correlation matrix
cat("Cluster 1: Spending & Visit variables\n")
cat("  - Rep_Visits <-> Total_Spent: r = 0.947 (very strong positive)\n")
cat("  - Team_Store_Total <-> Total_Spent: r = 0.853 (strong positive)\n")
cat("  Recommendation: Consider removing Rep_Visits or Total_Spent\n\n")

cat("Cluster 2: Concession & Seating\n")
cat("  - Concession_Total <-> NumSeats: r = 0.915 (strong positive)\n")
cat("  Recommendation: Makes business sense - more seats = more concessions\n\n")

cat("Cluster 3: Attendance pairs\n")
cat("  - Weekday_Attended <-> Weekday_Sold: r = -0.946 (strong NEGATIVE)\n")
cat("  Note: Negative correlation suggests inverse relationship\n")
cat("  Recommendation: Keep both - they capture different behaviors\n\n")

# Create list of variables to flag for multicollinearity
multicollinear_vars <- c("Rep_Visits", "Team_Store_Total")
cat("Variables flagged for potential removal due to multicollinearity:\n")
print(multicollinear_vars)

# Option: Remove highly correlated variables (uncomment to execute)
# FFdf <- FFdf[, !names(FFdf) %in% multicollinear_vars]
# excluded_cols <- c(excluded_cols, multicollinear_vars)
cat("\nDecision: Flag but keep for now; remove during modeling if VIF > 10\n")
```

**Step 9 Observations:**

Based on the correlation matrix analysis (actual results from output above):

| Cluster | Variables | Correlation | Recommendation |
|---------|-----------|-------------|----------------|
| 1 | Rep_Visits vs Total_Spent | r = 0.947 | Remove Rep_Visits |
| 1 | Team_Store_Total vs Total_Spent | r = 0.853 | Monitor for VIF |
| 2 | Concession_Total vs NumSeats | r = 0.915 | Keep - business logic |
| 3 | Weekday_Attended vs Weekday_Sold | r = -0.946 | Keep both - inverse relationship |
| - | State_Loc vs State_Name | Redundant | **REMOVED** |

**Action Items:**
- State_Loc removed (redundant with State_Name)
- Flagged 2 variables for potential removal: Rep_Visits, Team_Store_Total
- Will check VIF during modeling phase and remove if VIF > 10

---

**Step 10: Search for Outliers and Initial Search for Missing Values**

```{r step10_outliers, echo=FALSE, fig.width=10, fig.height=8}
# Boxplots for key numeric variables to identify outliers
par(mfrow = c(2, 3))

# Age
boxplot(FFdf$Age, main = "Age", col = "lightblue", outline = TRUE)
age_outliers <- boxplot.stats(FFdf$Age)$out
cat("Age outliers (IQR method):", length(age_outliers), "values\n")

# Tenure
boxplot(FFdf$Tenure, main = "Tenure", col = "lightblue", outline = TRUE)
tenure_outliers <- boxplot.stats(FFdf$Tenure)$out
cat("Tenure outliers:", length(tenure_outliers), "values | Max:", max(FFdf$Tenure, na.rm = TRUE), "\n")

# Total_Spent
boxplot(FFdf$Total_Spent, main = "Total_Spent", col = "lightblue", outline = TRUE)

# NumSeats
boxplot(FFdf$NumSeats, main = "NumSeats", col = "lightblue", outline = TRUE)

# Survey_Comp
boxplot(FFdf$Survey_Comp, main = "Survey_Comp", col = "lightblue", outline = TRUE)
survey_outliers <- sum(FFdf$Survey_Comp > 1, na.rm = TRUE)
cat("Survey_Comp values > 1:", survey_outliers, "\n")

# DistA
boxplot(FFdf$DistA, main = "DistA", col = "lightblue", outline = TRUE)

par(mfrow = c(1, 1))
```

```{r step10_missing_assessment, echo=FALSE, fig.width=10, fig.height=6}
# Comprehensive missing data assessment
cat("=== MISSING DATA ASSESSMENT ===\n\n")

# Total missing values
total_missing <- sum(is.na(FFdf))
total_cells <- nrow(FFdf) * ncol(FFdf)
cat("Total missing values:", total_missing, "out of", total_cells,
    "(", round(total_missing/total_cells * 100, 2), "%)\n\n")

# Missing by column
na_by_col <- colSums(is.na(FFdf))
na_by_col <- na_by_col[na_by_col > 0]
na_by_col <- sort(na_by_col, decreasing = TRUE)

cat("Columns with missing values:\n")
na_summary <- data.frame(
  Variable = names(na_by_col),
  Missing_Count = as.numeric(na_by_col),
  Missing_Pct = round(as.numeric(na_by_col) / nrow(FFdf) * 100, 2)
)
print(na_summary)

```

```{r step10_outlier_decisions, echo=FALSE}
# Document outlier decisions
cat("\n=== OUTLIER DECISIONS ===\n\n")

# Tenure = 400 (suspicious if years)
cat("1. Tenure max = 400:\n")
cat("   - If measured in years, this is impossible\n")
cat("   - May be measured in months (400 months = 33 years - plausible)\n")
cat("   - ACTION: Verify units with SME; flag for review\n\n")

# Age = 99
cat("2. Age = 99:\n")
cat("   - Could be real (elderly customer) or placeholder\n")
cat("   - ACTION: Verify with SME; consider if 99 is data entry default\n\n")

# Survey_Comp > 1
cat("3. Survey_Comp values > 1 (expected 0-1 range):\n")
cat("   - Count:", sum(FFdf$Survey_Comp > 1, na.rm = TRUE), "\n")
cat("   - Max value:", max(FFdf$Survey_Comp, na.rm = TRUE), "\n")
cat("   - ACTION: Possible scale issue; cap at 1 or investigate data source\n\n")

# Create outlier flags for further analysis
FFdf$Flag_Tenure_High <- ifelse(FFdf$Tenure > 100, 1, 0)
FFdf$Flag_Survey_Invalid <- ifelse(FFdf$Survey_Comp > 1, 1, 0)

cat("Created outlier flag variables: Flag_Tenure_High, Flag_Survey_Invalid\n")
```

---

**Step 11: Sanity Check Using Decision Tree (1 to 2 splits)**


```{r step11_decision_tree, echo=FALSE, fig.width=10, fig.height=8}
# Prepare data for decision tree
# Exclude ID columns, flag variables, and high-cardinality variables
exclude_from_tree <- c("ID", "Cust_ID", "Flag_Tenure_High", "Flag_Survey_Invalid",
                       "Marital_Original", "Last_Contact", "Contact_Year",
                       "Contact_Month", "Contact_Day", "Contact_Weekday", "Contact_Hour",
                       "State_Name", "Zip_Codes")

tree_vars <- names(FFdf)[!names(FFdf) %in% exclude_from_tree]
tree_data <- FFdf[, tree_vars]

# Remove rows with NA in response variable
tree_data <- tree_data[!is.na(tree_data$Y01), ]

# Convert Y01 to factor for classification tree
tree_data$Y01 <- as.factor(tree_data$Y01)

# Build a simple decision tree (max 2-3 splits for sanity check)
# Uses Region variable (from Step 6) instead of high-cardinality State_Name/Zip_Codes
sanity_tree <- rpart(Y01 ~ .,
                     data = tree_data,
                     method = "class",
                     control = rpart.control(maxdepth = 3, minsplit = 20, cp = 0.01))

# Plot the tree
rpart.plot(sanity_tree,
           main = "Sanity Check Decision Tree (max depth = 3)",
           extra = 104,  # Show percentage and count
           box.palette = "RdYlGn")

# Variable importance
cat("\n=== VARIABLE IMPORTANCE ===\n")
if(length(sanity_tree$variable.importance) > 0) {
  var_imp <- sort(sanity_tree$variable.importance, decreasing = TRUE)
  print(head(var_imp, 10))
} else {
  cat("No variables selected by the tree\n")
}
```

```{r step11_tree_analysis, echo=FALSE}
# Analyze tree results
cat("\n=== SANITY CHECK ANALYSIS ===\n\n")

# Check tree performance
predictions <- predict(sanity_tree, tree_data, type = "class")
accuracy <- mean(predictions == tree_data$Y01, na.rm = TRUE)
cat("Tree accuracy:", round(accuracy * 100, 2), "%\n")

if(accuracy > 0.90) {
  cat("WARNING: Very high accuracy - check for data leakage or identifier variables!\n")
} else {
  cat("Accuracy is reasonable - no obvious data leakage detected\n")
}
```

**Step 11 Results:**

The decision tree uses Region (from Step 6) instead of raw State_Name/Zip_Codes to avoid overfitting from high-cardinality variables.

Top Variable Importance:

| Variable | Importance |
|----------|------------|
| Region | 1505.2 |
| Favorite_Team | 253.5 |
| PerUsed | 245.2 |
| DistA | 196.8 |
| Tenure | 187.7 |

**Analysis:**

- **Accuracy ~88%** - reasonable, no obvious data leakage
- **Region is the dominant predictor** - geographic location strongly predicts Y01
- **No high-cardinality variables** causing artificial inflation of accuracy

---

## Summary of Steps 6-11 (Data Cleaning Complete)

```{r final_summary, echo=FALSE}
cat("=== DATA CLEANING SUMMARY (Steps 6-11) ===\n\n")

cat("Step 6 - Handle Categorical Variables:\n")
cat("  - Rare factor levels (< 5%) lumped into 'Other' via fct_lump_prop()\n")
cat("  - Marital 'U' kept as separate category\n")
cat("  - State_Name grouped into US Census regions\n\n")

cat("Step 7 - Zero-Variance Predictors:\n")
cat("  - Columns removed:", length(zero_var_cols), "\n\n")

cat("Step 8 - Near Zero-Variance Predictors:\n")
cat("  - Variables flagged:", length(nzv_variables), "\n")
cat("  - Decision: Keep for now but monitor during modeling\n\n")

cat("Step 9 - Redundant Columns:\n")
cat("  - State_Loc removed (redundant with State_Name)\n")
cat("  - Correlation matrix reviewed for multicollinearity\n\n")

cat("Step 10 - Outliers & Missing Data:\n")
cat("  - Total missing values:", sum(is.na(FFdf)), "\n")
cat("  - Outlier flags created for Tenure and Survey_Comp\n")
cat("  - Missing data summary table generated\n\n")

cat("Step 11 - Decision Tree Sanity Check:\n")
cat("  - Tree accuracy:", round(accuracy * 100, 2), "%\n")
cat("  - Review variable importance for potential data leakage\n\n")

cat("Final dataset dimensions:", nrow(FFdf), "rows x", ncol(FFdf), "columns\n")

# Save cleaned dataset for next phase (Missing Data)
write.csv(FFdf, "FFdf_cleaned.csv", row.names = FALSE)
cat("\nCleaned dataset saved to: FFdf_cleaned.csv\n")
```

### Issues Identified for Further Action:

| Step | Issue | Recommendation |
|------|-------|----------------|
| 6 | Marital "U" unknown | Keep as category or convert to NA during imputation |
| 6 | Rare factor levels (< 5%) | **RESOLVED:** Lumped into "Other" via fct_lump_prop() |
| 6 | State_Name high cardinality | **RESOLVED:** Grouped into US Census regions |
| 7 | Zero-variance columns | Removed from dataset |
| 8 | Near-zero variance | Monitor during modeling; consider binning |
| 9 | State_Loc redundant | Removed |
| 10 | Tenure = 400 | Verify units with SME (years vs months?) |
| 10 | Survey_Comp > 1 | Investigate scale/cap values at 1 |
| 10 | Missing data patterns | Address in Missing Data phase (Class 5+) |
| 11 | Tree predictors | **RESOLVED:** Using Region variable instead of State_Name |

---

## Class 7: Handling Missing Data - Steps 1-4

**Step 1: Identify Missing Data**

```{r md_step1_identify_missing, echo=TRUE}
cat("=== STEP 1: IDENTIFY MISSING DATA ===\n\n")

# Start Missing Data workflow from cleaned dataset
MDdf <- FFdf

missing_counts_step1 <- colSums(is.na(MDdf))
missing_counts_step1 <- sort(missing_counts_step1[missing_counts_step1 > 0], decreasing = TRUE)
missing_table_step1 <- data.frame(
  Variable = names(missing_counts_step1),
  Missing_Count = as.numeric(missing_counts_step1),
  Missing_Pct = round(as.numeric(missing_counts_step1) / nrow(MDdf) * 100, 2)
)

cat("Variables with missing values:", length(missing_counts_step1), "\n")
cat("Total missing cells:", sum(is.na(MDdf)), "\n\n")
print(missing_table_step1)

# Optional pattern table from mice
md_pattern_step1 <- tryCatch(mice::md.pattern(MDdf, plot = FALSE), error = function(e) NULL)
if(!is.null(md_pattern_step1)) {
  cat("\nTop missingness patterns (mice::md.pattern):\n")
  print(head(md_pattern_step1, 8))
}
```

**Step 1 Results and Interpretation:**

- Variables with missing values: **`r length(missing_counts_step1)`**
- Total missing cells: **`r format(sum(is.na(MDdf)), big.mark = ",")`**
- Highest-missing variables are concentrated in demographic/profile fields (for example: `r paste(names(head(missing_counts_step1, 5)), collapse = ", ")`)
- Interpretation: missingness is substantial and not isolated to a single field, so a structured missing-data workflow is required.

**Step 2: Mark Missing Data**

```{r md_step2_mark_missing, echo=TRUE}
cat("=== STEP 2: MARK MISSING DATA ===\n\n")

# Create indicator columns for variables with missingness
vars_with_na_step2 <- names(MDdf)[colSums(is.na(MDdf)) > 0]
indicator_names_step2 <- paste0(vars_with_na_step2, "_Missing")

for(v in vars_with_na_step2) {
  MDdf[[paste0(v, "_Missing")]] <- ifelse(is.na(MDdf[[v]]), 1, 0)
}

cat("Missingness indicators created:", length(vars_with_na_step2), "\n")
cat("Example indicators:\n")
print(head(indicator_names_step2, 10))
cat("\nIdentifier columns retained:",
    all(c("ID", "Cust_ID") %in% names(MDdf)), "\n")
```

**Step 2 Results and Interpretation:**

- Missingness indicators created: **`r length(vars_with_na_step2)`**
- Indicator columns preserve traceability between original NA values and imputed values.
- Interpretation: this supports mechanism testing (MCAR/MAR checks) and sensitivity analysis later.

**Step 3: Clean Up Obvious Mistakes**

```{r md_step3_cleanup_mistakes, echo=TRUE}
cat("=== STEP 3: CLEAN UP OBVIOUS MISTAKES ===\n\n")

# Standardize text fields and convert blank strings to NA
before_missing_step3 <- sum(is.na(MDdf))
added_na_from_blanks_step3 <- 0

text_cols <- names(MDdf)[sapply(MDdf, function(x) is.factor(x) || is.character(x))]
factor_cols <- names(MDdf)[sapply(MDdf, is.factor)]

for(col in text_cols) {
  x <- trimws(as.character(MDdf[[col]]))
  blank_idx <- !is.na(x) & x == ""
  added_na_from_blanks_step3 <- added_na_from_blanks_step3 + sum(blank_idx)
  x[blank_idx] <- NA
  if(is.factor(MDdf[[col]])) {
    MDdf[[col]] <- as.factor(x)
  } else {
    MDdf[[col]] <- x
  }
}



after_missing_step3 <- sum(is.na(MDdf))
net_new_missing_step3 <- after_missing_step3 - before_missing_step3

cat("Blank strings converted to NA:", added_na_from_blanks_step3, "\n")
cat("Marital 'U' converted to NA:", marital_u_to_na_step3, "\n")
cat("Net change in missing cells:", net_new_missing_step3, "\n")
cat("Total missing cells after cleanup:", after_missing_step3, "\n")

# Keep factors compact after cleanup
for(col in factor_cols) {
  MDdf[[col]] <- droplevels(MDdf[[col]])
}

cat("Whitespace cleanup applied to factor columns:", length(factor_cols), "\n")
if("Marital" %in% names(MDdf)) {
  cat("Marital levels after cleanup:\n")
  print(levels(MDdf$Marital))
}
```

**Step 3 Results and Interpretation:**

- Factor columns standardized: **`r length(factor_cols)`**
- Blank strings converted to NA: **`r added_na_from_blanks_step3`**
- Marital "U" converted to NA: **`r marital_u_to_na_step3`**
- Marital levels after cleanup: **`r if("Marital" %in% names(MDdf)) paste(levels(MDdf$Marital), collapse = ", ") else "Marital not present"`**
- Interpretation: obvious coding issues are cleaned and unknown categories are consistently represented as missing for downstream imputation decisions.

**Step 4: Make Decisions on Rows/Columns**

```{r md_step4_row_col_decisions, echo=TRUE}
cat("=== STEP 4: ROW/COLUMN DECISIONS ===\n\n")

# Missingness by column
na_pct_step4 <- colSums(is.na(MDdf)) / nrow(MDdf)
high_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.40])
extreme_missing_cols_step4 <- names(na_pct_step4[na_pct_step4 > 0.65])

cat("Columns > 40% missing:", length(high_missing_cols_step4), "\n")
if(length(high_missing_cols_step4) > 0) {
  print(high_missing_cols_step4)
} else {
  cat("No columns exceed 40% missingness threshold.\n")
}

# Missingness by row
row_missing_pct_step4 <- rowSums(is.na(MDdf)) / ncol(MDdf)
rows_high_missing_step4 <- which(row_missing_pct_step4 > 0.50)

cat("\nRows > 50% missing:", length(rows_high_missing_step4), "\n")

# Mark (do not delete) for modeling decisions
excluded_cols_md_step4 <- unique(c("ID", "Cust_ID", extreme_missing_cols_step4))
cat("Columns marked for potential exclusion from modeling (not deleted):\n")
print(excluded_cols_md_step4)
cat("Decision: mark rows/columns for modeling strategy; avoid hard deletes at this stage.\n")
```

**Step 4 Results and Interpretation:**

- Columns above 40% missingness: **`r length(high_missing_cols_step4)`**
- Rows above 50% missingness: **`r length(rows_high_missing_step4)`**
- Columns marked (not deleted): **`r length(excluded_cols_md_step4)`**
- Interpretation: column-level missingness is the dominant issue; variables are flagged for strategy decisions rather than removed outright.

---

## Class 8: Handling Missing Data - Step 5

**Step 5: Assess Missingness Patterns**

```{r md_step5_missing_patterns, echo=TRUE, fig.width=10, fig.height=6}
cat("=== STEP 5: ASSESS MISSINGNESS PATTERNS ===\n\n")

# Pairwise missingness summary
missing_matrix_step5 <- is.na(MDdf)
cat("Any complete rows:", any(rowSums(missing_matrix_step5) == 0), "\n")
cat("Rows with at least one missing value:", sum(rowSums(missing_matrix_step5) > 0), "\n")

# Bar chart of top missing variables
na_count_step5 <- sort(colSums(missing_matrix_step5), decreasing = TRUE)
na_count_step5 <- na_count_step5[na_count_step5 > 0]

if(length(na_count_step5) > 0) {
  top_na_step5 <- head(na_count_step5, 15)
  barplot(top_na_step5,
          las = 2,
          col = "steelblue",
          main = "Top Variables by Missing Count",
          ylab = "Missing Count")
} else {
  plot.new()
  text(0.5, 0.5, "No missing values detected")
}

cat("\n=== MCAR / MAR / MNAR ASSESSMENT (PRACTICAL) ===\n\n")

# Practical mechanism check:
# - Manual association tests + simple decision tree used for judgment (per course guidance)
# - If missingness relates to observed data -> likely MAR
# - If no relationships detected -> could be MCAR
# - MNAR is flagged as possible for sensitive/high-missing variables, then SME validated

vars_with_na_step5 <- names(MDdf)[colSums(is.na(MDdf)) > 0]
assessment_step5 <- data.frame(
  Variable = character(),
  Missing_Count = integer(),
  Missing_Pct = numeric(),
  Manual_Tests_Run = integer(),
  Significant_Assoc = integer(),
  Tree_Has_Splits = character(),
  Likely_Mechanism = character(),
  MNAR_Risk = character(),
  Judgement_Basis = character(),
  stringsAsFactors = FALSE
)

# Variables where nonresponse may depend on latent/unobserved value (domain risk)
mnar_sensitive_vars <- c(
  "Educational_Level", "Favorite_Caps_Player", "Favorite_Sport",
  "HouseHold_Income_True", "Job_Sector", "Mode_Of_Transport",
  "Net_Worth_True", "Team_B_STH", "Team_C_STH"
)

tree_predictors_step5 <- intersect(c(
  "Age", "Tenure", "Total_Spent", "NumSeats", "Survey_Comp", "DistA",
  "Sex", "Marital", "Account_Type", "Region", "Rep_Visits", "Rep_Calls"
), names(MDdf))

for(v in vars_with_na_step5) {
  miss_ind <- is.na(MDdf[[v]])
  if(sum(miss_ind) == 0 || sum(!miss_ind) == 0) next

  tests_run <- 0
  sig_assoc <- 0

  # Manual tests against a stable predictor set
  candidate_preds <- setdiff(tree_predictors_step5, v)

  for(p in candidate_preds) {
    pred <- MDdf[[p]]
    if(all(is.na(pred))) next
    ok <- !is.na(pred)
    if(sum(ok) < 30) next

    pval <- NA
    if(is.numeric(pred)) {
      # Numeric predictor vs missingness group
      if(length(unique(miss_ind[ok])) == 2) {
        pval <- tryCatch(wilcox.test(pred[ok] ~ miss_ind[ok])$p.value, error = function(e) NA)
      }
    } else if(is.factor(pred) || is.character(pred) || is.logical(pred)) {
      # Categorical predictor vs missingness group
      tab <- table(miss_ind[ok], pred[ok])
      if(all(dim(tab) >= c(2, 2))) {
        pval <- tryCatch(suppressWarnings(chisq.test(tab)$p.value), error = function(e) NA)
      }
    }

    if(!is.na(pval)) {
      tests_run <- tests_run + 1
      if(pval < 0.05) sig_assoc <- sig_assoc + 1
    }
  }

  # Decision tree check for non-random missingness structure
  tree_has_splits <- FALSE
  tree_df <- MDdf[, c(setdiff(tree_predictors_step5, v)), drop = FALSE]
  if(ncol(tree_df) > 0) {
    tree_df$miss_flag <- as.factor(ifelse(is.na(MDdf[[v]]), "Missing", "Observed"))
    tree_df <- tree_df[complete.cases(tree_df), , drop = FALSE]
    if(nrow(tree_df) >= 200 && length(unique(tree_df$miss_flag)) == 2) {
      tree_fit <- tryCatch(
        rpart(miss_flag ~ ., data = tree_df, method = "class",
              control = rpart.control(maxdepth = 2, minsplit = 100, cp = 0.01)),
        error = function(e) NULL
      )
      if(!is.null(tree_fit) && nrow(tree_fit$frame) > 1) {
        tree_has_splits <- TRUE
      }
    }
  }

  missing_pct <- round(mean(miss_ind) * 100, 2)

  mnar_risk <- FALSE
  judgement_basis <- "Observed-data checks only"
  if(v %in% mnar_sensitive_vars && missing_pct >= 40) {
    mnar_risk <- TRUE
    judgement_basis <- "Sensitive/self-report field with high missingness"
  }

  mech <- if(mnar_risk) {
    "Possible MNAR (needs SME validation)"
  } else if(sig_assoc > 0 || tree_has_splits) {
    "Likely MAR (observed-data relationship)"
  } else if(tests_run > 0) {
    "Could be MCAR"
  } else {
    "Insufficient evidence"
  }

  assessment_step5 <- rbind(assessment_step5, data.frame(
    Variable = v,
    Missing_Count = sum(miss_ind),
    Missing_Pct = missing_pct,
    Manual_Tests_Run = tests_run,
    Significant_Assoc = sig_assoc,
    Tree_Has_Splits = ifelse(tree_has_splits, "Yes", "No"),
    Likely_Mechanism = mech,
    MNAR_Risk = ifelse(mnar_risk, "Yes", "No"),
    Judgement_Basis = judgement_basis,
    stringsAsFactors = FALSE
  ))
}

if(nrow(assessment_step5) > 0) {
  assessment_step5 <- assessment_step5[order(assessment_step5$Missing_Pct, decreasing = TRUE), ]
  print(assessment_step5)
  cat("\nMechanism counts:\n")
  mechanism_counts_step5 <- table(assessment_step5$Likely_Mechanism)
  print(mechanism_counts_step5)

  cat("\nInterpretation guide:\n")
  cat("- MCAR: no detected relationship between missingness and observed variables.\n")
  cat("- MAR: missingness related to observed data (manual tests and/or tree split patterns).\n")
  cat("- MNAR: flagged as POSSIBLE for sensitive/high-missing fields; confirm with SME and sensitivity analysis.\n")
  cat("- Course note: judgment is prioritized over strict significance in this stage.\n")
} else {
  cat("No variables with missing values to assess.\n")
}
```

**Step 5 Results and Interpretation:**

- Rows with at least one missing value: **`r sum(rowSums(is.na(MDdf)) > 0)`** of **`r nrow(MDdf)`**
- Mechanism calls from assessment table:
  - Possible MNAR (needs SME validation): **`r if(exists("assessment_step5")) sum(assessment_step5$Likely_Mechanism == "Possible MNAR (needs SME validation)") else 0`**
  - Likely MAR (observed-data relationship): **`r if(exists("assessment_step5")) sum(assessment_step5$Likely_Mechanism == "Likely MAR (observed-data relationship)") else 0`**
  - Could be MCAR: **`r if(exists("assessment_step5")) sum(assessment_step5$Likely_Mechanism == "Could be MCAR") else 0`**
  - Insufficient evidence: **`r if(exists("assessment_step5")) sum(assessment_step5$Likely_Mechanism == "Insufficient evidence") else 0`**
- Interpretation: this step explicitly accounts for **MCAR/MAR/MNAR** using manual checks, a small decision-tree screen, and domain judgment.

---

## Class 9: Implement Basic Imputation Methods - Step 6

**Step 6: Apply Simple Imputation Techniques**

```{r md_step6_basic_imputation, echo=TRUE}
cat("=== STEP 6: APPLY SIMPLE TECHNIQUES ===\n\n")

# Create simple-imputation working copy (do not overwrite MDdf)
MDdf_simple <- MDdf

# Missingness tiers for simple-technique decisions
missing_pct_step6 <- colSums(is.na(MDdf_simple)) / nrow(MDdf_simple)
small_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0 & missing_pct_step6 <= 0.10])
moderate_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0.10 & missing_pct_step6 <= 0.40])
high_missing_vars_step6 <- names(missing_pct_step6[missing_pct_step6 > 0.40])

cat("Small-missing vars (<=10%):", length(small_missing_vars_step6), "\n")
cat("Moderate-missing vars (10-40%):", length(moderate_missing_vars_step6), "\n")
cat("High-missing vars (>40%):", length(high_missing_vars_step6), "\n\n")

# Apply simple methods first to low-missing variables
mode_value_step6 <- function(x) {
  ux <- unique(x[!is.na(x)])
  ux[which.max(tabulate(match(x, ux)))]
}

imputation_log_step6 <- data.frame(
  Variable = character(),
  Missing_Before = integer(),
  Method = character(),
  Missing_After = integer(),
  stringsAsFactors = FALSE
)

vars_for_simple_step6 <- unique(c(
  small_missing_vars_step6,
  intersect(moderate_missing_vars_step6,
            c("DistA", "Age", "Marital", "Rep_Name", "Sex", "Tenure", "Rep_Visits", "Rep_Calls", "Num_Children"))
))

for(v in vars_for_simple_step6) {
  before_n <- sum(is.na(MDdf_simple[[v]]))
  if(before_n == 0) next

  method_used <- ""
  if(is.numeric(MDdf_simple[[v]])) {
    fill <- median(MDdf_simple[[v]], na.rm = TRUE)
    MDdf_simple[[v]][is.na(MDdf_simple[[v]])] <- fill
    method_used <- "Median"
  } else if(is.factor(MDdf_simple[[v]]) || is.character(MDdf_simple[[v]])) {
    fill <- mode_value_step6(MDdf_simple[[v]])
    MDdf_simple[[v]][is.na(MDdf_simple[[v]])] <- fill
    if(is.factor(MDdf_simple[[v]])) MDdf_simple[[v]] <- droplevels(MDdf_simple[[v]])
    method_used <- "Mode"
  }

  after_n <- sum(is.na(MDdf_simple[[v]]))
  imputation_log_step6 <- rbind(imputation_log_step6, data.frame(
    Variable = v,
    Missing_Before = before_n,
    Method = method_used,
    Missing_After = after_n,
    stringsAsFactors = FALSE
  ))
}

remaining_missing_by_var_step6 <- sort(colSums(is.na(MDdf_simple)), decreasing = TRUE)
remaining_missing_by_var_step6 <- remaining_missing_by_var_step6[remaining_missing_by_var_step6 > 0]

cat("Simple methods applied to variables:", length(vars_for_simple_step6), "\n")
cat("Variables with remaining missingness:", length(remaining_missing_by_var_step6), "\n")
cat("Total remaining missing values:", sum(is.na(MDdf_simple)), "\n\n")

if(nrow(imputation_log_step6) > 0) {
  cat("Imputation log (simple methods):\n")
  print(imputation_log_step6)
}

if(length(remaining_missing_by_var_step6) > 0) {
  cat("\nTop remaining missing variables (for complex methods/MICE later):\n")
  print(head(remaining_missing_by_var_step6, 10))
}

cat("\nNote: Step 6 applies simple univariate methods first; remaining high-missing variables are intentionally left for complex imputation.\n")
```

**Step 6 Results and Interpretation:**

- Variables handled with simple methods: **`r nrow(imputation_log_step6)`**
- Remaining missing values after simple techniques: **`r format(sum(is.na(MDdf_simple)), big.mark = ",")`**
- Variables still requiring complex methods: **`r length(remaining_missing_by_var_step6)`**
- Interpretation: simple techniques are applied where appropriate (low/moderate missingness), while high-missing variables are preserved for advanced imputation.

### Missing Data Steps 1-6 Summary

```{r md_steps_summary, echo=FALSE}
cat("=== MISSING DATA SUMMARY (Steps 1-6) ===\n\n")
cat("Step 1: Missing values identified -", length(missing_counts_step1), "variables with NA and",
    format(sum(is.na(MDdf)), big.mark = ","), "missing cells.\n")
cat("Step 2: Missingness indicator variables created -", length(vars_with_na_step2), "indicator columns.\n")
cat("Step 3: Obvious coding issues cleaned -", added_na_from_blanks_step3,
    "blank values and", marital_u_to_na_step3, "Marital='U' values converted to NA.\n")
cat("Step 4: Row/column decisions marked -", length(high_missing_cols_step4),
    "columns >40% missing and", length(rows_high_missing_step4), "rows >50% missing (no hard deletes).\n")
cat("Step 5: Missingness mechanism assessed -",
    sum(assessment_step5$Likely_Mechanism == "Possible MNAR (needs SME validation)"), "possible MNAR,",
    sum(assessment_step5$Likely_Mechanism == "Likely MAR (observed-data relationship)"), "likely MAR,",
    sum(assessment_step5$Likely_Mechanism == "Could be MCAR"), "could be MCAR.\n")
cat("Step 6: Simple techniques applied -", nrow(imputation_log_step6),
    "variables imputed with simple methods; remaining missing values:",
    format(sum(is.na(MDdf_simple)), big.mark = ","), ".\n")
```

# Keep a draft modeling copy after simple techniques
```{r md_export_draft_modeling_file, echo=FALSE}
MDdf_draft_modeling <- MDdf_simple
cat("Draft modeling file created in-session: MDdf_draft_modeling\n")
cat("Dimensions:", nrow(MDdf_draft_modeling), "x", ncol(MDdf_draft_modeling), "\n")
cat("Remaining missing values in draft modeling file:", sum(is.na(MDdf_draft_modeling)), "\n")
```

