
RUNBOOK 


Purpose

`main_clean.py` trains and evaluates an incremental feed-forward neural network
for quantity prediction on pricing data.

The script is fully from-scratch NumPy training with:
- 3 hidden sigmoid layers
- Adam updates + gradient clipping + weight decay
- chunked CSV streaming for large-data training
- learning curve, permutation importance, and partial dependence plots


Code Scope Used for This Runbook

This runbook was created by reading `Deep Learning/main_clean.py` only.
No code was executed.


Required Inputs

Training file columns expected:
- `sku`, `price`, `quantity`, `order`, `duration`, `category`

Test file can be either:
1. with header matching the same columns, or
2. headerless with columns interpreted in this order:
   `sku, price, quantity, order, duration, category`


Path Resolution Behavior

`resolve_paths()` searches for files in this order:
- script directory
- current working directory
- `<cwd>/Deep Learning`

It resolves:
- `pricing.csv`
- `pricing_test.csv`
- output directory = script directory (same folder as `main_clean.py`)


Current Fixed Configuration in Code

From constants near top of `main_clean.py`:
- `BASE_SEED = int(os.environ.get("ILNN_SEED", "42"))`
- `FEATURE_COLS = ["sku", "price", "order", "duration", "category"]`
- `TARGET_COL = "quantity"`
- `BATCH_SIZE = 64`
- `EPOCHS = 12`
- `CHUNK_ROWS = 60000`
- `HIDDEN_SIZES = (160, 80, 40)`
- `LEARNING_RATE = 7.5e-4`
- `X_CLIP = 7.0`
- `WEIGHT_DECAY = 1e-6`
- `LR_DECAY = 0.996`
- `MIN_LR = 1e-5`
- `CATEGORY_HASH_BUCKETS = 16`
- `CATEGORY_SCALE = 0.50`
- `SHUFFLE_BUFFER_CHUNKS = 8`


Feature Engineering
`transform_features(x_raw)` builds 26 total input features:

Numeric block (10 columns):
1. `log1p(price)`
2. `log1p(order)`
3. `log1p(duration)`
4. `order - duration`
5. `price - order`
6. `price - duration`
7. `order / (duration + 1e-3)`
8. `price * duration`
9. `order * duration`
10. `price * order`

Category block (16 columns):
- hashed one-hot from `category % 16`

Important note :
- `sku` is read from CSV but not used in `transform_features` output.
- category one-hot columns are later scaled by `CATEGORY_SCALE` in preprocessing.


Standardization and Preprocessing

`compute_stats(train_file)` performs streaming mean/std over transformed features
and raw target using chunked reads.

`preprocess_x(...)` does:
1. feature transform
2. z-score with train mean/std
3. clip to `[-X_CLIP, X_CLIP]`
4. multiply category one-hot block by `CATEGORY_SCALE`

Target handling:
- no log transform
- standardized as `(y - y_mean) / y_std`
- predictions unstandardized back to original quantity scale


Model Architecture and Optimizer
--------------------------------
`IncrementalNeuralNetwork`:
- Input size inferred from transformed feature dimension (currently 26)
- Architecture: `26 -> 160 -> 80 -> 40 -> 1`
- Hidden activation: sigmoid
- Output: linear
- Weight init: Xavier uniform
- Optimizer: Adam (manual implementation)
  - beta1=0.9, beta2=0.999, epsilon=1e-8
- Regularization: L2 weight decay (`1e-6`)
- Gradient clipping: `[-5, 5]`


Training Flow
-------------
`train_incremental(...)`:
1. loops `EPOCHS` times (12)
2. per-epoch LR decay: `max(initial_lr * LR_DECAY^(epoch-1), MIN_LR)`
3. reads train CSV in chunks of `CHUNK_ROWS`
4. shuffles each chunk
5. stores chunks in a buffer and pops random chunks once buffer is full
6. trains mini-batches of size `BATCH_SIZE`
7. tracks:
   - `instances_trained`
   - `mse_history`
   - memory snapshots

Returns:
- training time
- max memory
- average memory


Evaluation Flow
---------------
`evaluate(...)`:
1. loads test data (header inferred automatically)
2. optionally rescales any test feature whose median ratio vs train median is
   extreme (`>50` or `<0.02`)
3. preprocesses test features with train stats
4. predicts quantity
5. computes metrics:
   - MSE
   - RMSE
   - R2


Interpretability Outputs
------------------------
Using a sampled subset of training rows (`n=5000`):
- permutation importance over raw columns in `FEATURE_COLS`
- partial dependence curves for each feature in `FEATURE_COLS`

Saved files:
- `learning_curve.png`
- `variable_importance.png`
- `partial_dependence.png`

Note:
- There is no `results_summary.json` output in current code.


Operational Notes
-----------------
- If you want reproducible runs, set `ILNN_SEED` before execution.
  Example (PowerShell):
    $env:ILNN_SEED="42"

- The script currently prints some mojibake characters in labels like `RÂ²`
  due to encoding in string literals; this is cosmetic and does not affect
  training logic.

================================================================================
